{"institution": ["Harbin Institute of Technology (Shenzhen)", "Huawei Noah’s Ark Lab"], "reasoning_step": "这篇论文的核心创新点在于统一知识蒸馏（KD）和强化学习（RL），但需要审慎评估其真实贡献：1）方法上，联合损失（joint loss）设计虽有效但本质是加权多任务学习，并非根本性突破；2）实验显示改进幅度有限（平均仅比GRPO高2.6%），且高度依赖教师模型质量；3）数学推理场景的泛化能力未验证，且训练需16次响应采样+教师推断，计算开销极大。需深挖是否通过技巧性设计放大了边际收益。", "problem_background": "现有LLM后训练存在效率与泛化矛盾：强化学习（RL）通过自我探索提升泛化能力，但初始策略探索低效导致样本效率低下；知识蒸馏（KD）通过模仿教师高效学习，但受限于教师能力且域外泛化差。传统两阶段方案（先KD后RL）割裂监督与探索，限制了训练效率和模型上限。", "method": "提出KDRL框架统一优化KD与RL：\n1. 核心目标：策略梯度优化中同时最小化学生-教师反向KL散度（RKL）和最大化规则奖励\n2. 关键设计：\n   - 集成方式：对比奖励塑造（reward shaping）与联合损失（joint loss），后者更稳定\n   - KL近似：采用k2估计器（梯度无偏）替代k3或Top-K\n   - 动态平衡：KL系数β线性退火（5e-3→1e-3），早期强监督后期重探索\n   - 奖励引导掩蔽：对正奖励响应屏蔽KD损失，避免梯度冲突\n3. 训练机制：基于GRPO算法，20K上下文采样16组响应，教师提供实时logit监督", "experiment": "实验设计存在显著局限：\n1. 改进幅度存疑：KDRL-Annealing仅比GRPO高2.6%（56.8%→57.2%），比KD-RKL高1.1%，且依赖强教师（Skywork-OR1-Math-7B）\n2. 场景单一：仅在数学推理基准（AIME/MATH等）验证，未测试常识/代码等泛化能力\n3. 效率矛盾：声称\"高效\"但需实时教师推断+16次采样，实际训练慢于GRPO（280步需80小时）\n4. 指标缺陷：长度控制依赖人为截断（15%截断率），未解决根本性冗余生成问题\n5. 对比不充分：未与最新DPO-based方案比较，且baseline SFT实现较弱（仅拒绝采样）", "one_sentence_summary": "KDRL通过策略梯度联合优化知识蒸馏的反向KL散度与强化学习奖励，在数学推理任务上以更高计算开销换取较基线平均2.6%的性能提升。", "slug": "unified-kd-rl-reasoning", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Reasoning", "Fine-tuning", "On-Policy Learning", "Efficiency"], "further_thoughts": "该工作揭示有趣权衡：KD提供低方差梯度但引致模型坍缩（长度暴涨15%），RL促进探索但高方差。其联合训练实则为方差控制机制，与课程学习本质相通。未来可探索：1）教师弱化场景的鲁棒性（如蒸馏过程噪声注入）；2）将奖励掩蔽扩展为基于置信度的自适应监督，避免人工阈值；3）与思维蒸馏（thought distillation）结合，解耦语义与符号推理的优化目标。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02208", "preference": "unknown", "summary_time": "2025-06-04T07:04:26.972895+00:00", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.", "date": "2025-06-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "created": "2025-06-02", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8738879545419062, "show": 1}
{"institution": ["Chinese Academy of Sciences", "University of California, Los Angeles", "Tsinghua University", "Microsoft", "Hong Kong University of Science and Technology"], "reasoning_step": "这篇论文声称通过动态重加权解决LLM推理冗余问题，但存在明显缺陷：1) 实验对比不充分，未包含主流基准模型如GPT-4的对比数据；2) 动态权重公式(λ计算)缺乏理论支撑，仅凭经验设定；3) 表4显示System-2数据源使用困难问题时性能反而下降(-1.7% GSM8K)，与核心论点矛盾；4) 压缩率计算未考虑不同数据集的token分布差异。需重点验证其方法在复杂任务上的泛化性。", "problem_background": "大型语言模型（LLMs）在复杂推理任务中普遍存在过度思考问题：对于简单问题也生成冗长的思维链（CoT），导致推理效率低下。现有解决方案（如模型融合、强化学习）需要复杂的数据标注或参数调整，过程繁琐且成本高昂。", "method": "提出动态重加权框架TlDr：\n1. 构建双系统数据：System-1（简单问题+简短CoT）和System-2（困难问题+详细CoT）\n2. 动态调整数据权重：每训练周期计算系统效用指标（λ_sys1=效率收益，λ_sys2=精度收益）\n3. 优化目标：最小化与System-1效率上界和System-2精度上界的差距 \n4. 不依赖人工标注：System-1数据来自基础模型，System-2数据通过LongCoT模型采样生成", "experiment": "实验结果存在显著问题：\n• 有效性：在DeepSeek-7B/14B上实现39%平均token压缩，但复杂任务性能下降明显（AIME准确率降1.7%）\n• 实验缺陷：1) 对比基线过时（如未对比2024年SOTA模型）；2) 表3显示相同token预算下L1基线性能优于TlDr；3) 表4表明System-2使用困难问题反而损害GSM8K性能（83.6% vs 原模型89.4%）\n• 指标可疑：压缩率计算未标准化不同数据集token长度分布，且精度评估依赖多次采样（最高8次）降低实用性", "one_sentence_summary": "本文提出动态重加权方法TlDr，通过平衡简短与详细思维链数据的训练比例，在DeepSeek模型上实现39%推理压缩，但复杂任务性能下降且实验设计存在缺陷。", "slug": "dynamic-reweighting-reasoning-compression", "keywords": ["Reasoning", "Efficiency", "Data Augmentation", "Fine-tuning", "Adaptive Systems"], "further_thoughts": "动态权重机制可能受任务难度分布影响：当系统遇到难度连续谱问题时（如数学证明），固定二分System-1/System-2的策略可能失效。未来可探索基于困惑度的实时难度评估，结合强化学习动态调整压缩强度。有趣的是，该论文无意中揭示了困难问题数据对简单任务的负面影响（表4），这对课程学习设计具有启示意义。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02678", "preference": "unknown", "summary_time": "2025-06-04T07:02:25.325534+00:00", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", "date": "2025-06-04", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "created": "2025-06-03", "updated": "2025-06-04", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.9341220055589254, "show": 1}

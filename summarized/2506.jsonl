{"institution": ["Institute of Automation, Chinese Academy of Sciences", "University of Chinese Academy of Sciences", "Meituan"], "reasoning_step": "这篇论文提出了一种新颖的强化学习辅助集成框架RLAE，用于动态调整多个大型语言模型(LLM)的集成权重。核心创新点在于将集成过程建模为马尔可夫决策过程(MDP)，采用span级集成策略平衡细粒度和效率。论文声称在多个基准测试上实现了最高3.3%的性能提升，并具备跨任务泛化能力。但需要批判性审视几个关键点：1) 3.3%的显著提升仅在特定任务(GPQA)和特定基线(DeePEn)对比中出现，其他任务提升幅度较小(如MBPP仅0.6-1.2%)；2) RL智能体采用400M参数的DeBERTa-V3-Large，显著增加计算开销却未量化训练成本；3) 稀疏奖励设计可能限制训练效率；4) 跨任务泛化实验仅测试MMLU→ARC-C迁移，覆盖范围不足；5) 未与近期混合代理(MoA)等先进方法对比。", "problem_background": "现有LLM集成方法存在两大局限：1) 基于排序器的方法(如PairRanker)需要二次计算复杂度且仅支持响应级粗粒度集成；2) 启发式加权方法(如GaC, DeePEn)使用固定权重策略，无法适应不同输入语境(如数学证明vs故事创作)和生成过程中的上下文依赖，导致局部最优但全局次优。这些方法难以动态捕捉不同LLM的语境相关能力差异，如GPT-4o擅长数学推理而Claude精于代码生成。", "method": "RLAE框架将LLM集成建模为马尔可夫决策过程：\n- **状态(s_t)**：输入提示+已生成响应历史\n- **动作(a_t)**：确定各模型在下一span的权重分布(∑wₖ=1)\n- **奖励(r)**：终端状态使用任务特定指标(如准确率)，中间状态可采用过程奖励模型\n\n采用**span级集成策略**：将连续L个token作为基本单元(L=4)，在span起始处调整权重并保持至span结束，大幅降低决策频率(从H次降至H/L次)。解决词表不匹配问题：通过映射矩阵将不同LLM概率向量投影到统一词表空间。实现两种变体：\n1. **RLAE_PPO**：单智能体PPO控制所有权重\n2. **RLAE_MAPPO**：多智能体框架(MAPPO)，各LLM对应独立智能体输出logit值，经Softmax生成权重，共享集中式评论家协调全局奖励。", "experiment": "在7个基准测试(MMLU/ARC-C/TriviaQA/GSM8K/PIQA/GPQA/MBPP)评估，使用5个7B-8B级LLM基模型：\n- **性能**：在多数任务优于基线，最高提升3.3%(GPQA上较DeePEn)，但提升幅度依赖任务和基模型：\n  - 模型性能相近时提升显著(MMLU: +2.3%)\n  - 基模型差距大时提升有限(整合Qwen-2.5时MMLU仅+1.9%)\n  - MAPPO在模型差异大时优于PPO，PPO在需全局一致性的代码任务(MBPP)更优\n- **效率**：延迟与GaC相当，显著低于PairRanker/DeePEn(因span级优化)\n- **泛化性**：MMLU→ARC-C迁移测试中性能降幅仅0.4-0.6点，远优于PairRanker(-3.8点)\n- **消融实验**：\n  - RL加权策略显著优于固定权重(均匀/困惑度加权)\n  - span长度=4时效果最佳\n\n**实验缺陷**：1) 3.3%峰值提升仅出现于特定任务-基线组合 2) 未量化RL智能体(400M参数)的训练开销 3) 稀疏奖励未充分验证 4) 跨任务泛化测试覆盖不足", "one_sentence_summary": "提出强化学习辅助的LLM集成框架RLAE，通过将集成过程建模为马尔可夫决策过程并采用span级动态权重调整，结合单智能体(PPO)或多智能体(MAPPO)算法，在多个基准任务上实现最高3.3%的性能提升并具备跨任务泛化能力。", "slug": "rl-assisted-ensemble-llm", "keywords": ["Reinforcement Learning", "Ensemble", "Foundation Model", "Multi-Agent", "Reasoning", "Efficiency"], "further_thoughts": "值得深入探讨的方向：1) 计算开销与性能的帕累托前沿——400M的RL智能体是否必要？轻量化方案如LSTM智能体或知识蒸馏压缩策略值得探索 2) 奖励函数设计的普适性：当前基于准确率的稀疏奖励可能不适用开放生成任务，需结合LLM-based评估器或人类偏好反馈 3) 与混合代理(MoA)的对比：MoA同样采用分层协调机制，但侧重专家选择而非权重调整，两者结合可能产生协同效应 4) 动态span长度机制：当前固定span长度限制适应性，可引入基于困惑度的自适应span调整 5) 理论解释性：权重分配决策缺乏可解释性，可结合注意力机制可视化决策依据。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.00439", "preference": "unknown", "summary_time": "2025-06-06T07:05:43.245150+00:00", "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs", "authors": ["Yuqian Fu", "Yuanheng Zhu", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Dongbin Zhao"], "abstract": "Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\\text{RLAE}_\\text{PPO}$ and $\\text{RLAE}_\\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.", "date": "2025-06-03", "categories": ["cs.LG", "cs.AI"], "created": "2025-05-31", "updated": "2025-06-03", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8750975253213766, "show": 1}
{"institution": [], "reasoning_step": "作者Edward Y. Chang提出UCCT理论，试图用认知科学框架解释LLM的少样本学习悖论。核心观点是将LLM视为无意识模式存储库，需通过语义锚定激活任务能力。但理论存在明显问题：数学公式（如阈值方程）缺乏实证支持，案例研究仅用演示代替严格实验；类比人脑认知（如全局工作空间理论）过度简化LLM机制；术语如模式密度ρ(P)和表征差距d_r(P,T)未定义可测量方法。需警惕作者用哲学包装掩盖理论空洞性。", "problem_background": "大型语言模型（LLMs）的少样本学习存在悖论：部分任务仅需少量示例即可泛化，而其他任务需大量监督。批评者（如LeCun、Marcus）指摘LLMs缺乏真正认知能力，仅属统计模式匹配。本文试图通过统一认知意识理论（UCCT）解释此现象：将LLMs重新定义为无意识基质——预训练形成的潜在模式存储库，需通过语义锚定（提示、角色分配等）激活任务相关语义，并声称该框架能统一解释提示工程、微调、RAG等方法。", "method": "提出三原则理论框架：\n1. 模式存储库原则：预训练形成未标记的潜在模式分布\n2. 语义锚定原则：通过提示/微调/RAG等外部约束激活特定模式\n3. 阈值跨越原则：锚定强度超越临界值后引发能力相变（类比ReLU激活）\n数学形式化为贝叶斯混合模型：p(y|𝒜,C) = ∫ p(y|P,𝒜) p(P|𝒜,C) dP，其中锚定强度α(𝒜) = αρ(P) - βd_r(P,T) - γlog k。宣称该公式统一涵盖少样本提示、微调（调整先验分布）、RAG（外部模式注入）等场景。", "experiment": "实验设计存在严重缺陷：\n1. 仅四个案例研究（少样本算术、角色辩论、微调医疗诊断、RAG量子计算），均使用商业API（Claude/GPT等）演示，缺乏控制组与统计检验\n2. 结果定性描述为主（如算术案例中模型对-符号的歧义解释），未报告量化指标\n3. 阈值相变声称未经验证：临界值α_c计算公式未在真实任务测试，相变宽度O(1/√n)仅为理论推测\n4. 案例 cherry-picking 明显：回避展示理论失效场景（如低密度模式任务）", "one_sentence_summary": "本文提出统一认知意识理论（UCCT），声称大型语言模型是无意识模式存储库，需通过语义锚定激活任务能力，但未提供可靠实证支持且理论框架过度依赖隐喻类比。", "slug": "unified-cognitive-consciousness-theory", "keywords": ["Emergent Abilities", "Few-Shot Learning", "Foundation Model", "RAG", "Prompt Engineering", "Multi-Agent"], "further_thoughts": "UCCT将认知科学术语（全局工作空间/注意图式）强行映射到LLM机制，存在根本缺陷：人脑意识依赖具身感知与进化预设的神经结构，而LLM仅通过文本统计建模。理论核心公式中的模式密度ρ(P)和表征差距d_r(P,T)无法实际测量，使整个框架不可证伪。有趣的是，阈值相变概念可能启发新的提示优化策略——通过小规模探测确定任务临界锚定强度，但这需脱离原理论框架重新设计实验验证。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02139", "preference": "unknown", "summary_time": "2025-06-05T07:04:39.753462+00:00", "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "authors": ["Edward Y. Chang"], "abstract": "Few-shot learning in large language models (LLMs) reveals a core paradox: certain tasks generalize from just a few examples, while others demand extensive supervision. To explain this, we introduce the Unified Cognitive Consciousness Theory (UCCT), which reconceptualizes LLMs not as deficient agents, but as unconscious substrates: dense, distributed repositories of linguistic and conceptual patterns that operate without explicit semantics, intention, or goal-directed reasoning. Under this view, LLMs are not flawed simulations of cognition but foundational substrates for general intelligence. UCCT posits that semantic anchoring, via prompts, role assignments, and structured interaction, functions as a conscious control layer that modulates latent representations toward task-relevant semantics and enables coherent, structured reasoning. It unifies prompting, fine-tuning, retrieval-augmented generalization, and multi-agent collaboration within a single framework, grounded in the probabilistic alignment between unconscious pattern space and externally imposed semantic constraints (e.g., prompts, supervision, task objectives). The core implication is not to replace LLMs, but to integrate and unify them through a structured cognitive layer that supports intentional reasoning. This enables collections of LLMs to operate within domain-specialized verticals (e.g., legal reasoning, medical diagnosis) that reason, regulate, and adapt together. Such integration is characterized by phase-transition behavior, wherein anchored representations cross coherence thresholds as a function of semantic constraint strength and interaction context.", "date": "2025-06-05", "categories": ["cs.AI"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8843068563462119, "show": 1}
{"institution": ["Harbin Institute of Technology (Shenzhen)", "Huawei Noah’s Ark Lab"], "reasoning_step": "这篇论文的核心创新点在于统一知识蒸馏（KD）和强化学习（RL），但需要审慎评估其真实贡献：1）方法上，联合损失（joint loss）设计虽有效但本质是加权多任务学习，并非根本性突破；2）实验显示改进幅度有限（平均仅比GRPO高2.6%），且高度依赖教师模型质量；3）数学推理场景的泛化能力未验证，且训练需16次响应采样+教师推断，计算开销极大。需深挖是否通过技巧性设计放大了边际收益。", "problem_background": "现有LLM后训练存在效率与泛化矛盾：强化学习（RL）通过自我探索提升泛化能力，但初始策略探索低效导致样本效率低下；知识蒸馏（KD）通过模仿教师高效学习，但受限于教师能力且域外泛化差。传统两阶段方案（先KD后RL）割裂监督与探索，限制了训练效率和模型上限。", "method": "提出KDRL框架统一优化KD与RL：\n1. 核心目标：策略梯度优化中同时最小化学生-教师反向KL散度（RKL）和最大化规则奖励\n2. 关键设计：\n   - 集成方式：对比奖励塑造（reward shaping）与联合损失（joint loss），后者更稳定\n   - KL近似：采用k2估计器（梯度无偏）替代k3或Top-K\n   - 动态平衡：KL系数β线性退火（5e-3→1e-3），早期强监督后期重探索\n   - 奖励引导掩蔽：对正奖励响应屏蔽KD损失，避免梯度冲突\n3. 训练机制：基于GRPO算法，20K上下文采样16组响应，教师提供实时logit监督", "experiment": "实验设计存在显著局限：\n1. 改进幅度存疑：KDRL-Annealing仅比GRPO高2.6%（56.8%→57.2%），比KD-RKL高1.1%，且依赖强教师（Skywork-OR1-Math-7B）\n2. 场景单一：仅在数学推理基准（AIME/MATH等）验证，未测试常识/代码等泛化能力\n3. 效率矛盾：声称\"高效\"但需实时教师推断+16次采样，实际训练慢于GRPO（280步需80小时）\n4. 指标缺陷：长度控制依赖人为截断（15%截断率），未解决根本性冗余生成问题\n5. 对比不充分：未与最新DPO-based方案比较，且baseline SFT实现较弱（仅拒绝采样）", "one_sentence_summary": "KDRL通过策略梯度联合优化知识蒸馏的反向KL散度与强化学习奖励，在数学推理任务上以更高计算开销换取较基线平均2.6%的性能提升。", "slug": "unified-kd-rl-reasoning", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Reasoning", "Fine-tuning", "On-Policy Learning", "Efficiency"], "further_thoughts": "该工作揭示有趣权衡：KD提供低方差梯度但引致模型坍缩（长度暴涨15%），RL促进探索但高方差。其联合训练实则为方差控制机制，与课程学习本质相通。未来可探索：1）教师弱化场景的鲁棒性（如蒸馏过程噪声注入）；2）将奖励掩蔽扩展为基于置信度的自适应监督，避免人工阈值；3）与思维蒸馏（thought distillation）结合，解耦语义与符号推理的优化目标。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02208", "preference": "unknown", "summary_time": "2025-06-04T07:04:26.972895+00:00", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.", "date": "2025-06-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "created": "2025-06-02", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8738879545419062, "show": 1}
{"institution": ["Chinese Academy of Sciences", "University of California, Los Angeles", "Tsinghua University", "Microsoft", "Hong Kong University of Science and Technology"], "reasoning_step": "这篇论文声称通过动态重加权解决LLM推理冗余问题，但存在明显缺陷：1) 实验对比不充分，未包含主流基准模型如GPT-4的对比数据；2) 动态权重公式(λ计算)缺乏理论支撑，仅凭经验设定；3) 表4显示System-2数据源使用困难问题时性能反而下降(-1.7% GSM8K)，与核心论点矛盾；4) 压缩率计算未考虑不同数据集的token分布差异。需重点验证其方法在复杂任务上的泛化性。", "problem_background": "大型语言模型（LLMs）在复杂推理任务中普遍存在过度思考问题：对于简单问题也生成冗长的思维链（CoT），导致推理效率低下。现有解决方案（如模型融合、强化学习）需要复杂的数据标注或参数调整，过程繁琐且成本高昂。", "method": "提出动态重加权框架TlDr：\n1. 构建双系统数据：System-1（简单问题+简短CoT）和System-2（困难问题+详细CoT）\n2. 动态调整数据权重：每训练周期计算系统效用指标（λ_sys1=效率收益，λ_sys2=精度收益）\n3. 优化目标：最小化与System-1效率上界和System-2精度上界的差距 \n4. 不依赖人工标注：System-1数据来自基础模型，System-2数据通过LongCoT模型采样生成", "experiment": "实验结果存在显著问题：\n• 有效性：在DeepSeek-7B/14B上实现39%平均token压缩，但复杂任务性能下降明显（AIME准确率降1.7%）\n• 实验缺陷：1) 对比基线过时（如未对比2024年SOTA模型）；2) 表3显示相同token预算下L1基线性能优于TlDr；3) 表4表明System-2使用困难问题反而损害GSM8K性能（83.6% vs 原模型89.4%）\n• 指标可疑：压缩率计算未标准化不同数据集token长度分布，且精度评估依赖多次采样（最高8次）降低实用性", "one_sentence_summary": "本文提出动态重加权方法TlDr，通过平衡简短与详细思维链数据的训练比例，在DeepSeek模型上实现39%推理压缩，但复杂任务性能下降且实验设计存在缺陷。", "slug": "dynamic-reweighting-reasoning-compression", "keywords": ["Reasoning", "Efficiency", "Data Augmentation", "Fine-tuning", "Adaptive Systems"], "further_thoughts": "动态权重机制可能受任务难度分布影响：当系统遇到难度连续谱问题时（如数学证明），固定二分System-1/System-2的策略可能失效。未来可探索基于困惑度的实时难度评估，结合强化学习动态调整压缩强度。有趣的是，该论文无意中揭示了困难问题数据对简单任务的负面影响（表4），这对课程学习设计具有启示意义。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02678", "preference": "unknown", "summary_time": "2025-06-04T07:02:25.325534+00:00", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", "date": "2025-06-04", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "created": "2025-06-03", "updated": "2025-06-04", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.9341220055589254, "show": 1}
{"institution": ["Università di Pisa", "National Research Council"], "reasoning_step": "这篇立场论文的核心论点是：尽管基础模型强大，但持续学习在解决其静态性、高成本和中心化风险方面仍不可替代。作者提出CPT、CFT、CCO三个方向，但未提供新实验数据支撑其核心主张（尤其是CCO的优越性）。需要批判性审视：1）CCO作为'未来范式'的论证是否充分（仅基于理论推演）；2）对CPT/CFT挑战的讨论深度不足（如未量化灾难性遗忘的影响）；3）未考虑多智能体系统的通信开销等现实瓶颈。", "problem_background": "基础模型（如LLM）存在静态性缺陷：1）知识过时需昂贵重训练（GPT-4训练成本数千万美元）；2）无法适应动态环境（如实时用户偏好）；3）集中化导致垄断与偏见风险。持续学习能解决模型漂移、个性化不足及高碳足迹问题，但需重新定义其在基础模型时代的角色。", "method": "提出持续学习三大方向：\n1. **持续预训练（CPT）**：增量更新基础模型参数整合新知识，采用自监督目标（如掩码预测）减轻灾难性遗忘\n2. **持续微调（CFT）**：轻量级任务适配技术（LoRA/Adapter），仅更新部分参数实现领域个性化\n3. **持续组合与编排（CCO）**：动态协调多智能体系统（如MoE架构），通过角色分工（专家模块）和自然语言通信解决复杂任务", "experiment": "**方法论缺陷**：\n- 无原创实验，仅文献综述（如引用LLaMA参数效率、LoRA遗忘率等二手数据）\n- 未验证关键主张：CCO相比单体模型的优势缺乏实证（如未对比ARC-AGI基准表现）\n- 实验设置片面：忽略多智能体系统的通信延迟/能耗等核心指标\n**有效性存疑**：\n1. CPT依赖未经验证的'稳定性差距'理论（初始性能下降现象）\n2. CFT的隐私保护方案（联邦学习）未考虑千亿参数传输开销\n3. CCO错误传播风险（如智能体幻觉放大）未量化评估", "one_sentence_summary": "本文论证基础模型时代持续学习仍不可或缺，提出持续预训练、持续微调与持续组合三大方向，主张动态编排多智能体的持续组合范式将主导未来去中心化AI生态。", "slug": "continual-learning-foundation-models-future", "keywords": ["Continual Learning", "Adaptive Systems", "Foundation Model", "Pre-training", "Fine-tuning", "Multi-Agent"], "further_thoughts": "作者将CCO类比'社会协作'存在严重简化：1）多智能体协调需解决纳什均衡问题（如任务分配冲突），但未引入博弈论机制；2）强调'去中心化'却忽视区块链验证等底层架构；3）与神经符号AI结合可能更高效（如用符号模块约束错误传播）。未来需在生化计算等低功耗场景验证CCO可行性，避免算力需求悖离可持续发展主张。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03320", "preference": "unknown", "summary_time": "2025-06-05T07:03:33.585061+00:00", "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "abstract": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.", "date": "2025-06-05", "categories": ["cs.LG", "cs.AI"], "created": "2025-06-03", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8882130776320888, "show": 1}
{"institution": ["Peking University", "WeChat AI", "William & Mary", "Westlake University"], "reasoning_step": "论文提出 RewardAnything 框架解决传统奖励模型的两个核心问题：泛化性差（需为每个新任务重训）和隐式偏好导致的偏差。方法创新在于将自然语言原则作为动态输入条件，结合 GRPO 强化学习训练生成式奖励模型。实验设计全面：1）创建 RABench 基准评估原则泛化能力；2）在 RM-Bench 验证偏差缓解效果；3）通过对齐案例证明实用性。需深究的点：合成训练数据（LLM 生成+共识算法）可能引入新偏差；GRPO 奖励函数设计复杂（4 个子指标加权），消融实验未量化各子项贡献；案例研究仅用 2000 prompts 的小规模验证。", "problem_background": "传统奖励模型存在两大瓶颈：1) 泛化性差：基于静态偏好数据集训练，无法适应动态需求（如客服需简洁性而研究助手需细节），切换任务需重收集数据并重训模型；2) 隐式偏好偏差：仅学习结果级监督（选择/拒绝标签）而忽略决策依据，导致模型通过虚假相关性学习价值观（如倾向长文本），产生偏差且缺乏可解释性。", "method": "1) 提出原则遵循范式：奖励模型以自然语言原则 P 为条件输入，输出响应 X_i 的分数 S(P,Q,X_i) → ℝ；2) 设计 RewardAnything：生成式架构输出结构化评估（推理+分数+排名）；3) 训练方法：采用 Group Relative Preference Learning (GRPO) 强化学习，自定义奖励函数 r = λ_f r_f + λ_a r_a，其中 r_f 激励格式规范（含推理质量/结构完整性等），r_a 衡量与真实偏好一致性（含加权错排惩罚/分数分布匹配等 4 项子指标）。", "experiment": "1) 评估体系：创建 RABench 基准（50 原则+1002 人工验证排名）测泛化性，RM-Bench 测偏差缓解；2) 有效性：在 RM-Bench 硬集上 SotA（86.4%），证明显式原则可缓解传统模型的长文本偏好偏差；在 RABench 接近 GPT-4.1（81.9% vs 82.5%），显示强泛化；3) 消融实验：移除原则导致性能崩溃（-13.4%），列表训练→成对训练降 8.4%，GRPO→监督微调降 11.2%；4) 案例：仅用自然语言原则对齐 Qwen3-8B，安全响应质量提升 12%。", "one_sentence_summary": "本文提出 RewardAnything——基于 GRPO 强化学习的生成式原则遵循奖励模型，通过自然语言原则动态调控奖励生成，实现零样本任务适应并缓解隐式偏好偏差，在 RABench/RM-Bench 达到 SotA 且案例验证高效对齐能力。", "slug": "principle-following-reward-models", "keywords": ["Reward Modeling", "Reinforcement Learning", "Zero-Shot Learning", "Interpretability", "Alignment"], "further_thoughts": "原则遵循范式可扩展至多模态对齐（如图文一致性原则），但需解决原则冲突时的优先级建模。当前训练依赖合成数据（LLM 生成+共识算法），可能继承基础模型偏见；GRPO 的列表式评估计算开销大，轻量化版本值得探索。有趣的是，该方法隐含实现『可编程价值观』——通过修改原则动态切换道德框架（如功利主义 vs 义务论），为伦理对齐提供新路径。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03637", "preference": "unknown", "summary_time": "2025-06-06T07:04:07.985442+00:00", "title": "RewardAnything: Generalizable Principle-Following Reward Models", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "abstract": "Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.", "date": "2025-06-05", "categories": ["cs.CL", "cs.AI", "cs.LG"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8694144410509131, "show": 1}
{"institution": ["University of Maryland", "University of Michigan", "Princeton University", "Amazon AGI", "University of Central Florida"], "reasoning_step": "本文的核心发现是：在推理模型中，延长思考时间（如添加'Wait'提示）会导致输出方差增加，形成性能先升后降的非单调现象。这一结论基于高斯概率模型的简化假设，但实际语言模型是高维离散分布，该假设可能过度简化了复杂性。作者提出的'并行思考'方案本质上是Best-of-N采样的变体，缺乏与现有方法（如自洽性采样）的对比实验。此外，实验仅使用1.5B-8B蒸馏模型，未验证在更大规模原始模型（如DeepSeek-R1本体）上的普适性。值得深究的是：方差增加是否必然导致性能下降？在MATH等高难度数据集中，增加探索性是否可能有益？", "problem_background": "当前流行观点认为延长推理模型的思考轨迹（如添加'Wait'提示）能提升性能。本文质疑该观点，通过实证研究发现：增加思考长度会先提升后降低准确率（过思考现象）。核心问题是揭示该非单调趋势的本质原因，并探索更优的测试时计算资源分配策略。", "method": "*   过思考实证分析：采用两种测试时预算控制方法——(1) 'Wait & Think more'：抑制终止符，迭代添加'Wait'延长思考；(2) 'Exact thinking tokens'：精确控制思考token数量。\n*   方差驱动解释：提出简化概率模型（假设输出服从高斯分布），证明方差增加会先扩大奖励函数覆盖（性能升）后稀释概率密度（性能降）。通过测量策略分布的熵验证方差增长。\n*   并行思考方案：给定总token预算B，生成N条独立推理路径（∑|z⁽ⁱ⁾|≤B），每条路径产生答案y⁽ⁱ⁾，最后通过多数投票选择最终答案yᵇᵉˢᵗ。", "experiment": "*   数据集：GSM-8K（1320样本）、MATH-500（500样本）、AIME（30竞赛题）。\n*   模型：DeepSeek-R1蒸馏版（1.5B/7B/8B）。\n*   关键结果：\n    - 过思考现象：在两种预算控制下，思考token增加均导致准确率呈倒U型曲线（如GSM8K上1.5B模型：token从385增至1100时准确率从82.2%→87.3%，继续增至15980则降至70.3%）。\n    - 方差验证：思考token增加使熵提升12倍（GSM8K熵0.23→2.79），与概率模型预测一致。\n    - 并行思考优势：相同16K token预算下，相比序列思考的11.8%性能下降，并行思考实现10.1%提升。\n*   实验缺陷：\n    - 仅对比自建基线，未与自洽性采样等标准方法比较；\n    - 熵测量基于相同prompt的多次采样，但实际扩展思考会改变上下文，测量方法存疑；\n    - 小规模蒸馏模型结论是否适用于原始大模型未经验证。", "one_sentence_summary": "本文揭示推理模型测试时延长思考会导致输出方差增加引发'过思考'现象，提出通过并行生成多条推理路径的预算分配策略，在固定计算资源下实现最高22%的性能提升。", "slug": "overthinking-parallel-thinking-reasoning", "keywords": ["Variance", "Parallel Sampling", "Test Time", "Efficiency", "Reasoning", "Entropy", "Scaling"], "further_thoughts": "过思考现象可能与任务复杂度相关：在GSM8K等简单任务中方差增加主要引入噪声，但在需要发散思维的创意推理中或有益处。并行思考本质上是用空间换稳定性，未来可结合早停机制（如基于方差的动态终止）进一步优化资源分配。值得注意的是，人类推理同样存在'过度分析瘫痪'现象，该研究为构建类人推理的AI模型提供了认知科学视角的启发。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.04210", "preference": "unknown", "summary_time": "2025-06-05T07:04:22.461739+00:00", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.", "date": "2025-06-05", "categories": ["cs.AI", "cs.CL"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8526188594042221, "show": 1}
{"institution": ["Chinese University of Hong Kong"], "reasoning_step": "这篇论文提出了一种基于函数逼近理论的新型Transformer架构，用于上下文学习。作者通过傅里叶分析构建通用特征表示，并设计Transformer层模拟近端梯度下降算法求解Lasso问题。理论证明显示该架构能逼近任意满足傅里叶梯度范数有界条件的函数类，但存在三个关键疑问：1) 傅里叶条件在实际任务中是否普遍成立？2) 所需维度n随覆盖数对数增长是否会导致实际模型过大？3) 未验证构造性方法在真实数据集的效果，理论边界可能过于乐观。这些限制需要后续实证研究验证。", "problem_background": "现有研究将Transformer解释为优化算法（如梯度下降）的逼近器，但该方法受限于优化算法本身的收敛性要求，仅适用于凸问题或线性函数类。本文旨在突破这一限制，从函数逼近视角构建通用的上下文学习理论框架，解决Transformer如何同时学习通用表示并动态适应上下文示例的核心问题。", "method": "1) 通过傅里叶分析构建通用特征集：利用Sigmoid激活函数构造特征映射ϕᵢ，使任意目标函数f(x)-f(0)可表示为∑ρᵢϕᵢ(x)的线性组合\n2) 设计Lasso估计器：定义带L1正则的优化问题min_ρ Σ(y_i-ϕᵢᵀρ)² + λ‖ρ‖₁求解系数\n3) Transformer实现近端梯度：构造多层多头Transformer精确模拟近端梯度迭代过程，每层更新规则为ρ_{t+1} = ST_{ηλ}(ρ_t + 2η/N Σ(y_i-ϕᵢᵀρ_t)ϕ_i) + e_{t+1}\n4) 输入编码设计：扩展输入维度至d+2n+7，包含原始输入、输出、特征向量、系数估计等多模态信息", "experiment": "1) 实验性质：纯理论分析，无实证验证\n2) 效果指标：推导预测误差上界𝔼[(ŷ_{N+1}-f(x_{N+1}))²] ≲ √(logN/N) + n/L + (log|𝒩_ε|/n)^{2/3}\n3) 实验缺陷：未在真实数据集验证理论边界；未比较现有上下文学习方法；构造的Transformer维度随覆盖数对数增长，实际计算开销存疑；傅里叶条件（梯度傅里叶变换的L1范数有界）的现实适用性未讨论", "one_sentence_summary": "本文提出基于通用函数逼近的Transformer构造方法，通过傅里叶特征分解和近端梯度模拟，理论上实现任意满足傅里叶条件的函数类在上下文学习中的逼近，突破现有优化算法逼近框架的凸性限制。", "slug": "transformers-universal-approximation-in-context-learning", "keywords": ["Transformer", "In-Context Learning", "Universal Approximation", "Representation Learning", "Regression"], "further_thoughts": "该理论揭示了Transformer通过特征空间线性重组实现上下文适应的本质机制，这与认知科学中的组合泛化理论形成有趣映射：人类在新任务中同样复用基础认知模块重组。但构造依赖显式傅里叶分解，与真实Transformer通过注意力隐式学习形成对比，后续可探索如何将特征构造过程融入预训练目标。另需关注理论边界中L∝ε^{-5/2}的苛刻深度要求，这或解释为何千亿参数模型才涌现强上下文学习能力。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.05200", "preference": "unknown", "summary_time": "2025-06-06T07:04:28.448942+00:00", "title": "Transformers Meet In-Context Learning: A Universal Approximation Theory", "authors": ["Gen Li", "Yuchen Jiao", "Yu Huang", "Yuting Wei", "Yuxin Chen"], "abstract": "Modern large language models are capable of in-context learning, the ability to perform new tasks at inference time using only a handful of input-output examples in the prompt, without any fine-tuning or parameter updates. We develop a universal approximation theory to better understand how transformers enable in-context learning. For any class of functions (each representing a distinct task), we demonstrate how to construct a transformer that, without any further weight updates, can perform reliable prediction given only a few in-context examples. In contrast to much of the recent literature that frames transformers as algorithm approximators -- i.e., constructing transformers to emulate the iterations of optimization algorithms as a means to approximate solutions of learning problems -- our work adopts a fundamentally different approach rooted in universal function approximation. This alternative approach offers approximation guarantees that are not constrained by the effectiveness of the optimization algorithms being approximated, thereby extending far beyond convex problems and linear function classes. Our construction sheds light on how transformers can simultaneously learn general-purpose representations and adapt dynamically to in-context examples.", "date": "2025-06-06", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "created": "2025-06-05", "updated": "2025-06-06", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8584234334691381, "show": 1}
{"institution": ["Stanford University", "SynthLabs"], "reasoning_step": "本文提出自适应长度惩罚（ALP）方法解决大推理模型在简单问题上过度计算的问题。核心创新在于利用在线解决率动态调整长度惩罚强度：高解决率（简单问题）施加强惩罚，低解决率（困难问题）弱惩罚。实验显示ALP显著降低平均token使用量（50%）并重分配计算资源，但需批判性审视：1) 实验仅限数学领域，泛化性存疑；2) 对比基线使用不同基础模型和训练配置，公平性不足；3) 行为分析显示压缩可能削弱回溯等关键推理机制；4) 多rollout训练机制计算开销未充分讨论。需深入分析ALP是否真正学习到问题本质难度，或仅是过拟合特定数据集模式。", "problem_background": "大型推理模型（LRM）为提升复杂问题性能常生成冗余推理轨迹，导致简单问题计算资源浪费（如DeepSeek-R1用10k token回答'2+3'）。现有方法存在局限：监督微调需精细数据标注；用户控制预算需手动配置；统一长度惩罚忽视问题难度差异。亟需自适应计算分配机制，使模型根据问题难度动态调整推理长度。", "method": "*   **核心机制：** 基于强化学习框架，引入与在线解决率成反比的自适应长度惩罚项。\n*   **实现过程：** \n    1. 对每个提示q采样K个独立推理轨迹，计算经验解决率p_solved(q) = (1/K)∑𝟙[answer(y⁽ᵏ⁾)=y∗]\n    2. 设计复合奖励函数：r(y,q) = 𝟙[答案正确] - βN max(p_solved(q), 1/K)\n    3. p_solved(q)越高（问题越简单），长度惩罚权重越大，强制模型缩短响应\n*   **关键特性：** 无缝集成GRPO等现有RL算法，利用其多轨迹采样机制估算解决率，无额外计算开销。", "experiment": "*   **有效性验证：** \n    - 在MATH-500/OlympiadBench/AIME数学数据集测试，ALP平均减少50% token使用\n    - 帕累托分析显示：ALP仅用21% token解决最简单50%问题，困难问题token分配提升5.35倍\n*   **实验缺陷：** \n    1. 对比基线（L1-Exact/ThinkPrune等）使用不同基础模型和训练配置，公平性存疑\n    2. 仅测试数学领域，未验证跨领域泛化能力（如代码/科学推理）\n    3. 行为分析显示回溯行为减少62%，可能削弱模型自我纠错能力\n*   **结果矛盾点：** 表1显示R1-Distill-Qwen模型应用ALP后OlympiadBench性能提升42%（0.36→0.51），但未解释为何基础模型性能突变", "one_sentence_summary": "提出基于强化学习的自适应长度惩罚机制，通过在线解决率动态调整推理长度，在数学任务上实现50%计算效率提升并重分配资源至困难问题，但泛化性与公平性验证不足。", "slug": "adaptive-length-penalty", "keywords": ["Reinforcement Learning", "Reasoning", "Efficiency", "Adaptive Systems"], "further_thoughts": "ALP的核心假设'解决率=问题难度'存在根本局限：1) 高解决率可能因记忆而非理解（如训练数据泄露）；2) 数学问题有明确答案，但开放域问题（道德推理等）无二值判断标准；3) 实验显示L1-Max在困难问题上token使用反而减少，暗示难度估计可能被对抗样本操纵。未来应探索多维度难度估计（如不确定性量化），并与《Chain-of-Thought Without Prompting》的隐式推理压缩机制结合。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.05256", "preference": "unknown", "summary_time": "2025-06-06T07:03:40.279754+00:00", "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning", "authors": ["Violet Xiang", "Chase Blagden", "Rafael Rafailov", "Nathan Lile", "Sang Truong", "Chelsea Finn", "Nick Haber"], "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.", "date": "2025-06-06", "categories": ["cs.AI", "cs.LG"], "created": "2025-06-05", "updated": "2025-06-06", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8660272910576859, "show": 1}

{"institution": [], "reasoning_step": "作者Edward Y. Chang提出UCCT理论，试图用认知科学框架解释LLM的少样本学习悖论。核心观点是将LLM视为无意识模式存储库，需通过语义锚定激活任务能力。但理论存在明显问题：数学公式（如阈值方程）缺乏实证支持，案例研究仅用演示代替严格实验；类比人脑认知（如全局工作空间理论）过度简化LLM机制；术语如模式密度ρ(P)和表征差距d_r(P,T)未定义可测量方法。需警惕作者用哲学包装掩盖理论空洞性。", "problem_background": "大型语言模型（LLMs）的少样本学习存在悖论：部分任务仅需少量示例即可泛化，而其他任务需大量监督。批评者（如LeCun、Marcus）指摘LLMs缺乏真正认知能力，仅属统计模式匹配。本文试图通过统一认知意识理论（UCCT）解释此现象：将LLMs重新定义为无意识基质——预训练形成的潜在模式存储库，需通过语义锚定（提示、角色分配等）激活任务相关语义，并声称该框架能统一解释提示工程、微调、RAG等方法。", "method": "提出三原则理论框架：\n1. 模式存储库原则：预训练形成未标记的潜在模式分布\n2. 语义锚定原则：通过提示/微调/RAG等外部约束激活特定模式\n3. 阈值跨越原则：锚定强度超越临界值后引发能力相变（类比ReLU激活）\n数学形式化为贝叶斯混合模型：p(y|𝒜,C) = ∫ p(y|P,𝒜) p(P|𝒜,C) dP，其中锚定强度α(𝒜) = αρ(P) - βd_r(P,T) - γlog k。宣称该公式统一涵盖少样本提示、微调（调整先验分布）、RAG（外部模式注入）等场景。", "experiment": "实验设计存在严重缺陷：\n1. 仅四个案例研究（少样本算术、角色辩论、微调医疗诊断、RAG量子计算），均使用商业API（Claude/GPT等）演示，缺乏控制组与统计检验\n2. 结果定性描述为主（如算术案例中模型对-符号的歧义解释），未报告量化指标\n3. 阈值相变声称未经验证：临界值α_c计算公式未在真实任务测试，相变宽度O(1/√n)仅为理论推测\n4. 案例 cherry-picking 明显：回避展示理论失效场景（如低密度模式任务）", "one_sentence_summary": "本文提出统一认知意识理论（UCCT），声称大型语言模型是无意识模式存储库，需通过语义锚定激活任务能力，但未提供可靠实证支持且理论框架过度依赖隐喻类比。", "slug": "unified-cognitive-consciousness-theory", "keywords": ["Emergent Abilities", "Few-Shot Learning", "Foundation Model", "RAG", "Prompt Engineering", "Multi-Agent"], "further_thoughts": "UCCT将认知科学术语（全局工作空间/注意图式）强行映射到LLM机制，存在根本缺陷：人脑意识依赖具身感知与进化预设的神经结构，而LLM仅通过文本统计建模。理论核心公式中的模式密度ρ(P)和表征差距d_r(P,T)无法实际测量，使整个框架不可证伪。有趣的是，阈值相变概念可能启发新的提示优化策略——通过小规模探测确定任务临界锚定强度，但这需脱离原理论框架重新设计实验验证。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02139", "preference": "unknown", "summary_time": "2025-06-05T07:04:39.753462+00:00", "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "authors": ["Edward Y. Chang"], "abstract": "Few-shot learning in large language models (LLMs) reveals a core paradox: certain tasks generalize from just a few examples, while others demand extensive supervision. To explain this, we introduce the Unified Cognitive Consciousness Theory (UCCT), which reconceptualizes LLMs not as deficient agents, but as unconscious substrates: dense, distributed repositories of linguistic and conceptual patterns that operate without explicit semantics, intention, or goal-directed reasoning. Under this view, LLMs are not flawed simulations of cognition but foundational substrates for general intelligence. UCCT posits that semantic anchoring, via prompts, role assignments, and structured interaction, functions as a conscious control layer that modulates latent representations toward task-relevant semantics and enables coherent, structured reasoning. It unifies prompting, fine-tuning, retrieval-augmented generalization, and multi-agent collaboration within a single framework, grounded in the probabilistic alignment between unconscious pattern space and externally imposed semantic constraints (e.g., prompts, supervision, task objectives). The core implication is not to replace LLMs, but to integrate and unify them through a structured cognitive layer that supports intentional reasoning. This enables collections of LLMs to operate within domain-specialized verticals (e.g., legal reasoning, medical diagnosis) that reason, regulate, and adapt together. Such integration is characterized by phase-transition behavior, wherein anchored representations cross coherence thresholds as a function of semantic constraint strength and interaction context.", "date": "2025-06-05", "categories": ["cs.AI"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8843068563462119, "show": 1}
{"institution": ["Harbin Institute of Technology (Shenzhen)", "Huawei Noah’s Ark Lab"], "reasoning_step": "这篇论文的核心创新点在于统一知识蒸馏（KD）和强化学习（RL），但需要审慎评估其真实贡献：1）方法上，联合损失（joint loss）设计虽有效但本质是加权多任务学习，并非根本性突破；2）实验显示改进幅度有限（平均仅比GRPO高2.6%），且高度依赖教师模型质量；3）数学推理场景的泛化能力未验证，且训练需16次响应采样+教师推断，计算开销极大。需深挖是否通过技巧性设计放大了边际收益。", "problem_background": "现有LLM后训练存在效率与泛化矛盾：强化学习（RL）通过自我探索提升泛化能力，但初始策略探索低效导致样本效率低下；知识蒸馏（KD）通过模仿教师高效学习，但受限于教师能力且域外泛化差。传统两阶段方案（先KD后RL）割裂监督与探索，限制了训练效率和模型上限。", "method": "提出KDRL框架统一优化KD与RL：\n1. 核心目标：策略梯度优化中同时最小化学生-教师反向KL散度（RKL）和最大化规则奖励\n2. 关键设计：\n   - 集成方式：对比奖励塑造（reward shaping）与联合损失（joint loss），后者更稳定\n   - KL近似：采用k2估计器（梯度无偏）替代k3或Top-K\n   - 动态平衡：KL系数β线性退火（5e-3→1e-3），早期强监督后期重探索\n   - 奖励引导掩蔽：对正奖励响应屏蔽KD损失，避免梯度冲突\n3. 训练机制：基于GRPO算法，20K上下文采样16组响应，教师提供实时logit监督", "experiment": "实验设计存在显著局限：\n1. 改进幅度存疑：KDRL-Annealing仅比GRPO高2.6%（56.8%→57.2%），比KD-RKL高1.1%，且依赖强教师（Skywork-OR1-Math-7B）\n2. 场景单一：仅在数学推理基准（AIME/MATH等）验证，未测试常识/代码等泛化能力\n3. 效率矛盾：声称\"高效\"但需实时教师推断+16次采样，实际训练慢于GRPO（280步需80小时）\n4. 指标缺陷：长度控制依赖人为截断（15%截断率），未解决根本性冗余生成问题\n5. 对比不充分：未与最新DPO-based方案比较，且baseline SFT实现较弱（仅拒绝采样）", "one_sentence_summary": "KDRL通过策略梯度联合优化知识蒸馏的反向KL散度与强化学习奖励，在数学推理任务上以更高计算开销换取较基线平均2.6%的性能提升。", "slug": "unified-kd-rl-reasoning", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Reasoning", "Fine-tuning", "On-Policy Learning", "Efficiency"], "further_thoughts": "该工作揭示有趣权衡：KD提供低方差梯度但引致模型坍缩（长度暴涨15%），RL促进探索但高方差。其联合训练实则为方差控制机制，与课程学习本质相通。未来可探索：1）教师弱化场景的鲁棒性（如蒸馏过程噪声注入）；2）将奖励掩蔽扩展为基于置信度的自适应监督，避免人工阈值；3）与思维蒸馏（thought distillation）结合，解耦语义与符号推理的优化目标。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02208", "preference": "unknown", "summary_time": "2025-06-04T07:04:26.972895+00:00", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.", "date": "2025-06-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "created": "2025-06-02", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8738879545419062, "show": 1}
{"institution": ["Chinese Academy of Sciences", "University of California, Los Angeles", "Tsinghua University", "Microsoft", "Hong Kong University of Science and Technology"], "reasoning_step": "这篇论文声称通过动态重加权解决LLM推理冗余问题，但存在明显缺陷：1) 实验对比不充分，未包含主流基准模型如GPT-4的对比数据；2) 动态权重公式(λ计算)缺乏理论支撑，仅凭经验设定；3) 表4显示System-2数据源使用困难问题时性能反而下降(-1.7% GSM8K)，与核心论点矛盾；4) 压缩率计算未考虑不同数据集的token分布差异。需重点验证其方法在复杂任务上的泛化性。", "problem_background": "大型语言模型（LLMs）在复杂推理任务中普遍存在过度思考问题：对于简单问题也生成冗长的思维链（CoT），导致推理效率低下。现有解决方案（如模型融合、强化学习）需要复杂的数据标注或参数调整，过程繁琐且成本高昂。", "method": "提出动态重加权框架TlDr：\n1. 构建双系统数据：System-1（简单问题+简短CoT）和System-2（困难问题+详细CoT）\n2. 动态调整数据权重：每训练周期计算系统效用指标（λ_sys1=效率收益，λ_sys2=精度收益）\n3. 优化目标：最小化与System-1效率上界和System-2精度上界的差距 \n4. 不依赖人工标注：System-1数据来自基础模型，System-2数据通过LongCoT模型采样生成", "experiment": "实验结果存在显著问题：\n• 有效性：在DeepSeek-7B/14B上实现39%平均token压缩，但复杂任务性能下降明显（AIME准确率降1.7%）\n• 实验缺陷：1) 对比基线过时（如未对比2024年SOTA模型）；2) 表3显示相同token预算下L1基线性能优于TlDr；3) 表4表明System-2使用困难问题反而损害GSM8K性能（83.6% vs 原模型89.4%）\n• 指标可疑：压缩率计算未标准化不同数据集token长度分布，且精度评估依赖多次采样（最高8次）降低实用性", "one_sentence_summary": "本文提出动态重加权方法TlDr，通过平衡简短与详细思维链数据的训练比例，在DeepSeek模型上实现39%推理压缩，但复杂任务性能下降且实验设计存在缺陷。", "slug": "dynamic-reweighting-reasoning-compression", "keywords": ["Reasoning", "Efficiency", "Data Augmentation", "Fine-tuning", "Adaptive Systems"], "further_thoughts": "动态权重机制可能受任务难度分布影响：当系统遇到难度连续谱问题时（如数学证明），固定二分System-1/System-2的策略可能失效。未来可探索基于困惑度的实时难度评估，结合强化学习动态调整压缩强度。有趣的是，该论文无意中揭示了困难问题数据对简单任务的负面影响（表4），这对课程学习设计具有启示意义。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02678", "preference": "unknown", "summary_time": "2025-06-04T07:02:25.325534+00:00", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", "date": "2025-06-04", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "created": "2025-06-03", "updated": "2025-06-04", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.9341220055589254, "show": 1}
{"institution": ["Università di Pisa", "National Research Council"], "reasoning_step": "这篇立场论文的核心论点是：尽管基础模型强大，但持续学习在解决其静态性、高成本和中心化风险方面仍不可替代。作者提出CPT、CFT、CCO三个方向，但未提供新实验数据支撑其核心主张（尤其是CCO的优越性）。需要批判性审视：1）CCO作为'未来范式'的论证是否充分（仅基于理论推演）；2）对CPT/CFT挑战的讨论深度不足（如未量化灾难性遗忘的影响）；3）未考虑多智能体系统的通信开销等现实瓶颈。", "problem_background": "基础模型（如LLM）存在静态性缺陷：1）知识过时需昂贵重训练（GPT-4训练成本数千万美元）；2）无法适应动态环境（如实时用户偏好）；3）集中化导致垄断与偏见风险。持续学习能解决模型漂移、个性化不足及高碳足迹问题，但需重新定义其在基础模型时代的角色。", "method": "提出持续学习三大方向：\n1. **持续预训练（CPT）**：增量更新基础模型参数整合新知识，采用自监督目标（如掩码预测）减轻灾难性遗忘\n2. **持续微调（CFT）**：轻量级任务适配技术（LoRA/Adapter），仅更新部分参数实现领域个性化\n3. **持续组合与编排（CCO）**：动态协调多智能体系统（如MoE架构），通过角色分工（专家模块）和自然语言通信解决复杂任务", "experiment": "**方法论缺陷**：\n- 无原创实验，仅文献综述（如引用LLaMA参数效率、LoRA遗忘率等二手数据）\n- 未验证关键主张：CCO相比单体模型的优势缺乏实证（如未对比ARC-AGI基准表现）\n- 实验设置片面：忽略多智能体系统的通信延迟/能耗等核心指标\n**有效性存疑**：\n1. CPT依赖未经验证的'稳定性差距'理论（初始性能下降现象）\n2. CFT的隐私保护方案（联邦学习）未考虑千亿参数传输开销\n3. CCO错误传播风险（如智能体幻觉放大）未量化评估", "one_sentence_summary": "本文论证基础模型时代持续学习仍不可或缺，提出持续预训练、持续微调与持续组合三大方向，主张动态编排多智能体的持续组合范式将主导未来去中心化AI生态。", "slug": "continual-learning-foundation-models-future", "keywords": ["Continual Learning", "Adaptive Systems", "Foundation Model", "Pre-training", "Fine-tuning", "Multi-Agent"], "further_thoughts": "作者将CCO类比'社会协作'存在严重简化：1）多智能体协调需解决纳什均衡问题（如任务分配冲突），但未引入博弈论机制；2）强调'去中心化'却忽视区块链验证等底层架构；3）与神经符号AI结合可能更高效（如用符号模块约束错误传播）。未来需在生化计算等低功耗场景验证CCO可行性，避免算力需求悖离可持续发展主张。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03320", "preference": "unknown", "summary_time": "2025-06-05T07:03:33.585061+00:00", "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "abstract": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.", "date": "2025-06-05", "categories": ["cs.LG", "cs.AI"], "created": "2025-06-03", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8882130776320888, "show": 1}
{"institution": ["University of Maryland", "University of Michigan", "Princeton University", "Amazon AGI", "University of Central Florida"], "reasoning_step": "本文的核心发现是：在推理模型中，延长思考时间（如添加'Wait'提示）会导致输出方差增加，形成性能先升后降的非单调现象。这一结论基于高斯概率模型的简化假设，但实际语言模型是高维离散分布，该假设可能过度简化了复杂性。作者提出的'并行思考'方案本质上是Best-of-N采样的变体，缺乏与现有方法（如自洽性采样）的对比实验。此外，实验仅使用1.5B-8B蒸馏模型，未验证在更大规模原始模型（如DeepSeek-R1本体）上的普适性。值得深究的是：方差增加是否必然导致性能下降？在MATH等高难度数据集中，增加探索性是否可能有益？", "problem_background": "当前流行观点认为延长推理模型的思考轨迹（如添加'Wait'提示）能提升性能。本文质疑该观点，通过实证研究发现：增加思考长度会先提升后降低准确率（过思考现象）。核心问题是揭示该非单调趋势的本质原因，并探索更优的测试时计算资源分配策略。", "method": "*   过思考实证分析：采用两种测试时预算控制方法——(1) 'Wait & Think more'：抑制终止符，迭代添加'Wait'延长思考；(2) 'Exact thinking tokens'：精确控制思考token数量。\n*   方差驱动解释：提出简化概率模型（假设输出服从高斯分布），证明方差增加会先扩大奖励函数覆盖（性能升）后稀释概率密度（性能降）。通过测量策略分布的熵验证方差增长。\n*   并行思考方案：给定总token预算B，生成N条独立推理路径（∑|z⁽ⁱ⁾|≤B），每条路径产生答案y⁽ⁱ⁾，最后通过多数投票选择最终答案yᵇᵉˢᵗ。", "experiment": "*   数据集：GSM-8K（1320样本）、MATH-500（500样本）、AIME（30竞赛题）。\n*   模型：DeepSeek-R1蒸馏版（1.5B/7B/8B）。\n*   关键结果：\n    - 过思考现象：在两种预算控制下，思考token增加均导致准确率呈倒U型曲线（如GSM8K上1.5B模型：token从385增至1100时准确率从82.2%→87.3%，继续增至15980则降至70.3%）。\n    - 方差验证：思考token增加使熵提升12倍（GSM8K熵0.23→2.79），与概率模型预测一致。\n    - 并行思考优势：相同16K token预算下，相比序列思考的11.8%性能下降，并行思考实现10.1%提升。\n*   实验缺陷：\n    - 仅对比自建基线，未与自洽性采样等标准方法比较；\n    - 熵测量基于相同prompt的多次采样，但实际扩展思考会改变上下文，测量方法存疑；\n    - 小规模蒸馏模型结论是否适用于原始大模型未经验证。", "one_sentence_summary": "本文揭示推理模型测试时延长思考会导致输出方差增加引发'过思考'现象，提出通过并行生成多条推理路径的预算分配策略，在固定计算资源下实现最高22%的性能提升。", "slug": "overthinking-parallel-thinking-reasoning", "keywords": ["Variance", "Parallel Sampling", "Test Time", "Efficiency", "Reasoning", "Entropy", "Scaling"], "further_thoughts": "过思考现象可能与任务复杂度相关：在GSM8K等简单任务中方差增加主要引入噪声，但在需要发散思维的创意推理中或有益处。并行思考本质上是用空间换稳定性，未来可结合早停机制（如基于方差的动态终止）进一步优化资源分配。值得注意的是，人类推理同样存在'过度分析瘫痪'现象，该研究为构建类人推理的AI模型提供了认知科学视角的启发。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.04210", "preference": "unknown", "summary_time": "2025-06-05T07:04:22.461739+00:00", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.", "date": "2025-06-05", "categories": ["cs.AI", "cs.CL"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8526188594042221, "show": 1}

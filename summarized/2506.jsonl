{"institution": [], "reasoning_step": "ä½œè€…Edward Y. Changæå‡ºUCCTç†è®ºï¼Œè¯•å›¾ç”¨è®¤çŸ¥ç§‘å­¦æ¡†æ¶è§£é‡ŠLLMçš„å°‘æ ·æœ¬å­¦ä¹ æ‚–è®ºã€‚æ ¸å¿ƒè§‚ç‚¹æ˜¯å°†LLMè§†ä¸ºæ— æ„è¯†æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šæ¿€æ´»ä»»åŠ¡èƒ½åŠ›ã€‚ä½†ç†è®ºå­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼šæ•°å­¦å…¬å¼ï¼ˆå¦‚é˜ˆå€¼æ–¹ç¨‹ï¼‰ç¼ºä¹å®è¯æ”¯æŒï¼Œæ¡ˆä¾‹ç ”ç©¶ä»…ç”¨æ¼”ç¤ºä»£æ›¿ä¸¥æ ¼å®éªŒï¼›ç±»æ¯”äººè„‘è®¤çŸ¥ï¼ˆå¦‚å…¨å±€å·¥ä½œç©ºé—´ç†è®ºï¼‰è¿‡åº¦ç®€åŒ–LLMæœºåˆ¶ï¼›æœ¯è¯­å¦‚æ¨¡å¼å¯†åº¦Ï(P)å’Œè¡¨å¾å·®è·d_r(P,T)æœªå®šä¹‰å¯æµ‹é‡æ–¹æ³•ã€‚éœ€è­¦æƒ•ä½œè€…ç”¨å“²å­¦åŒ…è£…æ©ç›–ç†è®ºç©ºæ´æ€§ã€‚", "problem_background": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ å­˜åœ¨æ‚–è®ºï¼šéƒ¨åˆ†ä»»åŠ¡ä»…éœ€å°‘é‡ç¤ºä¾‹å³å¯æ³›åŒ–ï¼Œè€Œå…¶ä»–ä»»åŠ¡éœ€å¤§é‡ç›‘ç£ã€‚æ‰¹è¯„è€…ï¼ˆå¦‚LeCunã€Marcusï¼‰æŒ‡æ‘˜LLMsç¼ºä¹çœŸæ­£è®¤çŸ¥èƒ½åŠ›ï¼Œä»…å±ç»Ÿè®¡æ¨¡å¼åŒ¹é…ã€‚æœ¬æ–‡è¯•å›¾é€šè¿‡ç»Ÿä¸€è®¤çŸ¥æ„è¯†ç†è®ºï¼ˆUCCTï¼‰è§£é‡Šæ­¤ç°è±¡ï¼šå°†LLMsé‡æ–°å®šä¹‰ä¸ºæ— æ„è¯†åŸºè´¨â€”â€”é¢„è®­ç»ƒå½¢æˆçš„æ½œåœ¨æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šï¼ˆæç¤ºã€è§’è‰²åˆ†é…ç­‰ï¼‰æ¿€æ´»ä»»åŠ¡ç›¸å…³è¯­ä¹‰ï¼Œå¹¶å£°ç§°è¯¥æ¡†æ¶èƒ½ç»Ÿä¸€è§£é‡Šæç¤ºå·¥ç¨‹ã€å¾®è°ƒã€RAGç­‰æ–¹æ³•ã€‚", "method": "æå‡ºä¸‰åŸåˆ™ç†è®ºæ¡†æ¶ï¼š\n1. æ¨¡å¼å­˜å‚¨åº“åŸåˆ™ï¼šé¢„è®­ç»ƒå½¢æˆæœªæ ‡è®°çš„æ½œåœ¨æ¨¡å¼åˆ†å¸ƒ\n2. è¯­ä¹‰é”šå®šåŸåˆ™ï¼šé€šè¿‡æç¤º/å¾®è°ƒ/RAGç­‰å¤–éƒ¨çº¦æŸæ¿€æ´»ç‰¹å®šæ¨¡å¼\n3. é˜ˆå€¼è·¨è¶ŠåŸåˆ™ï¼šé”šå®šå¼ºåº¦è¶…è¶Šä¸´ç•Œå€¼åå¼•å‘èƒ½åŠ›ç›¸å˜ï¼ˆç±»æ¯”ReLUæ¿€æ´»ï¼‰\næ•°å­¦å½¢å¼åŒ–ä¸ºè´å¶æ–¯æ··åˆæ¨¡å‹ï¼šp(y|ğ’œ,C) = âˆ« p(y|P,ğ’œ) p(P|ğ’œ,C) dPï¼Œå…¶ä¸­é”šå®šå¼ºåº¦Î±(ğ’œ) = Î±Ï(P) - Î²d_r(P,T) - Î³log kã€‚å®£ç§°è¯¥å…¬å¼ç»Ÿä¸€æ¶µç›–å°‘æ ·æœ¬æç¤ºã€å¾®è°ƒï¼ˆè°ƒæ•´å…ˆéªŒåˆ†å¸ƒï¼‰ã€RAGï¼ˆå¤–éƒ¨æ¨¡å¼æ³¨å…¥ï¼‰ç­‰åœºæ™¯ã€‚", "experiment": "å®éªŒè®¾è®¡å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼š\n1. ä»…å››ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼ˆå°‘æ ·æœ¬ç®—æœ¯ã€è§’è‰²è¾©è®ºã€å¾®è°ƒåŒ»ç–—è¯Šæ–­ã€RAGé‡å­è®¡ç®—ï¼‰ï¼Œå‡ä½¿ç”¨å•†ä¸šAPIï¼ˆClaude/GPTç­‰ï¼‰æ¼”ç¤ºï¼Œç¼ºä¹æ§åˆ¶ç»„ä¸ç»Ÿè®¡æ£€éªŒ\n2. ç»“æœå®šæ€§æè¿°ä¸ºä¸»ï¼ˆå¦‚ç®—æœ¯æ¡ˆä¾‹ä¸­æ¨¡å‹å¯¹-ç¬¦å·çš„æ­§ä¹‰è§£é‡Šï¼‰ï¼ŒæœªæŠ¥å‘Šé‡åŒ–æŒ‡æ ‡\n3. é˜ˆå€¼ç›¸å˜å£°ç§°æœªç»éªŒè¯ï¼šä¸´ç•Œå€¼Î±_cè®¡ç®—å…¬å¼æœªåœ¨çœŸå®ä»»åŠ¡æµ‹è¯•ï¼Œç›¸å˜å®½åº¦O(1/âˆšn)ä»…ä¸ºç†è®ºæ¨æµ‹\n4. æ¡ˆä¾‹ cherry-picking æ˜æ˜¾ï¼šå›é¿å±•ç¤ºç†è®ºå¤±æ•ˆåœºæ™¯ï¼ˆå¦‚ä½å¯†åº¦æ¨¡å¼ä»»åŠ¡ï¼‰", "one_sentence_summary": "æœ¬æ–‡æå‡ºç»Ÿä¸€è®¤çŸ¥æ„è¯†ç†è®ºï¼ˆUCCTï¼‰ï¼Œå£°ç§°å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯æ— æ„è¯†æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šæ¿€æ´»ä»»åŠ¡èƒ½åŠ›ï¼Œä½†æœªæä¾›å¯é å®è¯æ”¯æŒä¸”ç†è®ºæ¡†æ¶è¿‡åº¦ä¾èµ–éšå–»ç±»æ¯”ã€‚", "slug": "unified-cognitive-consciousness-theory", "keywords": ["Emergent Abilities", "Few-Shot Learning", "Foundation Model", "RAG", "Prompt Engineering", "Multi-Agent"], "further_thoughts": "UCCTå°†è®¤çŸ¥ç§‘å­¦æœ¯è¯­ï¼ˆå…¨å±€å·¥ä½œç©ºé—´/æ³¨æ„å›¾å¼ï¼‰å¼ºè¡Œæ˜ å°„åˆ°LLMæœºåˆ¶ï¼Œå­˜åœ¨æ ¹æœ¬ç¼ºé™·ï¼šäººè„‘æ„è¯†ä¾èµ–å…·èº«æ„ŸçŸ¥ä¸è¿›åŒ–é¢„è®¾çš„ç¥ç»ç»“æ„ï¼Œè€ŒLLMä»…é€šè¿‡æ–‡æœ¬ç»Ÿè®¡å»ºæ¨¡ã€‚ç†è®ºæ ¸å¿ƒå…¬å¼ä¸­çš„æ¨¡å¼å¯†åº¦Ï(P)å’Œè¡¨å¾å·®è·d_r(P,T)æ— æ³•å®é™…æµ‹é‡ï¼Œä½¿æ•´ä¸ªæ¡†æ¶ä¸å¯è¯ä¼ªã€‚æœ‰è¶£çš„æ˜¯ï¼Œé˜ˆå€¼ç›¸å˜æ¦‚å¿µå¯èƒ½å¯å‘æ–°çš„æç¤ºä¼˜åŒ–ç­–ç•¥â€”â€”é€šè¿‡å°è§„æ¨¡æ¢æµ‹ç¡®å®šä»»åŠ¡ä¸´ç•Œé”šå®šå¼ºåº¦ï¼Œä½†è¿™éœ€è„±ç¦»åŸç†è®ºæ¡†æ¶é‡æ–°è®¾è®¡å®éªŒéªŒè¯ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02139", "preference": "unknown", "summary_time": "2025-06-05T07:04:39.753462+00:00", "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "authors": ["Edward Y. Chang"], "abstract": "Few-shot learning in large language models (LLMs) reveals a core paradox: certain tasks generalize from just a few examples, while others demand extensive supervision. To explain this, we introduce the Unified Cognitive Consciousness Theory (UCCT), which reconceptualizes LLMs not as deficient agents, but as unconscious substrates: dense, distributed repositories of linguistic and conceptual patterns that operate without explicit semantics, intention, or goal-directed reasoning. Under this view, LLMs are not flawed simulations of cognition but foundational substrates for general intelligence. UCCT posits that semantic anchoring, via prompts, role assignments, and structured interaction, functions as a conscious control layer that modulates latent representations toward task-relevant semantics and enables coherent, structured reasoning. It unifies prompting, fine-tuning, retrieval-augmented generalization, and multi-agent collaboration within a single framework, grounded in the probabilistic alignment between unconscious pattern space and externally imposed semantic constraints (e.g., prompts, supervision, task objectives). The core implication is not to replace LLMs, but to integrate and unify them through a structured cognitive layer that supports intentional reasoning. This enables collections of LLMs to operate within domain-specialized verticals (e.g., legal reasoning, medical diagnosis) that reason, regulate, and adapt together. Such integration is characterized by phase-transition behavior, wherein anchored representations cross coherence thresholds as a function of semantic constraint strength and interaction context.", "date": "2025-06-05", "categories": ["cs.AI"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8843068563462119, "show": 1}
{"institution": ["Harbin Institute of Technology (Shenzhen)", "Huawei Noahâ€™s Ark Lab"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºç»Ÿä¸€çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½†éœ€è¦å®¡æ…è¯„ä¼°å…¶çœŸå®è´¡çŒ®ï¼š1ï¼‰æ–¹æ³•ä¸Šï¼Œè”åˆæŸå¤±ï¼ˆjoint lossï¼‰è®¾è®¡è™½æœ‰æ•ˆä½†æœ¬è´¨æ˜¯åŠ æƒå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶éæ ¹æœ¬æ€§çªç ´ï¼›2ï¼‰å®éªŒæ˜¾ç¤ºæ”¹è¿›å¹…åº¦æœ‰é™ï¼ˆå¹³å‡ä»…æ¯”GRPOé«˜2.6%ï¼‰ï¼Œä¸”é«˜åº¦ä¾èµ–æ•™å¸ˆæ¨¡å‹è´¨é‡ï¼›3ï¼‰æ•°å­¦æ¨ç†åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›æœªéªŒè¯ï¼Œä¸”è®­ç»ƒéœ€16æ¬¡å“åº”é‡‡æ ·+æ•™å¸ˆæ¨æ–­ï¼Œè®¡ç®—å¼€é”€æå¤§ã€‚éœ€æ·±æŒ–æ˜¯å¦é€šè¿‡æŠ€å·§æ€§è®¾è®¡æ”¾å¤§äº†è¾¹é™…æ”¶ç›Šã€‚", "problem_background": "ç°æœ‰LLMåè®­ç»ƒå­˜åœ¨æ•ˆç‡ä¸æ³›åŒ–çŸ›ç›¾ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡è‡ªæˆ‘æ¢ç´¢æå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†åˆå§‹ç­–ç•¥æ¢ç´¢ä½æ•ˆå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼›çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰é€šè¿‡æ¨¡ä»¿æ•™å¸ˆé«˜æ•ˆå­¦ä¹ ï¼Œä½†å—é™äºæ•™å¸ˆèƒ½åŠ›ä¸”åŸŸå¤–æ³›åŒ–å·®ã€‚ä¼ ç»Ÿä¸¤é˜¶æ®µæ–¹æ¡ˆï¼ˆå…ˆKDåRLï¼‰å‰²è£‚ç›‘ç£ä¸æ¢ç´¢ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹ä¸Šé™ã€‚", "method": "æå‡ºKDRLæ¡†æ¶ç»Ÿä¸€ä¼˜åŒ–KDä¸RLï¼š\n1. æ ¸å¿ƒç›®æ ‡ï¼šç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ä¸­åŒæ—¶æœ€å°åŒ–å­¦ç”Ÿ-æ•™å¸ˆåå‘KLæ•£åº¦ï¼ˆRKLï¼‰å’Œæœ€å¤§åŒ–è§„åˆ™å¥–åŠ±\n2. å…³é”®è®¾è®¡ï¼š\n   - é›†æˆæ–¹å¼ï¼šå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆreward shapingï¼‰ä¸è”åˆæŸå¤±ï¼ˆjoint lossï¼‰ï¼Œåè€…æ›´ç¨³å®š\n   - KLè¿‘ä¼¼ï¼šé‡‡ç”¨k2ä¼°è®¡å™¨ï¼ˆæ¢¯åº¦æ— åï¼‰æ›¿ä»£k3æˆ–Top-K\n   - åŠ¨æ€å¹³è¡¡ï¼šKLç³»æ•°Î²çº¿æ€§é€€ç«ï¼ˆ5e-3â†’1e-3ï¼‰ï¼Œæ—©æœŸå¼ºç›‘ç£åæœŸé‡æ¢ç´¢\n   - å¥–åŠ±å¼•å¯¼æ©è”½ï¼šå¯¹æ­£å¥–åŠ±å“åº”å±è”½KDæŸå¤±ï¼Œé¿å…æ¢¯åº¦å†²çª\n3. è®­ç»ƒæœºåˆ¶ï¼šåŸºäºGRPOç®—æ³•ï¼Œ20Kä¸Šä¸‹æ–‡é‡‡æ ·16ç»„å“åº”ï¼Œæ•™å¸ˆæä¾›å®æ—¶logitç›‘ç£", "experiment": "å®éªŒè®¾è®¡å­˜åœ¨æ˜¾è‘—å±€é™ï¼š\n1. æ”¹è¿›å¹…åº¦å­˜ç–‘ï¼šKDRL-Annealingä»…æ¯”GRPOé«˜2.6%ï¼ˆ56.8%â†’57.2%ï¼‰ï¼Œæ¯”KD-RKLé«˜1.1%ï¼Œä¸”ä¾èµ–å¼ºæ•™å¸ˆï¼ˆSkywork-OR1-Math-7Bï¼‰\n2. åœºæ™¯å•ä¸€ï¼šä»…åœ¨æ•°å­¦æ¨ç†åŸºå‡†ï¼ˆAIME/MATHç­‰ï¼‰éªŒè¯ï¼Œæœªæµ‹è¯•å¸¸è¯†/ä»£ç ç­‰æ³›åŒ–èƒ½åŠ›\n3. æ•ˆç‡çŸ›ç›¾ï¼šå£°ç§°\"é«˜æ•ˆ\"ä½†éœ€å®æ—¶æ•™å¸ˆæ¨æ–­+16æ¬¡é‡‡æ ·ï¼Œå®é™…è®­ç»ƒæ…¢äºGRPOï¼ˆ280æ­¥éœ€80å°æ—¶ï¼‰\n4. æŒ‡æ ‡ç¼ºé™·ï¼šé•¿åº¦æ§åˆ¶ä¾èµ–äººä¸ºæˆªæ–­ï¼ˆ15%æˆªæ–­ç‡ï¼‰ï¼Œæœªè§£å†³æ ¹æœ¬æ€§å†—ä½™ç”Ÿæˆé—®é¢˜\n5. å¯¹æ¯”ä¸å……åˆ†ï¼šæœªä¸æœ€æ–°DPO-basedæ–¹æ¡ˆæ¯”è¾ƒï¼Œä¸”baseline SFTå®ç°è¾ƒå¼±ï¼ˆä»…æ‹’ç»é‡‡æ ·ï¼‰", "one_sentence_summary": "KDRLé€šè¿‡ç­–ç•¥æ¢¯åº¦è”åˆä¼˜åŒ–çŸ¥è¯†è’¸é¦çš„åå‘KLæ•£åº¦ä¸å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä»¥æ›´é«˜è®¡ç®—å¼€é”€æ¢å–è¾ƒåŸºçº¿å¹³å‡2.6%çš„æ€§èƒ½æå‡ã€‚", "slug": "unified-kd-rl-reasoning", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Reasoning", "Fine-tuning", "On-Policy Learning", "Efficiency"], "further_thoughts": "è¯¥å·¥ä½œæ­ç¤ºæœ‰è¶£æƒè¡¡ï¼šKDæä¾›ä½æ–¹å·®æ¢¯åº¦ä½†å¼•è‡´æ¨¡å‹åç¼©ï¼ˆé•¿åº¦æš´æ¶¨15%ï¼‰ï¼ŒRLä¿ƒè¿›æ¢ç´¢ä½†é«˜æ–¹å·®ã€‚å…¶è”åˆè®­ç»ƒå®åˆ™ä¸ºæ–¹å·®æ§åˆ¶æœºåˆ¶ï¼Œä¸è¯¾ç¨‹å­¦ä¹ æœ¬è´¨ç›¸é€šã€‚æœªæ¥å¯æ¢ç´¢ï¼š1ï¼‰æ•™å¸ˆå¼±åŒ–åœºæ™¯çš„é²æ£’æ€§ï¼ˆå¦‚è’¸é¦è¿‡ç¨‹å™ªå£°æ³¨å…¥ï¼‰ï¼›2ï¼‰å°†å¥–åŠ±æ©è”½æ‰©å±•ä¸ºåŸºäºç½®ä¿¡åº¦çš„è‡ªé€‚åº”ç›‘ç£ï¼Œé¿å…äººå·¥é˜ˆå€¼ï¼›3ï¼‰ä¸æ€ç»´è’¸é¦ï¼ˆthought distillationï¼‰ç»“åˆï¼Œè§£è€¦è¯­ä¹‰ä¸ç¬¦å·æ¨ç†çš„ä¼˜åŒ–ç›®æ ‡ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02208", "preference": "unknown", "summary_time": "2025-06-04T07:04:26.972895+00:00", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.", "date": "2025-06-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "created": "2025-06-02", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8738879545419062, "show": 1}
{"institution": ["Chinese Academy of Sciences", "University of California, Los Angeles", "Tsinghua University", "Microsoft", "Hong Kong University of Science and Technology"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡å£°ç§°é€šè¿‡åŠ¨æ€é‡åŠ æƒè§£å†³LLMæ¨ç†å†—ä½™é—®é¢˜ï¼Œä½†å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š1) å®éªŒå¯¹æ¯”ä¸å……åˆ†ï¼ŒæœªåŒ…å«ä¸»æµåŸºå‡†æ¨¡å‹å¦‚GPT-4çš„å¯¹æ¯”æ•°æ®ï¼›2) åŠ¨æ€æƒé‡å…¬å¼(Î»è®¡ç®—)ç¼ºä¹ç†è®ºæ”¯æ’‘ï¼Œä»…å‡­ç»éªŒè®¾å®šï¼›3) è¡¨4æ˜¾ç¤ºSystem-2æ•°æ®æºä½¿ç”¨å›°éš¾é—®é¢˜æ—¶æ€§èƒ½åè€Œä¸‹é™(-1.7% GSM8K)ï¼Œä¸æ ¸å¿ƒè®ºç‚¹çŸ›ç›¾ï¼›4) å‹ç¼©ç‡è®¡ç®—æœªè€ƒè™‘ä¸åŒæ•°æ®é›†çš„tokenåˆ†å¸ƒå·®å¼‚ã€‚éœ€é‡ç‚¹éªŒè¯å…¶æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§ã€‚", "problem_background": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ™®éå­˜åœ¨è¿‡åº¦æ€è€ƒé—®é¢˜ï¼šå¯¹äºç®€å•é—®é¢˜ä¹Ÿç”Ÿæˆå†—é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ¨¡å‹èåˆã€å¼ºåŒ–å­¦ä¹ ï¼‰éœ€è¦å¤æ‚çš„æ•°æ®æ ‡æ³¨æˆ–å‚æ•°è°ƒæ•´ï¼Œè¿‡ç¨‹ç¹çä¸”æˆæœ¬é«˜æ˜‚ã€‚", "method": "æå‡ºåŠ¨æ€é‡åŠ æƒæ¡†æ¶TlDrï¼š\n1. æ„å»ºåŒç³»ç»Ÿæ•°æ®ï¼šSystem-1ï¼ˆç®€å•é—®é¢˜+ç®€çŸ­CoTï¼‰å’ŒSystem-2ï¼ˆå›°éš¾é—®é¢˜+è¯¦ç»†CoTï¼‰\n2. åŠ¨æ€è°ƒæ•´æ•°æ®æƒé‡ï¼šæ¯è®­ç»ƒå‘¨æœŸè®¡ç®—ç³»ç»Ÿæ•ˆç”¨æŒ‡æ ‡ï¼ˆÎ»_sys1=æ•ˆç‡æ”¶ç›Šï¼ŒÎ»_sys2=ç²¾åº¦æ”¶ç›Šï¼‰\n3. ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å°åŒ–ä¸System-1æ•ˆç‡ä¸Šç•Œå’ŒSystem-2ç²¾åº¦ä¸Šç•Œçš„å·®è· \n4. ä¸ä¾èµ–äººå·¥æ ‡æ³¨ï¼šSystem-1æ•°æ®æ¥è‡ªåŸºç¡€æ¨¡å‹ï¼ŒSystem-2æ•°æ®é€šè¿‡LongCoTæ¨¡å‹é‡‡æ ·ç”Ÿæˆ", "experiment": "å®éªŒç»“æœå­˜åœ¨æ˜¾è‘—é—®é¢˜ï¼š\nâ€¢ æœ‰æ•ˆæ€§ï¼šåœ¨DeepSeek-7B/14Bä¸Šå®ç°39%å¹³å‡tokenå‹ç¼©ï¼Œä½†å¤æ‚ä»»åŠ¡æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼ˆAIMEå‡†ç¡®ç‡é™1.7%ï¼‰\nâ€¢ å®éªŒç¼ºé™·ï¼š1) å¯¹æ¯”åŸºçº¿è¿‡æ—¶ï¼ˆå¦‚æœªå¯¹æ¯”2024å¹´SOTAæ¨¡å‹ï¼‰ï¼›2) è¡¨3æ˜¾ç¤ºç›¸åŒtokené¢„ç®—ä¸‹L1åŸºçº¿æ€§èƒ½ä¼˜äºTlDrï¼›3) è¡¨4è¡¨æ˜System-2ä½¿ç”¨å›°éš¾é—®é¢˜åè€ŒæŸå®³GSM8Kæ€§èƒ½ï¼ˆ83.6% vs åŸæ¨¡å‹89.4%ï¼‰\nâ€¢ æŒ‡æ ‡å¯ç–‘ï¼šå‹ç¼©ç‡è®¡ç®—æœªæ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†tokené•¿åº¦åˆ†å¸ƒï¼Œä¸”ç²¾åº¦è¯„ä¼°ä¾èµ–å¤šæ¬¡é‡‡æ ·ï¼ˆæœ€é«˜8æ¬¡ï¼‰é™ä½å®ç”¨æ€§", "one_sentence_summary": "æœ¬æ–‡æå‡ºåŠ¨æ€é‡åŠ æƒæ–¹æ³•TlDrï¼Œé€šè¿‡å¹³è¡¡ç®€çŸ­ä¸è¯¦ç»†æ€ç»´é“¾æ•°æ®çš„è®­ç»ƒæ¯”ä¾‹ï¼Œåœ¨DeepSeekæ¨¡å‹ä¸Šå®ç°39%æ¨ç†å‹ç¼©ï¼Œä½†å¤æ‚ä»»åŠ¡æ€§èƒ½ä¸‹é™ä¸”å®éªŒè®¾è®¡å­˜åœ¨ç¼ºé™·ã€‚", "slug": "dynamic-reweighting-reasoning-compression", "keywords": ["Reasoning", "Efficiency", "Data Augmentation", "Fine-tuning", "Adaptive Systems"], "further_thoughts": "åŠ¨æ€æƒé‡æœºåˆ¶å¯èƒ½å—ä»»åŠ¡éš¾åº¦åˆ†å¸ƒå½±å“ï¼šå½“ç³»ç»Ÿé‡åˆ°éš¾åº¦è¿ç»­è°±é—®é¢˜æ—¶ï¼ˆå¦‚æ•°å­¦è¯æ˜ï¼‰ï¼Œå›ºå®šäºŒåˆ†System-1/System-2çš„ç­–ç•¥å¯èƒ½å¤±æ•ˆã€‚æœªæ¥å¯æ¢ç´¢åŸºäºå›°æƒ‘åº¦çš„å®æ—¶éš¾åº¦è¯„ä¼°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´å‹ç¼©å¼ºåº¦ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥è®ºæ–‡æ— æ„ä¸­æ­ç¤ºäº†å›°éš¾é—®é¢˜æ•°æ®å¯¹ç®€å•ä»»åŠ¡çš„è´Ÿé¢å½±å“ï¼ˆè¡¨4ï¼‰ï¼Œè¿™å¯¹è¯¾ç¨‹å­¦ä¹ è®¾è®¡å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02678", "preference": "unknown", "summary_time": "2025-06-04T07:02:25.325534+00:00", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", "date": "2025-06-04", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "created": "2025-06-03", "updated": "2025-06-04", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.9341220055589254, "show": 1}
{"institution": ["UniversitÃ  di Pisa", "National Research Council"], "reasoning_step": "è¿™ç¯‡ç«‹åœºè®ºæ–‡çš„æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼šå°½ç®¡åŸºç¡€æ¨¡å‹å¼ºå¤§ï¼Œä½†æŒç»­å­¦ä¹ åœ¨è§£å†³å…¶é™æ€æ€§ã€é«˜æˆæœ¬å’Œä¸­å¿ƒåŒ–é£é™©æ–¹é¢ä»ä¸å¯æ›¿ä»£ã€‚ä½œè€…æå‡ºCPTã€CFTã€CCOä¸‰ä¸ªæ–¹å‘ï¼Œä½†æœªæä¾›æ–°å®éªŒæ•°æ®æ”¯æ’‘å…¶æ ¸å¿ƒä¸»å¼ ï¼ˆå°¤å…¶æ˜¯CCOçš„ä¼˜è¶Šæ€§ï¼‰ã€‚éœ€è¦æ‰¹åˆ¤æ€§å®¡è§†ï¼š1ï¼‰CCOä½œä¸º'æœªæ¥èŒƒå¼'çš„è®ºè¯æ˜¯å¦å……åˆ†ï¼ˆä»…åŸºäºç†è®ºæ¨æ¼”ï¼‰ï¼›2ï¼‰å¯¹CPT/CFTæŒ‘æˆ˜çš„è®¨è®ºæ·±åº¦ä¸è¶³ï¼ˆå¦‚æœªé‡åŒ–ç¾éš¾æ€§é—å¿˜çš„å½±å“ï¼‰ï¼›3ï¼‰æœªè€ƒè™‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€šä¿¡å¼€é”€ç­‰ç°å®ç“¶é¢ˆã€‚", "problem_background": "åŸºç¡€æ¨¡å‹ï¼ˆå¦‚LLMï¼‰å­˜åœ¨é™æ€æ€§ç¼ºé™·ï¼š1ï¼‰çŸ¥è¯†è¿‡æ—¶éœ€æ˜‚è´µé‡è®­ç»ƒï¼ˆGPT-4è®­ç»ƒæˆæœ¬æ•°åƒä¸‡ç¾å…ƒï¼‰ï¼›2ï¼‰æ— æ³•é€‚åº”åŠ¨æ€ç¯å¢ƒï¼ˆå¦‚å®æ—¶ç”¨æˆ·åå¥½ï¼‰ï¼›3ï¼‰é›†ä¸­åŒ–å¯¼è‡´å„æ–­ä¸åè§é£é™©ã€‚æŒç»­å­¦ä¹ èƒ½è§£å†³æ¨¡å‹æ¼‚ç§»ã€ä¸ªæ€§åŒ–ä¸è¶³åŠé«˜ç¢³è¶³è¿¹é—®é¢˜ï¼Œä½†éœ€é‡æ–°å®šä¹‰å…¶åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£çš„è§’è‰²ã€‚", "method": "æå‡ºæŒç»­å­¦ä¹ ä¸‰å¤§æ–¹å‘ï¼š\n1. **æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰**ï¼šå¢é‡æ›´æ–°åŸºç¡€æ¨¡å‹å‚æ•°æ•´åˆæ–°çŸ¥è¯†ï¼Œé‡‡ç”¨è‡ªç›‘ç£ç›®æ ‡ï¼ˆå¦‚æ©ç é¢„æµ‹ï¼‰å‡è½»ç¾éš¾æ€§é—å¿˜\n2. **æŒç»­å¾®è°ƒï¼ˆCFTï¼‰**ï¼šè½»é‡çº§ä»»åŠ¡é€‚é…æŠ€æœ¯ï¼ˆLoRA/Adapterï¼‰ï¼Œä»…æ›´æ–°éƒ¨åˆ†å‚æ•°å®ç°é¢†åŸŸä¸ªæ€§åŒ–\n3. **æŒç»­ç»„åˆä¸ç¼–æ’ï¼ˆCCOï¼‰**ï¼šåŠ¨æ€åè°ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆå¦‚MoEæ¶æ„ï¼‰ï¼Œé€šè¿‡è§’è‰²åˆ†å·¥ï¼ˆä¸“å®¶æ¨¡å—ï¼‰å’Œè‡ªç„¶è¯­è¨€é€šä¿¡è§£å†³å¤æ‚ä»»åŠ¡", "experiment": "**æ–¹æ³•è®ºç¼ºé™·**ï¼š\n- æ— åŸåˆ›å®éªŒï¼Œä»…æ–‡çŒ®ç»¼è¿°ï¼ˆå¦‚å¼•ç”¨LLaMAå‚æ•°æ•ˆç‡ã€LoRAé—å¿˜ç‡ç­‰äºŒæ‰‹æ•°æ®ï¼‰\n- æœªéªŒè¯å…³é”®ä¸»å¼ ï¼šCCOç›¸æ¯”å•ä½“æ¨¡å‹çš„ä¼˜åŠ¿ç¼ºä¹å®è¯ï¼ˆå¦‚æœªå¯¹æ¯”ARC-AGIåŸºå‡†è¡¨ç°ï¼‰\n- å®éªŒè®¾ç½®ç‰‡é¢ï¼šå¿½ç•¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€šä¿¡å»¶è¿Ÿ/èƒ½è€—ç­‰æ ¸å¿ƒæŒ‡æ ‡\n**æœ‰æ•ˆæ€§å­˜ç–‘**ï¼š\n1. CPTä¾èµ–æœªç»éªŒè¯çš„'ç¨³å®šæ€§å·®è·'ç†è®ºï¼ˆåˆå§‹æ€§èƒ½ä¸‹é™ç°è±¡ï¼‰\n2. CFTçš„éšç§ä¿æŠ¤æ–¹æ¡ˆï¼ˆè”é‚¦å­¦ä¹ ï¼‰æœªè€ƒè™‘åƒäº¿å‚æ•°ä¼ è¾“å¼€é”€\n3. CCOé”™è¯¯ä¼ æ’­é£é™©ï¼ˆå¦‚æ™ºèƒ½ä½“å¹»è§‰æ”¾å¤§ï¼‰æœªé‡åŒ–è¯„ä¼°", "one_sentence_summary": "æœ¬æ–‡è®ºè¯åŸºç¡€æ¨¡å‹æ—¶ä»£æŒç»­å­¦ä¹ ä»ä¸å¯æˆ–ç¼ºï¼Œæå‡ºæŒç»­é¢„è®­ç»ƒã€æŒç»­å¾®è°ƒä¸æŒç»­ç»„åˆä¸‰å¤§æ–¹å‘ï¼Œä¸»å¼ åŠ¨æ€ç¼–æ’å¤šæ™ºèƒ½ä½“çš„æŒç»­ç»„åˆèŒƒå¼å°†ä¸»å¯¼æœªæ¥å»ä¸­å¿ƒåŒ–AIç”Ÿæ€ã€‚", "slug": "continual-learning-foundation-models-future", "keywords": ["Continual Learning", "Adaptive Systems", "Foundation Model", "Pre-training", "Fine-tuning", "Multi-Agent"], "further_thoughts": "ä½œè€…å°†CCOç±»æ¯”'ç¤¾ä¼šåä½œ'å­˜åœ¨ä¸¥é‡ç®€åŒ–ï¼š1ï¼‰å¤šæ™ºèƒ½ä½“åè°ƒéœ€è§£å†³çº³ä»€å‡è¡¡é—®é¢˜ï¼ˆå¦‚ä»»åŠ¡åˆ†é…å†²çªï¼‰ï¼Œä½†æœªå¼•å…¥åšå¼ˆè®ºæœºåˆ¶ï¼›2ï¼‰å¼ºè°ƒ'å»ä¸­å¿ƒåŒ–'å´å¿½è§†åŒºå—é“¾éªŒè¯ç­‰åº•å±‚æ¶æ„ï¼›3ï¼‰ä¸ç¥ç»ç¬¦å·AIç»“åˆå¯èƒ½æ›´é«˜æ•ˆï¼ˆå¦‚ç”¨ç¬¦å·æ¨¡å—çº¦æŸé”™è¯¯ä¼ æ’­ï¼‰ã€‚æœªæ¥éœ€åœ¨ç”ŸåŒ–è®¡ç®—ç­‰ä½åŠŸè€—åœºæ™¯éªŒè¯CCOå¯è¡Œæ€§ï¼Œé¿å…ç®—åŠ›éœ€æ±‚æ‚–ç¦»å¯æŒç»­å‘å±•ä¸»å¼ ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03320", "preference": "unknown", "summary_time": "2025-06-05T07:03:33.585061+00:00", "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "abstract": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.", "date": "2025-06-05", "categories": ["cs.LG", "cs.AI"], "created": "2025-06-03", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8882130776320888, "show": 1}
{"institution": ["University of Maryland", "University of Michigan", "Princeton University", "Amazon AGI", "University of Central Florida"], "reasoning_step": "æœ¬æ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼šåœ¨æ¨ç†æ¨¡å‹ä¸­ï¼Œå»¶é•¿æ€è€ƒæ—¶é—´ï¼ˆå¦‚æ·»åŠ 'Wait'æç¤ºï¼‰ä¼šå¯¼è‡´è¾“å‡ºæ–¹å·®å¢åŠ ï¼Œå½¢æˆæ€§èƒ½å…ˆå‡åé™çš„éå•è°ƒç°è±¡ã€‚è¿™ä¸€ç»“è®ºåŸºäºé«˜æ–¯æ¦‚ç‡æ¨¡å‹çš„ç®€åŒ–å‡è®¾ï¼Œä½†å®é™…è¯­è¨€æ¨¡å‹æ˜¯é«˜ç»´ç¦»æ•£åˆ†å¸ƒï¼Œè¯¥å‡è®¾å¯èƒ½è¿‡åº¦ç®€åŒ–äº†å¤æ‚æ€§ã€‚ä½œè€…æå‡ºçš„'å¹¶è¡Œæ€è€ƒ'æ–¹æ¡ˆæœ¬è´¨ä¸Šæ˜¯Best-of-Né‡‡æ ·çš„å˜ä½“ï¼Œç¼ºä¹ä¸ç°æœ‰æ–¹æ³•ï¼ˆå¦‚è‡ªæ´½æ€§é‡‡æ ·ï¼‰çš„å¯¹æ¯”å®éªŒã€‚æ­¤å¤–ï¼Œå®éªŒä»…ä½¿ç”¨1.5B-8Bè’¸é¦æ¨¡å‹ï¼ŒæœªéªŒè¯åœ¨æ›´å¤§è§„æ¨¡åŸå§‹æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1æœ¬ä½“ï¼‰ä¸Šçš„æ™®é€‚æ€§ã€‚å€¼å¾—æ·±ç©¶çš„æ˜¯ï¼šæ–¹å·®å¢åŠ æ˜¯å¦å¿…ç„¶å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Ÿåœ¨MATHç­‰é«˜éš¾åº¦æ•°æ®é›†ä¸­ï¼Œå¢åŠ æ¢ç´¢æ€§æ˜¯å¦å¯èƒ½æœ‰ç›Šï¼Ÿ", "problem_background": "å½“å‰æµè¡Œè§‚ç‚¹è®¤ä¸ºå»¶é•¿æ¨ç†æ¨¡å‹çš„æ€è€ƒè½¨è¿¹ï¼ˆå¦‚æ·»åŠ 'Wait'æç¤ºï¼‰èƒ½æå‡æ€§èƒ½ã€‚æœ¬æ–‡è´¨ç–‘è¯¥è§‚ç‚¹ï¼Œé€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼šå¢åŠ æ€è€ƒé•¿åº¦ä¼šå…ˆæå‡åé™ä½å‡†ç¡®ç‡ï¼ˆè¿‡æ€è€ƒç°è±¡ï¼‰ã€‚æ ¸å¿ƒé—®é¢˜æ˜¯æ­ç¤ºè¯¥éå•è°ƒè¶‹åŠ¿çš„æœ¬è´¨åŸå› ï¼Œå¹¶æ¢ç´¢æ›´ä¼˜çš„æµ‹è¯•æ—¶è®¡ç®—èµ„æºåˆ†é…ç­–ç•¥ã€‚", "method": "*   è¿‡æ€è€ƒå®è¯åˆ†æï¼šé‡‡ç”¨ä¸¤ç§æµ‹è¯•æ—¶é¢„ç®—æ§åˆ¶æ–¹æ³•â€”â€”(1) 'Wait & Think more'ï¼šæŠ‘åˆ¶ç»ˆæ­¢ç¬¦ï¼Œè¿­ä»£æ·»åŠ 'Wait'å»¶é•¿æ€è€ƒï¼›(2) 'Exact thinking tokens'ï¼šç²¾ç¡®æ§åˆ¶æ€è€ƒtokenæ•°é‡ã€‚\n*   æ–¹å·®é©±åŠ¨è§£é‡Šï¼šæå‡ºç®€åŒ–æ¦‚ç‡æ¨¡å‹ï¼ˆå‡è®¾è¾“å‡ºæœä»é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œè¯æ˜æ–¹å·®å¢åŠ ä¼šå…ˆæ‰©å¤§å¥–åŠ±å‡½æ•°è¦†ç›–ï¼ˆæ€§èƒ½å‡ï¼‰åç¨€é‡Šæ¦‚ç‡å¯†åº¦ï¼ˆæ€§èƒ½é™ï¼‰ã€‚é€šè¿‡æµ‹é‡ç­–ç•¥åˆ†å¸ƒçš„ç†µéªŒè¯æ–¹å·®å¢é•¿ã€‚\n*   å¹¶è¡Œæ€è€ƒæ–¹æ¡ˆï¼šç»™å®šæ€»tokené¢„ç®—Bï¼Œç”ŸæˆNæ¡ç‹¬ç«‹æ¨ç†è·¯å¾„ï¼ˆâˆ‘|zâ½â±â¾|â‰¤Bï¼‰ï¼Œæ¯æ¡è·¯å¾„äº§ç”Ÿç­”æ¡ˆyâ½â±â¾ï¼Œæœ€åé€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ç»ˆç­”æ¡ˆyáµ‡áµ‰Ë¢áµ—ã€‚", "experiment": "*   æ•°æ®é›†ï¼šGSM-8Kï¼ˆ1320æ ·æœ¬ï¼‰ã€MATH-500ï¼ˆ500æ ·æœ¬ï¼‰ã€AIMEï¼ˆ30ç«èµ›é¢˜ï¼‰ã€‚\n*   æ¨¡å‹ï¼šDeepSeek-R1è’¸é¦ç‰ˆï¼ˆ1.5B/7B/8Bï¼‰ã€‚\n*   å…³é”®ç»“æœï¼š\n    - è¿‡æ€è€ƒç°è±¡ï¼šåœ¨ä¸¤ç§é¢„ç®—æ§åˆ¶ä¸‹ï¼Œæ€è€ƒtokenå¢åŠ å‡å¯¼è‡´å‡†ç¡®ç‡å‘ˆå€’Uå‹æ›²çº¿ï¼ˆå¦‚GSM8Kä¸Š1.5Bæ¨¡å‹ï¼štokenä»385å¢è‡³1100æ—¶å‡†ç¡®ç‡ä»82.2%â†’87.3%ï¼Œç»§ç»­å¢è‡³15980åˆ™é™è‡³70.3%ï¼‰ã€‚\n    - æ–¹å·®éªŒè¯ï¼šæ€è€ƒtokenå¢åŠ ä½¿ç†µæå‡12å€ï¼ˆGSM8Kç†µ0.23â†’2.79ï¼‰ï¼Œä¸æ¦‚ç‡æ¨¡å‹é¢„æµ‹ä¸€è‡´ã€‚\n    - å¹¶è¡Œæ€è€ƒä¼˜åŠ¿ï¼šç›¸åŒ16K tokené¢„ç®—ä¸‹ï¼Œç›¸æ¯”åºåˆ—æ€è€ƒçš„11.8%æ€§èƒ½ä¸‹é™ï¼Œå¹¶è¡Œæ€è€ƒå®ç°10.1%æå‡ã€‚\n*   å®éªŒç¼ºé™·ï¼š\n    - ä»…å¯¹æ¯”è‡ªå»ºåŸºçº¿ï¼Œæœªä¸è‡ªæ´½æ€§é‡‡æ ·ç­‰æ ‡å‡†æ–¹æ³•æ¯”è¾ƒï¼›\n    - ç†µæµ‹é‡åŸºäºç›¸åŒpromptçš„å¤šæ¬¡é‡‡æ ·ï¼Œä½†å®é™…æ‰©å±•æ€è€ƒä¼šæ”¹å˜ä¸Šä¸‹æ–‡ï¼Œæµ‹é‡æ–¹æ³•å­˜ç–‘ï¼›\n    - å°è§„æ¨¡è’¸é¦æ¨¡å‹ç»“è®ºæ˜¯å¦é€‚ç”¨äºåŸå§‹å¤§æ¨¡å‹æœªç»éªŒè¯ã€‚", "one_sentence_summary": "æœ¬æ–‡æ­ç¤ºæ¨ç†æ¨¡å‹æµ‹è¯•æ—¶å»¶é•¿æ€è€ƒä¼šå¯¼è‡´è¾“å‡ºæ–¹å·®å¢åŠ å¼•å‘'è¿‡æ€è€ƒ'ç°è±¡ï¼Œæå‡ºé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„çš„é¢„ç®—åˆ†é…ç­–ç•¥ï¼Œåœ¨å›ºå®šè®¡ç®—èµ„æºä¸‹å®ç°æœ€é«˜22%çš„æ€§èƒ½æå‡ã€‚", "slug": "overthinking-parallel-thinking-reasoning", "keywords": ["Variance", "Parallel Sampling", "Test Time", "Efficiency", "Reasoning", "Entropy", "Scaling"], "further_thoughts": "è¿‡æ€è€ƒç°è±¡å¯èƒ½ä¸ä»»åŠ¡å¤æ‚åº¦ç›¸å…³ï¼šåœ¨GSM8Kç­‰ç®€å•ä»»åŠ¡ä¸­æ–¹å·®å¢åŠ ä¸»è¦å¼•å…¥å™ªå£°ï¼Œä½†åœ¨éœ€è¦å‘æ•£æ€ç»´çš„åˆ›æ„æ¨ç†ä¸­æˆ–æœ‰ç›Šå¤„ã€‚å¹¶è¡Œæ€è€ƒæœ¬è´¨ä¸Šæ˜¯ç”¨ç©ºé—´æ¢ç¨³å®šæ€§ï¼Œæœªæ¥å¯ç»“åˆæ—©åœæœºåˆ¶ï¼ˆå¦‚åŸºäºæ–¹å·®çš„åŠ¨æ€ç»ˆæ­¢ï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–èµ„æºåˆ†é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œäººç±»æ¨ç†åŒæ ·å­˜åœ¨'è¿‡åº¦åˆ†æç˜«ç—ª'ç°è±¡ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºç±»äººæ¨ç†çš„AIæ¨¡å‹æä¾›äº†è®¤çŸ¥ç§‘å­¦è§†è§’çš„å¯å‘ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.04210", "preference": "unknown", "summary_time": "2025-06-05T07:04:22.461739+00:00", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.", "date": "2025-06-05", "categories": ["cs.AI", "cs.CL"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8526188594042221, "show": 1}

{"institution": ["Institute of Automation, Chinese Academy of Sciences", "University of Chinese Academy of Sciences", "Meituan"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ è¾…åŠ©é›†æˆæ¡†æ¶RLAEï¼Œç”¨äºåŠ¨æ€è°ƒæ•´å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é›†æˆæƒé‡ã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºå°†é›†æˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ï¼Œé‡‡ç”¨spançº§é›†æˆç­–ç•¥å¹³è¡¡ç»†ç²’åº¦å’Œæ•ˆç‡ã€‚è®ºæ–‡å£°ç§°åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€é«˜3.3%çš„æ€§èƒ½æå‡ï¼Œå¹¶å…·å¤‡è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚ä½†éœ€è¦æ‰¹åˆ¤æ€§å®¡è§†å‡ ä¸ªå…³é”®ç‚¹ï¼š1) 3.3%çš„æ˜¾è‘—æå‡ä»…åœ¨ç‰¹å®šä»»åŠ¡(GPQA)å’Œç‰¹å®šåŸºçº¿(DeePEn)å¯¹æ¯”ä¸­å‡ºç°ï¼Œå…¶ä»–ä»»åŠ¡æå‡å¹…åº¦è¾ƒå°(å¦‚MBPPä»…0.6-1.2%)ï¼›2) RLæ™ºèƒ½ä½“é‡‡ç”¨400Må‚æ•°çš„DeBERTa-V3-Largeï¼Œæ˜¾è‘—å¢åŠ è®¡ç®—å¼€é”€å´æœªé‡åŒ–è®­ç»ƒæˆæœ¬ï¼›3) ç¨€ç–å¥–åŠ±è®¾è®¡å¯èƒ½é™åˆ¶è®­ç»ƒæ•ˆç‡ï¼›4) è·¨ä»»åŠ¡æ³›åŒ–å®éªŒä»…æµ‹è¯•MMLUâ†’ARC-Cè¿ç§»ï¼Œè¦†ç›–èŒƒå›´ä¸è¶³ï¼›5) æœªä¸è¿‘æœŸæ··åˆä»£ç†(MoA)ç­‰å…ˆè¿›æ–¹æ³•å¯¹æ¯”ã€‚", "problem_background": "ç°æœ‰LLMé›†æˆæ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼š1) åŸºäºæ’åºå™¨çš„æ–¹æ³•(å¦‚PairRanker)éœ€è¦äºŒæ¬¡è®¡ç®—å¤æ‚åº¦ä¸”ä»…æ”¯æŒå“åº”çº§ç²—ç²’åº¦é›†æˆï¼›2) å¯å‘å¼åŠ æƒæ–¹æ³•(å¦‚GaC, DeePEn)ä½¿ç”¨å›ºå®šæƒé‡ç­–ç•¥ï¼Œæ— æ³•é€‚åº”ä¸åŒè¾“å…¥è¯­å¢ƒ(å¦‚æ•°å­¦è¯æ˜vsæ•…äº‹åˆ›ä½œ)å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œå¯¼è‡´å±€éƒ¨æœ€ä¼˜ä½†å…¨å±€æ¬¡ä¼˜ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åŠ¨æ€æ•æ‰ä¸åŒLLMçš„è¯­å¢ƒç›¸å…³èƒ½åŠ›å·®å¼‚ï¼Œå¦‚GPT-4oæ“…é•¿æ•°å­¦æ¨ç†è€ŒClaudeç²¾äºä»£ç ç”Ÿæˆã€‚", "method": "RLAEæ¡†æ¶å°†LLMé›†æˆå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š\n- **çŠ¶æ€(s_t)**ï¼šè¾“å…¥æç¤º+å·²ç”Ÿæˆå“åº”å†å²\n- **åŠ¨ä½œ(a_t)**ï¼šç¡®å®šå„æ¨¡å‹åœ¨ä¸‹ä¸€spançš„æƒé‡åˆ†å¸ƒ(âˆ‘wâ‚–=1)\n- **å¥–åŠ±(r)**ï¼šç»ˆç«¯çŠ¶æ€ä½¿ç”¨ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡(å¦‚å‡†ç¡®ç‡)ï¼Œä¸­é—´çŠ¶æ€å¯é‡‡ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹\n\né‡‡ç”¨**spançº§é›†æˆç­–ç•¥**ï¼šå°†è¿ç»­Lä¸ªtokenä½œä¸ºåŸºæœ¬å•å…ƒ(L=4)ï¼Œåœ¨spanèµ·å§‹å¤„è°ƒæ•´æƒé‡å¹¶ä¿æŒè‡³spanç»“æŸï¼Œå¤§å¹…é™ä½å†³ç­–é¢‘ç‡(ä»Hæ¬¡é™è‡³H/Læ¬¡)ã€‚è§£å†³è¯è¡¨ä¸åŒ¹é…é—®é¢˜ï¼šé€šè¿‡æ˜ å°„çŸ©é˜µå°†ä¸åŒLLMæ¦‚ç‡å‘é‡æŠ•å½±åˆ°ç»Ÿä¸€è¯è¡¨ç©ºé—´ã€‚å®ç°ä¸¤ç§å˜ä½“ï¼š\n1. **RLAE_PPO**ï¼šå•æ™ºèƒ½ä½“PPOæ§åˆ¶æ‰€æœ‰æƒé‡\n2. **RLAE_MAPPO**ï¼šå¤šæ™ºèƒ½ä½“æ¡†æ¶(MAPPO)ï¼Œå„LLMå¯¹åº”ç‹¬ç«‹æ™ºèƒ½ä½“è¾“å‡ºlogitå€¼ï¼Œç»Softmaxç”Ÿæˆæƒé‡ï¼Œå…±äº«é›†ä¸­å¼è¯„è®ºå®¶åè°ƒå…¨å±€å¥–åŠ±ã€‚", "experiment": "åœ¨7ä¸ªåŸºå‡†æµ‹è¯•(MMLU/ARC-C/TriviaQA/GSM8K/PIQA/GPQA/MBPP)è¯„ä¼°ï¼Œä½¿ç”¨5ä¸ª7B-8Bçº§LLMåŸºæ¨¡å‹ï¼š\n- **æ€§èƒ½**ï¼šåœ¨å¤šæ•°ä»»åŠ¡ä¼˜äºåŸºçº¿ï¼Œæœ€é«˜æå‡3.3%(GPQAä¸Šè¾ƒDeePEn)ï¼Œä½†æå‡å¹…åº¦ä¾èµ–ä»»åŠ¡å’ŒåŸºæ¨¡å‹ï¼š\n  - æ¨¡å‹æ€§èƒ½ç›¸è¿‘æ—¶æå‡æ˜¾è‘—(MMLU: +2.3%)\n  - åŸºæ¨¡å‹å·®è·å¤§æ—¶æå‡æœ‰é™(æ•´åˆQwen-2.5æ—¶MMLUä»…+1.9%)\n  - MAPPOåœ¨æ¨¡å‹å·®å¼‚å¤§æ—¶ä¼˜äºPPOï¼ŒPPOåœ¨éœ€å…¨å±€ä¸€è‡´æ€§çš„ä»£ç ä»»åŠ¡(MBPP)æ›´ä¼˜\n- **æ•ˆç‡**ï¼šå»¶è¿Ÿä¸GaCç›¸å½“ï¼Œæ˜¾è‘—ä½äºPairRanker/DeePEn(å› spançº§ä¼˜åŒ–)\n- **æ³›åŒ–æ€§**ï¼šMMLUâ†’ARC-Cè¿ç§»æµ‹è¯•ä¸­æ€§èƒ½é™å¹…ä»…0.4-0.6ç‚¹ï¼Œè¿œä¼˜äºPairRanker(-3.8ç‚¹)\n- **æ¶ˆèå®éªŒ**ï¼š\n  - RLåŠ æƒç­–ç•¥æ˜¾è‘—ä¼˜äºå›ºå®šæƒé‡(å‡åŒ€/å›°æƒ‘åº¦åŠ æƒ)\n  - spané•¿åº¦=4æ—¶æ•ˆæœæœ€ä½³\n\n**å®éªŒç¼ºé™·**ï¼š1) 3.3%å³°å€¼æå‡ä»…å‡ºç°äºç‰¹å®šä»»åŠ¡-åŸºçº¿ç»„åˆ 2) æœªé‡åŒ–RLæ™ºèƒ½ä½“(400Må‚æ•°)çš„è®­ç»ƒå¼€é”€ 3) ç¨€ç–å¥–åŠ±æœªå……åˆ†éªŒè¯ 4) è·¨ä»»åŠ¡æ³›åŒ–æµ‹è¯•è¦†ç›–ä¸è¶³", "one_sentence_summary": "æå‡ºå¼ºåŒ–å­¦ä¹ è¾…åŠ©çš„LLMé›†æˆæ¡†æ¶RLAEï¼Œé€šè¿‡å°†é›†æˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¹¶é‡‡ç”¨spançº§åŠ¨æ€æƒé‡è°ƒæ•´ï¼Œç»“åˆå•æ™ºèƒ½ä½“(PPO)æˆ–å¤šæ™ºèƒ½ä½“(MAPPO)ç®—æ³•ï¼Œåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå®ç°æœ€é«˜3.3%çš„æ€§èƒ½æå‡å¹¶å…·å¤‡è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚", "slug": "rl-assisted-ensemble-llm", "keywords": ["Reinforcement Learning", "Ensemble", "Foundation Model", "Multi-Agent", "Reasoning", "Efficiency"], "further_thoughts": "å€¼å¾—æ·±å…¥æ¢è®¨çš„æ–¹å‘ï¼š1) è®¡ç®—å¼€é”€ä¸æ€§èƒ½çš„å¸•ç´¯æ‰˜å‰æ²¿â€”â€”400Mçš„RLæ™ºèƒ½ä½“æ˜¯å¦å¿…è¦ï¼Ÿè½»é‡åŒ–æ–¹æ¡ˆå¦‚LSTMæ™ºèƒ½ä½“æˆ–çŸ¥è¯†è’¸é¦å‹ç¼©ç­–ç•¥å€¼å¾—æ¢ç´¢ 2) å¥–åŠ±å‡½æ•°è®¾è®¡çš„æ™®é€‚æ€§ï¼šå½“å‰åŸºäºå‡†ç¡®ç‡çš„ç¨€ç–å¥–åŠ±å¯èƒ½ä¸é€‚ç”¨å¼€æ”¾ç”Ÿæˆä»»åŠ¡ï¼Œéœ€ç»“åˆLLM-basedè¯„ä¼°å™¨æˆ–äººç±»åå¥½åé¦ˆ 3) ä¸æ··åˆä»£ç†(MoA)çš„å¯¹æ¯”ï¼šMoAåŒæ ·é‡‡ç”¨åˆ†å±‚åè°ƒæœºåˆ¶ï¼Œä½†ä¾§é‡ä¸“å®¶é€‰æ‹©è€Œéæƒé‡è°ƒæ•´ï¼Œä¸¤è€…ç»“åˆå¯èƒ½äº§ç”ŸååŒæ•ˆåº” 4) åŠ¨æ€spané•¿åº¦æœºåˆ¶ï¼šå½“å‰å›ºå®šspané•¿åº¦é™åˆ¶é€‚åº”æ€§ï¼Œå¯å¼•å…¥åŸºäºå›°æƒ‘åº¦çš„è‡ªé€‚åº”spanè°ƒæ•´ 5) ç†è®ºè§£é‡Šæ€§ï¼šæƒé‡åˆ†é…å†³ç­–ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯ç»“åˆæ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–å†³ç­–ä¾æ®ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.00439", "preference": "unknown", "summary_time": "2025-06-06T07:05:43.245150+00:00", "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs", "authors": ["Yuqian Fu", "Yuanheng Zhu", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Dongbin Zhao"], "abstract": "Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\\text{RLAE}_\\text{PPO}$ and $\\text{RLAE}_\\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.", "date": "2025-06-03", "categories": ["cs.LG", "cs.AI"], "created": "2025-05-31", "updated": "2025-06-03", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8750975253213766, "show": 1}
{"institution": ["National University of Singapore", "Nanyang Technological University", "Institute for Infocomm Research (I2R), A*STAR", "Salesforce AI Research"], "reasoning_step": "é¦–å…ˆç¡®è®¤è®ºæ–‡æ ¸å¿ƒä¸»å¼ ï¼šICLåœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œéœ€è¦æ˜¾å¼æŒ‡å¯¼åŸåˆ™ã€‚ä½œè€…æå‡ºLongGuideæ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆåº¦é‡å‡†åˆ™ï¼ˆMGï¼‰å’Œè¾“å‡ºçº¦æŸå‡†åˆ™ï¼ˆOCGï¼‰æ¥æå‡æ¨¡å‹è¡¨ç°ã€‚éœ€éªŒè¯ï¼š1) ICLä¸è¶³çš„å®è¯æ˜¯å¦å……åˆ†ï¼ˆè¡¨1æ˜¾ç¤ºå…³é”®æŒ‡æ ‡æœªè¾¾100%ï¼‰ï¼›2) LongGuideçš„æœ‰æ•ˆæ€§æ˜¯å¦æ™®é€‚ï¼ˆ7ä¸ªä»»åŠ¡+AlpacaEval2ï¼‰ï¼›3) å®éªŒè®¾è®¡æ˜¯å¦ä¸¥è°¨ï¼ˆæ§åˆ¶å˜é‡ã€å¤šæ¨¡å‹å¯¹æ¯”ï¼‰ã€‚å…³é”®ç–‘ç‚¹ï¼šOCGåœ¨SWiPEæ•°æ®é›†ä¸Šçš„è´Ÿé¢æ•ˆæœï¼ˆè¡¨4ï¼‰ï¼Œä½œè€…å½’å› äºé«˜æ–¹å·®ï¼Œä½†æœªæä¾›ç»Ÿè®¡æ£€éªŒï¼›äººç±»è¯„ä¼°çš„Î±=68.9%ä»…ä¸­ç­‰å¯ä¿¡åº¦ï¼›ç†è®ºåˆ†ææœªè¦†ç›–å®é™…è¾“å‡ºåˆ†å¸ƒã€‚", "problem_background": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­æœ‰æ•ˆï¼Œä½†åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ã€ç¿»è¯‘ï¼‰ä¸­è¡¨ç°ä¸ä½³ã€‚è®ºæ–‡é€šè¿‡å®è¯å’Œç†è®ºåˆ†ææŒ‡å‡ºï¼šå½“LLMæœªå®Œå…¨æ•è·ä»»åŠ¡çš„è¯­è¨€/æ ¼å¼åˆ†å¸ƒæ—¶ï¼Œä»…é ç¤ºä¾‹æ¼”ç¤ºæ— æ³•è§£å†³æ–‡æœ¬å±æ€§ä¼ é€’ï¼ˆPTï¼‰é—®é¢˜â€”â€”å³æ¨¡å‹æ— æ³•åœ¨ç”Ÿæˆä¸­ä¸€è‡´ä¿æŒä»»åŠ¡ç‰¹å®šçš„è¯­è¨€é£æ ¼å’Œè¾“å‡ºæ ¼å¼ï¼ˆå¦‚å¥å­æ•°ã€tokenæ•°ï¼‰ã€‚è¿™å¯¼è‡´ç”Ÿæˆç»“æœä¸çœŸå®åˆ†å¸ƒå­˜åœ¨åå·®ï¼Œå½±å“ä»»åŠ¡æ€§èƒ½ã€‚", "method": "æå‡ºLongGuideæ¡†æ¶ï¼Œé€šè¿‡æœ‰é™è®­ç»ƒæ•°æ®è‡ªåŠ¨ç”Ÿæˆä¸¤ç±»å¹¶è¡ŒæŒ‡å¯¼å‡†åˆ™ï¼š\n1. åº¦é‡å‡†åˆ™ï¼ˆMGï¼‰ï¼šä»27ä¸ªé¢„å®šä¹‰æŒ‡æ ‡æ± ä¸­ç­›é€‰ä»»åŠ¡ç›¸å…³æŒ‡æ ‡ï¼ˆæ­¥éª¤1ï¼‰ï¼Œç”¨LLMè‡ªè¯„ä¼°è®­ç»ƒé›†å¾—åˆ†ï¼ˆæ­¥éª¤2ï¼‰ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„è´¨é‡æ ‡å‡†ï¼ˆæ­¥éª¤3ï¼‰ï¼›\n2. è¾“å‡ºçº¦æŸå‡†åˆ™ï¼ˆOCGï¼‰ï¼šåˆ†æè®­ç»ƒé›†å“åº”çš„token/å¥å­æ•°é‡åˆ†å¸ƒï¼Œæå–æœ€å°å€¼ã€æœ€å¤§å€¼ã€å¹³å‡å€¼ä½œä¸ºæ ¼å¼çº¦æŸï¼ˆæ­¥éª¤4ï¼‰ï¼›\næœ€ç»ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜MG+OCGç»„åˆï¼ˆæ­¥éª¤5ï¼‰ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯å°†ä»»åŠ¡åˆ†å¸ƒæ˜¾å¼ç¼–ç ä¸ºå¯ç†è§£çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œè€Œéä¾èµ–éšå¼ICLã€‚", "experiment": "åœ¨7ä¸ªé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆæ‘˜è¦/ç®€åŒ–/ç¿»è¯‘/å¯¹è¯ç­‰ï¼‰å’ŒAlpacaEval2èŠå¤©åŸºå‡†æµ‹è¯•ï¼š\n- æœ‰æ•ˆæ€§ï¼šç›¸æ¯”é›¶æ ·æœ¬/å°‘æ ·æœ¬åŸºçº¿ï¼ŒLongGuideå¹³å‡æå‡ROUGE-L 6%ã€GPT-4oè¯„åˆ†0.8åˆ†ï¼ˆè¡¨3ï¼‰ï¼ŒJensen-Shannonæ•£åº¦æ˜¾è‘—é™ä½ï¼ˆè¡¨2ï¼‰ï¼ŒéªŒè¯å…¶ç¼“è§£PTé—®é¢˜çš„èƒ½åŠ›ï¼›\n- å±€é™æ€§ï¼šOCGåœ¨SWiPEæ•°æ®é›†æŸå®³æ€§èƒ½ï¼ˆè¡¨4ï¼‰ï¼Œå› ç­”æ¡ˆé•¿åº¦æ–¹å·®å¤§ï¼›AlpacaEval2å®éªŒä½¿ç”¨åˆ†å¸ƒå¤–è®­ç»ƒæ•°æ®ï¼Œå½±å“æ³›åŒ–æ€§ï¼›\n- æ¶ˆèå®éªŒï¼šMG-OCGç»„åˆæœ€æœ‰æ•ˆï¼ˆ15/25æ¡ˆä¾‹æœ€ä¼˜ï¼‰ï¼Œä½†å•ç”¨OCGåœ¨ç¿»è¯‘/è¡¨æ ¼ç”Ÿæˆæ›´ä¼˜ï¼ˆè¡¨4ï¼‰ï¼›\n- äººç±»è¯„ä¼°ï¼šLongGuideåœ¨95%æ¡ˆä¾‹æå‡æ ¼å¼ä¸€è‡´æ€§ï¼Œä½†ç›¸å…³æ€§ä»…12%æå‡ï¼ˆå› åŸºçº¿å·²è¾ƒå¥½ï¼‰ã€‚", "one_sentence_summary": "LongGuideé€šè¿‡è‡ªåŠ¨ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„åº¦é‡å‡†åˆ™å’Œè¾“å‡ºçº¦æŸå‡†åˆ™ï¼Œæ˜¾å¼æŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹å¯¹é½é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯­è¨€å’Œæ ¼å¼åˆ†å¸ƒï¼Œåœ¨7ä¸ªä»»åŠ¡ä¸Šå¹³å‡æå‡ROUGE-L 6%ã€‚", "slug": "longform-generation-alignment-guidelines", "keywords": ["Prompt Engineering", "In-Context Learning", "Controllable Generation", "Text Generation", "Self-Evaluation"], "further_thoughts": "LongGuideçš„å‡†åˆ™ç”Ÿæˆæœ¬è´¨æ˜¯ä»»åŠ¡åˆ†å¸ƒçš„å‹ç¼©è¡¨ç¤ºï¼Œè¿™ç§æ€æƒ³å¯æ‰©å±•åˆ°ï¼š1) å®‰å…¨å¯¹é½é¢†åŸŸâ€”â€”å°†å®‰å…¨å‡†åˆ™è½¬åŒ–ä¸ºå¯é‡åŒ–çš„è‡ªè¯„ä¼°æŒ‡æ ‡ï¼›2) æ¨¡å‹è’¸é¦â€”â€”ä½œä¸ºè½»é‡çº§ä»»åŠ¡æè¿°å™¨æ›¿ä»£å®Œæ•´ç¤ºä¾‹ï¼›3) ç†è®ºç¼ºé™·ï¼šä»…å»ºæ¨¡è¯­è¨€åˆ†å¸ƒP_M(X)ï¼Œæœªè¦†ç›–å®é™…è¾“å‡ºåˆ†å¸ƒargmax P_M(Y|X)ï¼Œæœªæ¥éœ€ç»“åˆé‡‡æ ·ç­–ç•¥åˆ†æã€‚æœ‰è¶£çš„æ˜¯ï¼ŒChatGPTæ¯”Mistralè·ç›Šæ›´å¤§ï¼ˆ+6.58% vs +5.39%ï¼‰ï¼Œæš—ç¤ºè¯¥æ–¹æ³•å¯¹æ›´å¼ºæ¨¡å‹æ½œåŠ›æ›´å¤§ï¼Œæˆ–å› å…¶æŒ‡ä»¤è·Ÿéšèƒ½åŠ›æ›´ä¼˜ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.01265", "preference": "unknown", "summary_time": "2025-06-07T07:05:27.995239+00:00", "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines", "authors": ["Do Xuan Long", "Duong Ngoc Yen", "Do Xuan Trong", "Luu Anh Tuan", "Kenji Kawaguchi", "Shafiq Joty", "Min-Yen Kan", "Nancy F. Chen"], "abstract": "In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.", "date": "2025-06-03", "categories": ["cs.CL"], "created": "2025-06-02", "updated": "2025-06-03", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8609293445363413, "show": 1}
{"institution": ["UC Berkeley"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„éæ³¨æ„åŠ›æœºåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸTransformeråœ¨è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„O(nÂ²)å¤æ‚åº¦é—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºå®Œå…¨æ‘’å¼ƒè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»£ä¹‹ä»¥çŠ¶æ€ç©ºé—´æ¨¡å‹ã€å¤šåˆ†è¾¨ç‡å·ç§¯ã€å¾ªç¯ç›‘ç£å™¨å’Œæ£€ç´¢å¢å¼ºè®°å¿†çš„å››å…ƒç»„åˆã€‚å®éªŒéƒ¨åˆ†æ˜¾ç¤ºåœ¨WikiText-103å’ŒEnwik8åŸºå‡†ä¸Šè¾¾åˆ°SOTAæ°´å¹³ï¼Œä½†å­˜åœ¨ä¸‰ä¸ªå…³é”®ç–‘ç‚¹ï¼š1) å£°ç§°æ”¯æŒç™¾ä¸‡tokençš„å®éªŒä¸­å®é™…ä»…éªŒè¯åˆ°50ä¸‡token 2) ä¸MAMBAç­‰æœ€æ–°SSMæ¨¡å‹çš„å¯¹æ¯”ä¸è¶³ 3) æ£€ç´¢æœºåˆ¶å¯¹å™ªå£°æ•æ„Ÿæ€§çš„æ¶ˆèå®éªŒä¸å……åˆ†ã€‚éœ€è¦ç‰¹åˆ«å…³æ³¨å…¶åˆ†å—å¤„ç†å¯èƒ½å¼•å‘çš„ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–é—®é¢˜ã€‚", "problem_background": "ä¼ ç»ŸTransformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨O(nÂ²)è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ï¼Œå¯¼è‡´å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆæ•°åä¸‡è‡³ç™¾ä¸‡tokenï¼‰æ—¶é¢ä¸´è®¡ç®—ä¸å¯è¡Œæ€§ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆå¦‚ç¨€ç–æ³¨æ„åŠ›ï¼ˆBigBirdï¼‰æˆ–è¿‘ä¼¼æ–¹æ³•ï¼ˆPerformerï¼‰ä»æ— æ³•å½»åº•è§£å†³æç«¯é•¿åº¦ä¸‹çš„æ•ˆç‡ç“¶é¢ˆï¼Œä¸”ä¾èµ–æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨é™åˆ¶äº†å…¶æ‰©å±•æ€§ã€‚è¿™é˜»ç¢äº†æ³•å¾‹æ–‡æ¡£åˆ†æã€ä»£ç åº“ç†è§£ç­‰éœ€è¦æµ·é‡ä¸Šä¸‹æ–‡çš„åº”ç”¨åœºæ™¯ã€‚", "method": "é‡‡ç”¨åˆ†å—å¤„ç†èŒƒå¼ï¼ˆchunk size=1024/2048ï¼‰ï¼Œæ¯ä¸ªå—å†…ä¾æ¬¡åº”ç”¨ï¼š\n1. **çŠ¶æ€ç©ºé—´å—**ï¼šç±»S4çš„è¿ç»­æ—¶é—´å·ç§¯æ ¸ï¼Œä»¥O(c log c)å¤æ‚åº¦æ•è·å—å†…é•¿ç¨‹ä¾èµ–\n2. **å¤šåˆ†è¾¨ç‡å·ç§¯**ï¼šå¹¶è¡Œç©ºæ´å·ç§¯ï¼ˆdilation=1,2,4ï¼‰æå–å¤šå°ºåº¦å±€éƒ¨ç‰¹å¾\n3. **æ£€ç´¢å¢å¼ºè®°å¿†**ï¼šå—åµŒå…¥å‡å€¼æ± åŒ–åå­˜å…¥é”®å€¼åº“ï¼Œé€šè¿‡FAISSè¿‘ä¼¼æœ€è¿‘é‚»æ£€ç´¢å†å²å—ï¼ˆtop-kï¼‰ï¼Œé—¨æ§MLPèåˆæ£€ç´¢ç»“æœ\n4. **å…¨å±€å¾ªç¯ç›‘ç£å™¨**ï¼šGRUå•å…ƒç»´æŠ¤è·¨å—éšè—çŠ¶æ€ï¼ˆh_gâˆˆâ„^{BÃ—512}ï¼‰\n5. æœ€ç»ˆé€šè¿‡çº¿æ€§æŠ•å½±ç”Ÿæˆtokençº§è¾“å‡ºã€‚å…¨ç¨‹é¿å…ä»»ä½•QKâŠ¤Vè®¡ç®—ã€‚", "experiment": "**åŸºå‡†æµ‹è¯•**ï¼š\n- WikiText-103ï¼šå›°æƒ‘åº¦18.7ï¼ˆä¼˜äºBigBirdçš„19.2ï¼‰\n- Enwik8ï¼š1.04 bpcï¼ˆåª²ç¾TransformeråŸºçº¿ï¼‰\n**é•¿ä¸Šä¸‹æ–‡éªŒè¯**ï¼š\n- åˆæˆæ–‡æœ¬ï¼ˆLSTï¼‰ä¸­æˆåŠŸè¿½è¸ª10ä¸‡tokenè·¨åº¦çš„æ¨¡å¼ä¾èµ–\n- ä¹¦ç±æ‘˜è¦ä»»åŠ¡ï¼ˆ20ä¸‡tokenï¼‰æ˜¾å­˜å ç”¨ä»…çº¿æ€§å¢é•¿\n**å…³é”®ç¼ºé™·**ï¼š\n1. è¶…é•¿ä¸Šä¸‹æ–‡å®éªŒä»…è¾¾50ä¸‡tokenï¼ŒæœªéªŒè¯ç™¾ä¸‡çº§æ‰¿è¯º\n2. æ£€ç´¢æœºåˆ¶æ¶ˆèå®éªŒä¸è¶³ï¼šæœªæµ‹è¯•ç›¸ä¼¼åº¦é˜ˆå€¼å˜åŒ–å¯¹æ€§èƒ½å½±å“\n3. ä¸MAMBAç­‰æœ€æ–°SSMå¯¹æ¯”ç¼ºå¤±ï¼Œè¡¨2ä¸­S4-onlyåŸºçº¿è®¾ç½®ä¸åˆç†ï¼ˆ8kä¸Šä¸‹æ–‡ vs æœ¬æ–‡32kï¼‰\n4. å®é™…ååé‡æ•°æ®æœªæŠ¥å‘Šï¼Œæ£€ç´¢å»¶è¿Ÿå¯èƒ½æˆä¸ºç“¶é¢ˆ", "one_sentence_summary": "é€šè¿‡èåˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€å¤šåˆ†è¾¨ç‡å·ç§¯ã€å¾ªç¯ç›‘ç£å™¨ä¸æ£€ç´¢å¢å¼ºè®°å¿†çš„çº¯éæ³¨æ„åŠ›æ¶æ„ï¼Œå®ç°è¿‘çº¿æ€§å¤æ‚åº¦çš„ç™¾ä¸‡çº§ä¸Šä¸‹æ–‡å¤„ç†ï¼Œåœ¨WikiText-103è¾¾åˆ°18.7å›°æƒ‘åº¦ã€‚", "slug": "non-attention-llm-ultra-long-context", "keywords": ["State Space Model", "CNN", "RNN", "RAG", "Efficiency", "Large Language Model"], "further_thoughts": "è¯¥æ¶æ„å½»åº•æ‘’å¼ƒæ³¨æ„åŠ›çš„è®¾è®¡æå…·å¯å‘æ€§ï¼Œä½†åˆ†å—å¤„ç†å¯èƒ½å¯¼è‡´è·¨å—é•¿ç¨‹ä¾èµ–æ–­è£‚â€”â€”è™½ç„¶é€šè¿‡å¾ªç¯çŠ¶æ€å’Œæ£€ç´¢æœºåˆ¶ç¼“è§£ï¼Œä½†å½“å…³é”®ä¿¡æ¯ä½äºå—è¾¹ç•Œæˆ–æ£€ç´¢è¿”å›å™ªå£°ç‰‡æ®µæ—¶ä»å¯èƒ½å¤±æ•ˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ç§'åˆ†æ²»+è®°å¿†æ£€ç´¢'èŒƒå¼ä¸äººè„‘å¤„ç†é•¿æ–‡æœ¬çš„æœºåˆ¶ç›¸ä¼¼ï¼ˆåˆ†å—è®°å¿†+è”æƒ³æå–ï¼‰ï¼Œå»ºè®®å€Ÿé‰´è®¤çŸ¥ç§‘å­¦ä¸­çš„'æƒ…å¢ƒè®°å¿†'ç†è®ºä¼˜åŒ–æ£€ç´¢ç­–ç•¥ã€‚ä¸æ··åˆä¸“å®¶ç³»ç»Ÿï¼ˆMoEï¼‰ç»“åˆå¯èƒ½çªç ´å½“å‰ç“¶é¢ˆï¼šè®©ä¸“å®¶ç½‘ç»œåˆ†åˆ«å¤„ç†ä¸åŒè¯­ä¹‰å—ï¼Œé€šè¿‡è·¯ç”±æœºåˆ¶æ›¿ä»£å½“å‰å‡å€¼æ± åŒ–ï¼Œæˆ–è®¸èƒ½æå‡å—åµŒå…¥çš„åˆ¤åˆ«æ€§ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.01963", "preference": "unknown", "summary_time": "2025-06-07T07:04:53.884595+00:00", "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "abstract": "We present a novel non attention based architecture for large language models (LLMs) that efficiently handles very long context windows, on the order of hundreds of thousands to potentially millions of tokens. Unlike traditional Transformer designs, which suffer from quadratic memory and computation overload due to the nature of the self attention mechanism, our model avoids token to token attention entirely. Instead, it combines the following complementary components: State Space blocks (inspired by S4) that learn continuous time convolution kernels and scale near linearly with sequence length, Multi Resolution Convolution layers that capture local context at different dilation levels, a lightweight Recurrent Supervisor to maintain a global hidden state across sequential chunks, and Retrieval Augmented External Memory that stores and retrieves high-level chunk embeddings without reintroducing quadratic operations.", "date": "2025-06-04", "categories": ["cs.LG", "cs.CL"], "created": "2025-05-09", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8946353061189664, "show": 1}
{"institution": [], "reasoning_step": "ä½œè€…Edward Y. Changæå‡ºUCCTç†è®ºï¼Œè¯•å›¾ç”¨è®¤çŸ¥ç§‘å­¦æ¡†æ¶è§£é‡ŠLLMçš„å°‘æ ·æœ¬å­¦ä¹ æ‚–è®ºã€‚æ ¸å¿ƒè§‚ç‚¹æ˜¯å°†LLMè§†ä¸ºæ— æ„è¯†æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šæ¿€æ´»ä»»åŠ¡èƒ½åŠ›ã€‚ä½†ç†è®ºå­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼šæ•°å­¦å…¬å¼ï¼ˆå¦‚é˜ˆå€¼æ–¹ç¨‹ï¼‰ç¼ºä¹å®è¯æ”¯æŒï¼Œæ¡ˆä¾‹ç ”ç©¶ä»…ç”¨æ¼”ç¤ºä»£æ›¿ä¸¥æ ¼å®éªŒï¼›ç±»æ¯”äººè„‘è®¤çŸ¥ï¼ˆå¦‚å…¨å±€å·¥ä½œç©ºé—´ç†è®ºï¼‰è¿‡åº¦ç®€åŒ–LLMæœºåˆ¶ï¼›æœ¯è¯­å¦‚æ¨¡å¼å¯†åº¦Ï(P)å’Œè¡¨å¾å·®è·d_r(P,T)æœªå®šä¹‰å¯æµ‹é‡æ–¹æ³•ã€‚éœ€è­¦æƒ•ä½œè€…ç”¨å“²å­¦åŒ…è£…æ©ç›–ç†è®ºç©ºæ´æ€§ã€‚", "problem_background": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ å­˜åœ¨æ‚–è®ºï¼šéƒ¨åˆ†ä»»åŠ¡ä»…éœ€å°‘é‡ç¤ºä¾‹å³å¯æ³›åŒ–ï¼Œè€Œå…¶ä»–ä»»åŠ¡éœ€å¤§é‡ç›‘ç£ã€‚æ‰¹è¯„è€…ï¼ˆå¦‚LeCunã€Marcusï¼‰æŒ‡æ‘˜LLMsç¼ºä¹çœŸæ­£è®¤çŸ¥èƒ½åŠ›ï¼Œä»…å±ç»Ÿè®¡æ¨¡å¼åŒ¹é…ã€‚æœ¬æ–‡è¯•å›¾é€šè¿‡ç»Ÿä¸€è®¤çŸ¥æ„è¯†ç†è®ºï¼ˆUCCTï¼‰è§£é‡Šæ­¤ç°è±¡ï¼šå°†LLMsé‡æ–°å®šä¹‰ä¸ºæ— æ„è¯†åŸºè´¨â€”â€”é¢„è®­ç»ƒå½¢æˆçš„æ½œåœ¨æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šï¼ˆæç¤ºã€è§’è‰²åˆ†é…ç­‰ï¼‰æ¿€æ´»ä»»åŠ¡ç›¸å…³è¯­ä¹‰ï¼Œå¹¶å£°ç§°è¯¥æ¡†æ¶èƒ½ç»Ÿä¸€è§£é‡Šæç¤ºå·¥ç¨‹ã€å¾®è°ƒã€RAGç­‰æ–¹æ³•ã€‚", "method": "æå‡ºä¸‰åŸåˆ™ç†è®ºæ¡†æ¶ï¼š\n1. æ¨¡å¼å­˜å‚¨åº“åŸåˆ™ï¼šé¢„è®­ç»ƒå½¢æˆæœªæ ‡è®°çš„æ½œåœ¨æ¨¡å¼åˆ†å¸ƒ\n2. è¯­ä¹‰é”šå®šåŸåˆ™ï¼šé€šè¿‡æç¤º/å¾®è°ƒ/RAGç­‰å¤–éƒ¨çº¦æŸæ¿€æ´»ç‰¹å®šæ¨¡å¼\n3. é˜ˆå€¼è·¨è¶ŠåŸåˆ™ï¼šé”šå®šå¼ºåº¦è¶…è¶Šä¸´ç•Œå€¼åå¼•å‘èƒ½åŠ›ç›¸å˜ï¼ˆç±»æ¯”ReLUæ¿€æ´»ï¼‰\næ•°å­¦å½¢å¼åŒ–ä¸ºè´å¶æ–¯æ··åˆæ¨¡å‹ï¼šp(y|ğ’œ,C) = âˆ« p(y|P,ğ’œ) p(P|ğ’œ,C) dPï¼Œå…¶ä¸­é”šå®šå¼ºåº¦Î±(ğ’œ) = Î±Ï(P) - Î²d_r(P,T) - Î³log kã€‚å®£ç§°è¯¥å…¬å¼ç»Ÿä¸€æ¶µç›–å°‘æ ·æœ¬æç¤ºã€å¾®è°ƒï¼ˆè°ƒæ•´å…ˆéªŒåˆ†å¸ƒï¼‰ã€RAGï¼ˆå¤–éƒ¨æ¨¡å¼æ³¨å…¥ï¼‰ç­‰åœºæ™¯ã€‚", "experiment": "å®éªŒè®¾è®¡å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼š\n1. ä»…å››ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼ˆå°‘æ ·æœ¬ç®—æœ¯ã€è§’è‰²è¾©è®ºã€å¾®è°ƒåŒ»ç–—è¯Šæ–­ã€RAGé‡å­è®¡ç®—ï¼‰ï¼Œå‡ä½¿ç”¨å•†ä¸šAPIï¼ˆClaude/GPTç­‰ï¼‰æ¼”ç¤ºï¼Œç¼ºä¹æ§åˆ¶ç»„ä¸ç»Ÿè®¡æ£€éªŒ\n2. ç»“æœå®šæ€§æè¿°ä¸ºä¸»ï¼ˆå¦‚ç®—æœ¯æ¡ˆä¾‹ä¸­æ¨¡å‹å¯¹-ç¬¦å·çš„æ­§ä¹‰è§£é‡Šï¼‰ï¼ŒæœªæŠ¥å‘Šé‡åŒ–æŒ‡æ ‡\n3. é˜ˆå€¼ç›¸å˜å£°ç§°æœªç»éªŒè¯ï¼šä¸´ç•Œå€¼Î±_cè®¡ç®—å…¬å¼æœªåœ¨çœŸå®ä»»åŠ¡æµ‹è¯•ï¼Œç›¸å˜å®½åº¦O(1/âˆšn)ä»…ä¸ºç†è®ºæ¨æµ‹\n4. æ¡ˆä¾‹ cherry-picking æ˜æ˜¾ï¼šå›é¿å±•ç¤ºç†è®ºå¤±æ•ˆåœºæ™¯ï¼ˆå¦‚ä½å¯†åº¦æ¨¡å¼ä»»åŠ¡ï¼‰", "one_sentence_summary": "æœ¬æ–‡æå‡ºç»Ÿä¸€è®¤çŸ¥æ„è¯†ç†è®ºï¼ˆUCCTï¼‰ï¼Œå£°ç§°å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯æ— æ„è¯†æ¨¡å¼å­˜å‚¨åº“ï¼Œéœ€é€šè¿‡è¯­ä¹‰é”šå®šæ¿€æ´»ä»»åŠ¡èƒ½åŠ›ï¼Œä½†æœªæä¾›å¯é å®è¯æ”¯æŒä¸”ç†è®ºæ¡†æ¶è¿‡åº¦ä¾èµ–éšå–»ç±»æ¯”ã€‚", "slug": "unified-cognitive-consciousness-theory", "keywords": ["Emergent Abilities", "Few-Shot Learning", "Foundation Model", "RAG", "Prompt Engineering", "Multi-Agent"], "further_thoughts": "UCCTå°†è®¤çŸ¥ç§‘å­¦æœ¯è¯­ï¼ˆå…¨å±€å·¥ä½œç©ºé—´/æ³¨æ„å›¾å¼ï¼‰å¼ºè¡Œæ˜ å°„åˆ°LLMæœºåˆ¶ï¼Œå­˜åœ¨æ ¹æœ¬ç¼ºé™·ï¼šäººè„‘æ„è¯†ä¾èµ–å…·èº«æ„ŸçŸ¥ä¸è¿›åŒ–é¢„è®¾çš„ç¥ç»ç»“æ„ï¼Œè€ŒLLMä»…é€šè¿‡æ–‡æœ¬ç»Ÿè®¡å»ºæ¨¡ã€‚ç†è®ºæ ¸å¿ƒå…¬å¼ä¸­çš„æ¨¡å¼å¯†åº¦Ï(P)å’Œè¡¨å¾å·®è·d_r(P,T)æ— æ³•å®é™…æµ‹é‡ï¼Œä½¿æ•´ä¸ªæ¡†æ¶ä¸å¯è¯ä¼ªã€‚æœ‰è¶£çš„æ˜¯ï¼Œé˜ˆå€¼ç›¸å˜æ¦‚å¿µå¯èƒ½å¯å‘æ–°çš„æç¤ºä¼˜åŒ–ç­–ç•¥â€”â€”é€šè¿‡å°è§„æ¨¡æ¢æµ‹ç¡®å®šä»»åŠ¡ä¸´ç•Œé”šå®šå¼ºåº¦ï¼Œä½†è¿™éœ€è„±ç¦»åŸç†è®ºæ¡†æ¶é‡æ–°è®¾è®¡å®éªŒéªŒè¯ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02139", "preference": "unknown", "summary_time": "2025-06-05T07:04:39.753462+00:00", "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "authors": ["Edward Y. Chang"], "abstract": "Few-shot learning in large language models (LLMs) reveals a core paradox: certain tasks generalize from just a few examples, while others demand extensive supervision. To explain this, we introduce the Unified Cognitive Consciousness Theory (UCCT), which reconceptualizes LLMs not as deficient agents, but as unconscious substrates: dense, distributed repositories of linguistic and conceptual patterns that operate without explicit semantics, intention, or goal-directed reasoning. Under this view, LLMs are not flawed simulations of cognition but foundational substrates for general intelligence. UCCT posits that semantic anchoring, via prompts, role assignments, and structured interaction, functions as a conscious control layer that modulates latent representations toward task-relevant semantics and enables coherent, structured reasoning. It unifies prompting, fine-tuning, retrieval-augmented generalization, and multi-agent collaboration within a single framework, grounded in the probabilistic alignment between unconscious pattern space and externally imposed semantic constraints (e.g., prompts, supervision, task objectives). The core implication is not to replace LLMs, but to integrate and unify them through a structured cognitive layer that supports intentional reasoning. This enables collections of LLMs to operate within domain-specialized verticals (e.g., legal reasoning, medical diagnosis) that reason, regulate, and adapt together. Such integration is characterized by phase-transition behavior, wherein anchored representations cross coherence thresholds as a function of semantic constraint strength and interaction context.", "date": "2025-06-05", "categories": ["cs.AI"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8843068563462119, "show": 1}
{"institution": ["Harbin Institute of Technology (Shenzhen)", "Huawei Noahâ€™s Ark Lab"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºç»Ÿä¸€çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½†éœ€è¦å®¡æ…è¯„ä¼°å…¶çœŸå®è´¡çŒ®ï¼š1ï¼‰æ–¹æ³•ä¸Šï¼Œè”åˆæŸå¤±ï¼ˆjoint lossï¼‰è®¾è®¡è™½æœ‰æ•ˆä½†æœ¬è´¨æ˜¯åŠ æƒå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶éæ ¹æœ¬æ€§çªç ´ï¼›2ï¼‰å®éªŒæ˜¾ç¤ºæ”¹è¿›å¹…åº¦æœ‰é™ï¼ˆå¹³å‡ä»…æ¯”GRPOé«˜2.6%ï¼‰ï¼Œä¸”é«˜åº¦ä¾èµ–æ•™å¸ˆæ¨¡å‹è´¨é‡ï¼›3ï¼‰æ•°å­¦æ¨ç†åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›æœªéªŒè¯ï¼Œä¸”è®­ç»ƒéœ€16æ¬¡å“åº”é‡‡æ ·+æ•™å¸ˆæ¨æ–­ï¼Œè®¡ç®—å¼€é”€æå¤§ã€‚éœ€æ·±æŒ–æ˜¯å¦é€šè¿‡æŠ€å·§æ€§è®¾è®¡æ”¾å¤§äº†è¾¹é™…æ”¶ç›Šã€‚", "problem_background": "ç°æœ‰LLMåè®­ç»ƒå­˜åœ¨æ•ˆç‡ä¸æ³›åŒ–çŸ›ç›¾ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡è‡ªæˆ‘æ¢ç´¢æå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†åˆå§‹ç­–ç•¥æ¢ç´¢ä½æ•ˆå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼›çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰é€šè¿‡æ¨¡ä»¿æ•™å¸ˆé«˜æ•ˆå­¦ä¹ ï¼Œä½†å—é™äºæ•™å¸ˆèƒ½åŠ›ä¸”åŸŸå¤–æ³›åŒ–å·®ã€‚ä¼ ç»Ÿä¸¤é˜¶æ®µæ–¹æ¡ˆï¼ˆå…ˆKDåRLï¼‰å‰²è£‚ç›‘ç£ä¸æ¢ç´¢ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹ä¸Šé™ã€‚", "method": "æå‡ºKDRLæ¡†æ¶ç»Ÿä¸€ä¼˜åŒ–KDä¸RLï¼š\n1. æ ¸å¿ƒç›®æ ‡ï¼šç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ä¸­åŒæ—¶æœ€å°åŒ–å­¦ç”Ÿ-æ•™å¸ˆåå‘KLæ•£åº¦ï¼ˆRKLï¼‰å’Œæœ€å¤§åŒ–è§„åˆ™å¥–åŠ±\n2. å…³é”®è®¾è®¡ï¼š\n   - é›†æˆæ–¹å¼ï¼šå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆreward shapingï¼‰ä¸è”åˆæŸå¤±ï¼ˆjoint lossï¼‰ï¼Œåè€…æ›´ç¨³å®š\n   - KLè¿‘ä¼¼ï¼šé‡‡ç”¨k2ä¼°è®¡å™¨ï¼ˆæ¢¯åº¦æ— åï¼‰æ›¿ä»£k3æˆ–Top-K\n   - åŠ¨æ€å¹³è¡¡ï¼šKLç³»æ•°Î²çº¿æ€§é€€ç«ï¼ˆ5e-3â†’1e-3ï¼‰ï¼Œæ—©æœŸå¼ºç›‘ç£åæœŸé‡æ¢ç´¢\n   - å¥–åŠ±å¼•å¯¼æ©è”½ï¼šå¯¹æ­£å¥–åŠ±å“åº”å±è”½KDæŸå¤±ï¼Œé¿å…æ¢¯åº¦å†²çª\n3. è®­ç»ƒæœºåˆ¶ï¼šåŸºäºGRPOç®—æ³•ï¼Œ20Kä¸Šä¸‹æ–‡é‡‡æ ·16ç»„å“åº”ï¼Œæ•™å¸ˆæä¾›å®æ—¶logitç›‘ç£", "experiment": "å®éªŒè®¾è®¡å­˜åœ¨æ˜¾è‘—å±€é™ï¼š\n1. æ”¹è¿›å¹…åº¦å­˜ç–‘ï¼šKDRL-Annealingä»…æ¯”GRPOé«˜2.6%ï¼ˆ56.8%â†’57.2%ï¼‰ï¼Œæ¯”KD-RKLé«˜1.1%ï¼Œä¸”ä¾èµ–å¼ºæ•™å¸ˆï¼ˆSkywork-OR1-Math-7Bï¼‰\n2. åœºæ™¯å•ä¸€ï¼šä»…åœ¨æ•°å­¦æ¨ç†åŸºå‡†ï¼ˆAIME/MATHç­‰ï¼‰éªŒè¯ï¼Œæœªæµ‹è¯•å¸¸è¯†/ä»£ç ç­‰æ³›åŒ–èƒ½åŠ›\n3. æ•ˆç‡çŸ›ç›¾ï¼šå£°ç§°\"é«˜æ•ˆ\"ä½†éœ€å®æ—¶æ•™å¸ˆæ¨æ–­+16æ¬¡é‡‡æ ·ï¼Œå®é™…è®­ç»ƒæ…¢äºGRPOï¼ˆ280æ­¥éœ€80å°æ—¶ï¼‰\n4. æŒ‡æ ‡ç¼ºé™·ï¼šé•¿åº¦æ§åˆ¶ä¾èµ–äººä¸ºæˆªæ–­ï¼ˆ15%æˆªæ–­ç‡ï¼‰ï¼Œæœªè§£å†³æ ¹æœ¬æ€§å†—ä½™ç”Ÿæˆé—®é¢˜\n5. å¯¹æ¯”ä¸å……åˆ†ï¼šæœªä¸æœ€æ–°DPO-basedæ–¹æ¡ˆæ¯”è¾ƒï¼Œä¸”baseline SFTå®ç°è¾ƒå¼±ï¼ˆä»…æ‹’ç»é‡‡æ ·ï¼‰", "one_sentence_summary": "KDRLé€šè¿‡ç­–ç•¥æ¢¯åº¦è”åˆä¼˜åŒ–çŸ¥è¯†è’¸é¦çš„åå‘KLæ•£åº¦ä¸å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä»¥æ›´é«˜è®¡ç®—å¼€é”€æ¢å–è¾ƒåŸºçº¿å¹³å‡2.6%çš„æ€§èƒ½æå‡ã€‚", "slug": "unified-kd-rl-reasoning", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Reasoning", "Fine-tuning", "On-Policy Learning", "Efficiency"], "further_thoughts": "è¯¥å·¥ä½œæ­ç¤ºæœ‰è¶£æƒè¡¡ï¼šKDæä¾›ä½æ–¹å·®æ¢¯åº¦ä½†å¼•è‡´æ¨¡å‹åç¼©ï¼ˆé•¿åº¦æš´æ¶¨15%ï¼‰ï¼ŒRLä¿ƒè¿›æ¢ç´¢ä½†é«˜æ–¹å·®ã€‚å…¶è”åˆè®­ç»ƒå®åˆ™ä¸ºæ–¹å·®æ§åˆ¶æœºåˆ¶ï¼Œä¸è¯¾ç¨‹å­¦ä¹ æœ¬è´¨ç›¸é€šã€‚æœªæ¥å¯æ¢ç´¢ï¼š1ï¼‰æ•™å¸ˆå¼±åŒ–åœºæ™¯çš„é²æ£’æ€§ï¼ˆå¦‚è’¸é¦è¿‡ç¨‹å™ªå£°æ³¨å…¥ï¼‰ï¼›2ï¼‰å°†å¥–åŠ±æ©è”½æ‰©å±•ä¸ºåŸºäºç½®ä¿¡åº¦çš„è‡ªé€‚åº”ç›‘ç£ï¼Œé¿å…äººå·¥é˜ˆå€¼ï¼›3ï¼‰ä¸æ€ç»´è’¸é¦ï¼ˆthought distillationï¼‰ç»“åˆï¼Œè§£è€¦è¯­ä¹‰ä¸ç¬¦å·æ¨ç†çš„ä¼˜åŒ–ç›®æ ‡ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02208", "preference": "unknown", "summary_time": "2025-06-04T07:04:26.972895+00:00", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.", "date": "2025-06-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "created": "2025-06-02", "updated": "2025-06-04", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8738879545419062, "show": 1}
{"institution": ["Chinese Academy of Sciences", "University of California, Los Angeles", "Tsinghua University", "Microsoft", "Hong Kong University of Science and Technology"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡å£°ç§°é€šè¿‡åŠ¨æ€é‡åŠ æƒè§£å†³LLMæ¨ç†å†—ä½™é—®é¢˜ï¼Œä½†å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š1) å®éªŒå¯¹æ¯”ä¸å……åˆ†ï¼ŒæœªåŒ…å«ä¸»æµåŸºå‡†æ¨¡å‹å¦‚GPT-4çš„å¯¹æ¯”æ•°æ®ï¼›2) åŠ¨æ€æƒé‡å…¬å¼(Î»è®¡ç®—)ç¼ºä¹ç†è®ºæ”¯æ’‘ï¼Œä»…å‡­ç»éªŒè®¾å®šï¼›3) è¡¨4æ˜¾ç¤ºSystem-2æ•°æ®æºä½¿ç”¨å›°éš¾é—®é¢˜æ—¶æ€§èƒ½åè€Œä¸‹é™(-1.7% GSM8K)ï¼Œä¸æ ¸å¿ƒè®ºç‚¹çŸ›ç›¾ï¼›4) å‹ç¼©ç‡è®¡ç®—æœªè€ƒè™‘ä¸åŒæ•°æ®é›†çš„tokenåˆ†å¸ƒå·®å¼‚ã€‚éœ€é‡ç‚¹éªŒè¯å…¶æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§ã€‚", "problem_background": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ™®éå­˜åœ¨è¿‡åº¦æ€è€ƒé—®é¢˜ï¼šå¯¹äºç®€å•é—®é¢˜ä¹Ÿç”Ÿæˆå†—é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ¨¡å‹èåˆã€å¼ºåŒ–å­¦ä¹ ï¼‰éœ€è¦å¤æ‚çš„æ•°æ®æ ‡æ³¨æˆ–å‚æ•°è°ƒæ•´ï¼Œè¿‡ç¨‹ç¹çä¸”æˆæœ¬é«˜æ˜‚ã€‚", "method": "æå‡ºåŠ¨æ€é‡åŠ æƒæ¡†æ¶TlDrï¼š\n1. æ„å»ºåŒç³»ç»Ÿæ•°æ®ï¼šSystem-1ï¼ˆç®€å•é—®é¢˜+ç®€çŸ­CoTï¼‰å’ŒSystem-2ï¼ˆå›°éš¾é—®é¢˜+è¯¦ç»†CoTï¼‰\n2. åŠ¨æ€è°ƒæ•´æ•°æ®æƒé‡ï¼šæ¯è®­ç»ƒå‘¨æœŸè®¡ç®—ç³»ç»Ÿæ•ˆç”¨æŒ‡æ ‡ï¼ˆÎ»_sys1=æ•ˆç‡æ”¶ç›Šï¼ŒÎ»_sys2=ç²¾åº¦æ”¶ç›Šï¼‰\n3. ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å°åŒ–ä¸System-1æ•ˆç‡ä¸Šç•Œå’ŒSystem-2ç²¾åº¦ä¸Šç•Œçš„å·®è· \n4. ä¸ä¾èµ–äººå·¥æ ‡æ³¨ï¼šSystem-1æ•°æ®æ¥è‡ªåŸºç¡€æ¨¡å‹ï¼ŒSystem-2æ•°æ®é€šè¿‡LongCoTæ¨¡å‹é‡‡æ ·ç”Ÿæˆ", "experiment": "å®éªŒç»“æœå­˜åœ¨æ˜¾è‘—é—®é¢˜ï¼š\nâ€¢ æœ‰æ•ˆæ€§ï¼šåœ¨DeepSeek-7B/14Bä¸Šå®ç°39%å¹³å‡tokenå‹ç¼©ï¼Œä½†å¤æ‚ä»»åŠ¡æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼ˆAIMEå‡†ç¡®ç‡é™1.7%ï¼‰\nâ€¢ å®éªŒç¼ºé™·ï¼š1) å¯¹æ¯”åŸºçº¿è¿‡æ—¶ï¼ˆå¦‚æœªå¯¹æ¯”2024å¹´SOTAæ¨¡å‹ï¼‰ï¼›2) è¡¨3æ˜¾ç¤ºç›¸åŒtokené¢„ç®—ä¸‹L1åŸºçº¿æ€§èƒ½ä¼˜äºTlDrï¼›3) è¡¨4è¡¨æ˜System-2ä½¿ç”¨å›°éš¾é—®é¢˜åè€ŒæŸå®³GSM8Kæ€§èƒ½ï¼ˆ83.6% vs åŸæ¨¡å‹89.4%ï¼‰\nâ€¢ æŒ‡æ ‡å¯ç–‘ï¼šå‹ç¼©ç‡è®¡ç®—æœªæ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†tokené•¿åº¦åˆ†å¸ƒï¼Œä¸”ç²¾åº¦è¯„ä¼°ä¾èµ–å¤šæ¬¡é‡‡æ ·ï¼ˆæœ€é«˜8æ¬¡ï¼‰é™ä½å®ç”¨æ€§", "one_sentence_summary": "æœ¬æ–‡æå‡ºåŠ¨æ€é‡åŠ æƒæ–¹æ³•TlDrï¼Œé€šè¿‡å¹³è¡¡ç®€çŸ­ä¸è¯¦ç»†æ€ç»´é“¾æ•°æ®çš„è®­ç»ƒæ¯”ä¾‹ï¼Œåœ¨DeepSeekæ¨¡å‹ä¸Šå®ç°39%æ¨ç†å‹ç¼©ï¼Œä½†å¤æ‚ä»»åŠ¡æ€§èƒ½ä¸‹é™ä¸”å®éªŒè®¾è®¡å­˜åœ¨ç¼ºé™·ã€‚", "slug": "dynamic-reweighting-reasoning-compression", "keywords": ["Reasoning", "Efficiency", "Data Augmentation", "Fine-tuning", "Adaptive Systems"], "further_thoughts": "åŠ¨æ€æƒé‡æœºåˆ¶å¯èƒ½å—ä»»åŠ¡éš¾åº¦åˆ†å¸ƒå½±å“ï¼šå½“ç³»ç»Ÿé‡åˆ°éš¾åº¦è¿ç»­è°±é—®é¢˜æ—¶ï¼ˆå¦‚æ•°å­¦è¯æ˜ï¼‰ï¼Œå›ºå®šäºŒåˆ†System-1/System-2çš„ç­–ç•¥å¯èƒ½å¤±æ•ˆã€‚æœªæ¥å¯æ¢ç´¢åŸºäºå›°æƒ‘åº¦çš„å®æ—¶éš¾åº¦è¯„ä¼°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´å‹ç¼©å¼ºåº¦ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥è®ºæ–‡æ— æ„ä¸­æ­ç¤ºäº†å›°éš¾é—®é¢˜æ•°æ®å¯¹ç®€å•ä»»åŠ¡çš„è´Ÿé¢å½±å“ï¼ˆè¡¨4ï¼‰ï¼Œè¿™å¯¹è¯¾ç¨‹å­¦ä¹ è®¾è®¡å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.02678", "preference": "unknown", "summary_time": "2025-06-04T07:02:25.325534+00:00", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", "date": "2025-06-04", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "created": "2025-06-03", "updated": "2025-06-04", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.9341220055589254, "show": 1}
{"institution": ["Google DeepMind"], "reasoning_step": "é¦–å…ˆåˆ†æè®ºæ–‡æœºæ„ï¼šä½œè€…æ¥è‡ªGoogle DeepMindï¼ˆè‡´è°¢éƒ¨åˆ†ç¡®è®¤ï¼‰ã€‚ç ”ç©¶èƒŒæ™¯æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¼ ç»Ÿé¡ºåºå¾®è°ƒåå‘å¯å¡‘æ€§è€Œç‰ºç‰²ç¨³å®šæ€§ã€‚æ–¹æ³•æ ¸å¿ƒæ˜¯æå‡ºPAMï¼šè®­ç»ƒæ—¶å¯¹é½å‚æ•°+ä»»åŠ¡åæ¨¡å‹åˆå¹¶ã€‚å®éªŒåœ¨CoINç­‰åŸºå‡†æµ‹è¯•ï¼Œä½†éœ€å®¡æ…è¯„ä¼°ï¼š1) ä»…æµ‹è¯•VQAä»»åŠ¡ï¼Œæœªè¦†ç›–å…¶ä»–æ¨¡æ€ 2) å¯¹é½æ¯”ä¾‹p=50%æœªç»å……åˆ†æ¶ˆè 3) åˆå¹¶ä»…ç”¨ç®€å•å¹³å‡ï¼Œæœªä¸å…ˆè¿›åˆå¹¶æŠ€æœ¯å¯¹æ¯”ã€‚ç»“æœè™½æ˜¾ç¤ºBWTæ”¹å–„ï¼Œä½†ACCç»å¯¹æå‡æœ‰é™ï¼ˆ49.89 vs åŸºçº¿76.46ï¼‰ï¼Œä¸”è®¡ç®—å¼€é”€æœªé‡åŒ–ã€‚", "problem_background": "è§†è§‰è¯­è¨€æ¨¡å‹æŒç»­å­¦ä¹ é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š1) äº¿çº§å‚æ•°å…¨å¾®è°ƒè®¡ç®—æˆæœ¬è¿‡é«˜ 2) é¡ºåºå¾®è°ƒå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰è™½è§£å†³æ•ˆç‡é—®é¢˜ï¼Œä½†ä»å—ä»»åŠ¡é¡ºåºæ•æ„Ÿæ€§å’Œæ³›åŒ–èƒ½åŠ›å·®çš„åˆ¶çº¦ã€‚æ ¸å¿ƒçŸ›ç›¾åœ¨äºé¡ºåºå¾®è°ƒå›ºæœ‰åœ°åå‘æ–°ä»»åŠ¡å¯å¡‘æ€§ï¼Œç‰ºç‰²æ—§ä»»åŠ¡ç¨³å®šæ€§ã€‚", "method": "æå‡ºå‚æ•°å¯¹é½åˆå¹¶æ¡†æ¶PAMï¼š\n1. *åŒæ¨¡å—ç»“æ„*ï¼šå…¨å±€LoRAæ¨¡å—ç´¯ç§¯å†å²çŸ¥è¯† + ä¸´æ—¶LoRAæ¨¡å—å­¦ä¹ æ–°ä»»åŠ¡\n2. *è®­ç»ƒæ—¶å¯¹é½*ï¼šå‘¨æœŸæ€§æ£€æŸ¥ä¸´æ—¶æ¨¡å—æƒé‡ç¬¦å·ï¼Œè‹¥ä¸å…¨å±€æ¨¡å—top-p%æƒé‡ç¬¦å·å†²çªåˆ™é‡æ–°åˆå§‹åŒ–\n3. *åˆå¹¶æœºåˆ¶*ï¼šä»»åŠ¡è®­ç»ƒåé€šè¿‡å…ƒç´ å¹³å‡åˆå¹¶ä¸´æ—¶æ¨¡å—åˆ°å…¨å±€æ¨¡å—\nåˆ›æ–°ç‚¹åœ¨äºå°†æ¨¡å‹åˆå¹¶ä½œä¸ºæŒç»­å­¦ä¹ æ–°èŒƒå¼ï¼Œå–ä»£ä¼ ç»Ÿé¡ºåºå¾®è°ƒï¼Œå¹¶é€šè¿‡æƒé‡ç¬¦å·å¯¹é½å‡å°‘ä»»åŠ¡é—´å¹²æ‰°ã€‚", "experiment": "åœ¨CoINè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ï¼š\n- *ä¼˜åŠ¿*ï¼šPAMç›¸æ¯”é¡ºåºå¾®è°ƒæ˜¾è‘—é™ä½é—å¿˜ï¼ˆBWT: -19.45 vs -39.51ï¼‰ï¼Œæå‡å‰å‘è¿ç§»ï¼ˆFWT: 11.11 vs 7.71ï¼‰\n- *å±€é™*ï¼š\n  1) ä»…éªŒè¯å›¾åƒ/è§†é¢‘QAä»»åŠ¡ï¼Œæœªæµ‹è¯•æ–‡æœ¬ç”Ÿæˆç­‰åœºæ™¯\n  2) å¯¹é½æ¯”ä¾‹p=50%æœªç»å……åˆ†è°ƒå‚ï¼Œè¡¨5æ˜¾ç¤ºp=70%æ—¶A_tä¸‹é™\n  3) åˆå¹¶ä»…ç”¨ç®€å•å¹³å‡ï¼Œæœªä¸æ¨¡å‹ç®—æœ¯ç­‰å…ˆè¿›æ–¹æ³•å¯¹æ¯”\n  4) è®¡ç®—å¼€é”€ï¼šè®­ç»ƒæ—¶å‘¨æœŸæ€§å¯¹é½å¢åŠ æ“ä½œï¼Œä½†æœªé‡åŒ–TPUè€—æ—¶å½±å“\n- *ç»“æœå¯ç–‘ç‚¹*ï¼šç‹¬ç«‹è®­ç»ƒLoRAç²¾åº¦76.46ï¼Œè€ŒPAMä»…49.89ï¼Œæ˜¾ç¤ºåˆå¹¶å¯¼è‡´æ˜¾è‘—æ€§èƒ½æŸå¤±", "one_sentence_summary": "æå‡ºå‚æ•°å¯¹é½åˆå¹¶æ–¹æ³•PAMï¼Œé€šè¿‡åœ¨è®­ç»ƒæ—¶å¼ºåˆ¶LoRAæƒé‡ç¬¦å·å¯¹é½å¹¶ä»»åŠ¡åæ¨¡å‹åˆå¹¶ï¼Œç¼“è§£è§†è§‰è¯­è¨€æ¨¡å‹æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œåœ¨CoINåŸºå‡†ä¸Šç›¸æ¯”é¡ºåºå¾®è°ƒå°†åå‘è¿ç§»æå‡20ä¸ªç™¾åˆ†ç‚¹ã€‚", "slug": "aligned-model-merging-continual-learning", "keywords": ["Continual Learning", "Low-Rank Adaptation", "Model Merging", "Parameter Alignment", "Multimodal Systems"], "further_thoughts": "æƒé‡ç¬¦å·å¯¹é½æœºåˆ¶å¯æ‰©å±•åˆ°æ¢¯åº¦ç©ºé—´å¯¹é½ï¼Œåˆ©ç”¨HessiançŸ©é˜µè¯†åˆ«å…³é”®æƒé‡ï¼›åˆå¹¶ç­–ç•¥åº”æ¢ç´¢ä»»åŠ¡ç›¸ä¼¼æ€§åŠ æƒï¼Œé¿å…è¿œç¨‹ä»»åŠ¡è¢«ç¨€é‡Šã€‚ä¸ç¥ç»åˆ‡çº¿æ ¸ç†è®ºç»“åˆå¯åˆ†æåˆå¹¶åçš„å‡½æ•°ç©ºé—´å˜åŒ–ã€‚ä¼¦ç†æ–¹é¢éœ€è­¦æƒ•ï¼šæ¶æ„ä»»åŠ¡å¯èƒ½é€šè¿‡ç¬¦å·å†²çªç ´åå…¨å±€æ¨¡å‹ï¼Œéœ€è®¾è®¡å¯¹æŠ—æ€§å¯¹é½éªŒè¯ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03189", "preference": "unknown", "summary_time": "2025-06-07T07:04:26.602849+00:00", "title": "Continual Learning in Vision-Language Models via Aligned Model Merging", "authors": ["Ghada Sokar", "Gintare Karolina Dziugaite", "Anurag Arnab", "Ahmet Iscen", "Pablo Samuel Castro", "Cordelia Schmid"], "abstract": "Continual learning is conventionally tackled through sequential fine-tuning, a process that, while enabling adaptation, inherently favors plasticity over the stability needed to retain prior knowledge. While existing approaches attempt to mitigate catastrophic forgetting, a bias towards recent tasks persists as they build upon this sequential nature. In this work we present a new perspective based on model merging to maintain stability while still retaining plasticity. Rather than just sequentially updating the model weights, we propose merging newly trained task parameters with previously learned ones, promoting a better balance. To maximize the effectiveness of the merging process, we propose a simple mechanism that promotes learning aligned weights with previous ones, thereby avoiding interference when merging. We evaluate this approach on large Vision-Language Models (VLMs), and demonstrate its effectiveness in reducing forgetting, increasing robustness to various task orders and similarities, and improving generalization.", "date": "2025-06-05", "categories": ["cs.CV", "cs.AI"], "created": "2025-05-30", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8877149062754041, "show": 1}
{"institution": ["UniversitÃ  di Pisa", "National Research Council"], "reasoning_step": "è¿™ç¯‡ç«‹åœºè®ºæ–‡çš„æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼šå°½ç®¡åŸºç¡€æ¨¡å‹å¼ºå¤§ï¼Œä½†æŒç»­å­¦ä¹ åœ¨è§£å†³å…¶é™æ€æ€§ã€é«˜æˆæœ¬å’Œä¸­å¿ƒåŒ–é£é™©æ–¹é¢ä»ä¸å¯æ›¿ä»£ã€‚ä½œè€…æå‡ºCPTã€CFTã€CCOä¸‰ä¸ªæ–¹å‘ï¼Œä½†æœªæä¾›æ–°å®éªŒæ•°æ®æ”¯æ’‘å…¶æ ¸å¿ƒä¸»å¼ ï¼ˆå°¤å…¶æ˜¯CCOçš„ä¼˜è¶Šæ€§ï¼‰ã€‚éœ€è¦æ‰¹åˆ¤æ€§å®¡è§†ï¼š1ï¼‰CCOä½œä¸º'æœªæ¥èŒƒå¼'çš„è®ºè¯æ˜¯å¦å……åˆ†ï¼ˆä»…åŸºäºç†è®ºæ¨æ¼”ï¼‰ï¼›2ï¼‰å¯¹CPT/CFTæŒ‘æˆ˜çš„è®¨è®ºæ·±åº¦ä¸è¶³ï¼ˆå¦‚æœªé‡åŒ–ç¾éš¾æ€§é—å¿˜çš„å½±å“ï¼‰ï¼›3ï¼‰æœªè€ƒè™‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€šä¿¡å¼€é”€ç­‰ç°å®ç“¶é¢ˆã€‚", "problem_background": "åŸºç¡€æ¨¡å‹ï¼ˆå¦‚LLMï¼‰å­˜åœ¨é™æ€æ€§ç¼ºé™·ï¼š1ï¼‰çŸ¥è¯†è¿‡æ—¶éœ€æ˜‚è´µé‡è®­ç»ƒï¼ˆGPT-4è®­ç»ƒæˆæœ¬æ•°åƒä¸‡ç¾å…ƒï¼‰ï¼›2ï¼‰æ— æ³•é€‚åº”åŠ¨æ€ç¯å¢ƒï¼ˆå¦‚å®æ—¶ç”¨æˆ·åå¥½ï¼‰ï¼›3ï¼‰é›†ä¸­åŒ–å¯¼è‡´å„æ–­ä¸åè§é£é™©ã€‚æŒç»­å­¦ä¹ èƒ½è§£å†³æ¨¡å‹æ¼‚ç§»ã€ä¸ªæ€§åŒ–ä¸è¶³åŠé«˜ç¢³è¶³è¿¹é—®é¢˜ï¼Œä½†éœ€é‡æ–°å®šä¹‰å…¶åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£çš„è§’è‰²ã€‚", "method": "æå‡ºæŒç»­å­¦ä¹ ä¸‰å¤§æ–¹å‘ï¼š\n1. **æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰**ï¼šå¢é‡æ›´æ–°åŸºç¡€æ¨¡å‹å‚æ•°æ•´åˆæ–°çŸ¥è¯†ï¼Œé‡‡ç”¨è‡ªç›‘ç£ç›®æ ‡ï¼ˆå¦‚æ©ç é¢„æµ‹ï¼‰å‡è½»ç¾éš¾æ€§é—å¿˜\n2. **æŒç»­å¾®è°ƒï¼ˆCFTï¼‰**ï¼šè½»é‡çº§ä»»åŠ¡é€‚é…æŠ€æœ¯ï¼ˆLoRA/Adapterï¼‰ï¼Œä»…æ›´æ–°éƒ¨åˆ†å‚æ•°å®ç°é¢†åŸŸä¸ªæ€§åŒ–\n3. **æŒç»­ç»„åˆä¸ç¼–æ’ï¼ˆCCOï¼‰**ï¼šåŠ¨æ€åè°ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆå¦‚MoEæ¶æ„ï¼‰ï¼Œé€šè¿‡è§’è‰²åˆ†å·¥ï¼ˆä¸“å®¶æ¨¡å—ï¼‰å’Œè‡ªç„¶è¯­è¨€é€šä¿¡è§£å†³å¤æ‚ä»»åŠ¡", "experiment": "**æ–¹æ³•è®ºç¼ºé™·**ï¼š\n- æ— åŸåˆ›å®éªŒï¼Œä»…æ–‡çŒ®ç»¼è¿°ï¼ˆå¦‚å¼•ç”¨LLaMAå‚æ•°æ•ˆç‡ã€LoRAé—å¿˜ç‡ç­‰äºŒæ‰‹æ•°æ®ï¼‰\n- æœªéªŒè¯å…³é”®ä¸»å¼ ï¼šCCOç›¸æ¯”å•ä½“æ¨¡å‹çš„ä¼˜åŠ¿ç¼ºä¹å®è¯ï¼ˆå¦‚æœªå¯¹æ¯”ARC-AGIåŸºå‡†è¡¨ç°ï¼‰\n- å®éªŒè®¾ç½®ç‰‡é¢ï¼šå¿½ç•¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€šä¿¡å»¶è¿Ÿ/èƒ½è€—ç­‰æ ¸å¿ƒæŒ‡æ ‡\n**æœ‰æ•ˆæ€§å­˜ç–‘**ï¼š\n1. CPTä¾èµ–æœªç»éªŒè¯çš„'ç¨³å®šæ€§å·®è·'ç†è®ºï¼ˆåˆå§‹æ€§èƒ½ä¸‹é™ç°è±¡ï¼‰\n2. CFTçš„éšç§ä¿æŠ¤æ–¹æ¡ˆï¼ˆè”é‚¦å­¦ä¹ ï¼‰æœªè€ƒè™‘åƒäº¿å‚æ•°ä¼ è¾“å¼€é”€\n3. CCOé”™è¯¯ä¼ æ’­é£é™©ï¼ˆå¦‚æ™ºèƒ½ä½“å¹»è§‰æ”¾å¤§ï¼‰æœªé‡åŒ–è¯„ä¼°", "one_sentence_summary": "æœ¬æ–‡è®ºè¯åŸºç¡€æ¨¡å‹æ—¶ä»£æŒç»­å­¦ä¹ ä»ä¸å¯æˆ–ç¼ºï¼Œæå‡ºæŒç»­é¢„è®­ç»ƒã€æŒç»­å¾®è°ƒä¸æŒç»­ç»„åˆä¸‰å¤§æ–¹å‘ï¼Œä¸»å¼ åŠ¨æ€ç¼–æ’å¤šæ™ºèƒ½ä½“çš„æŒç»­ç»„åˆèŒƒå¼å°†ä¸»å¯¼æœªæ¥å»ä¸­å¿ƒåŒ–AIç”Ÿæ€ã€‚", "slug": "continual-learning-foundation-models-future", "keywords": ["Continual Learning", "Adaptive Systems", "Foundation Model", "Pre-training", "Fine-tuning", "Multi-Agent"], "further_thoughts": "ä½œè€…å°†CCOç±»æ¯”'ç¤¾ä¼šåä½œ'å­˜åœ¨ä¸¥é‡ç®€åŒ–ï¼š1ï¼‰å¤šæ™ºèƒ½ä½“åè°ƒéœ€è§£å†³çº³ä»€å‡è¡¡é—®é¢˜ï¼ˆå¦‚ä»»åŠ¡åˆ†é…å†²çªï¼‰ï¼Œä½†æœªå¼•å…¥åšå¼ˆè®ºæœºåˆ¶ï¼›2ï¼‰å¼ºè°ƒ'å»ä¸­å¿ƒåŒ–'å´å¿½è§†åŒºå—é“¾éªŒè¯ç­‰åº•å±‚æ¶æ„ï¼›3ï¼‰ä¸ç¥ç»ç¬¦å·AIç»“åˆå¯èƒ½æ›´é«˜æ•ˆï¼ˆå¦‚ç”¨ç¬¦å·æ¨¡å—çº¦æŸé”™è¯¯ä¼ æ’­ï¼‰ã€‚æœªæ¥éœ€åœ¨ç”ŸåŒ–è®¡ç®—ç­‰ä½åŠŸè€—åœºæ™¯éªŒè¯CCOå¯è¡Œæ€§ï¼Œé¿å…ç®—åŠ›éœ€æ±‚æ‚–ç¦»å¯æŒç»­å‘å±•ä¸»å¼ ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03320", "preference": "unknown", "summary_time": "2025-06-05T07:03:33.585061+00:00", "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "abstract": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.", "date": "2025-06-05", "categories": ["cs.LG", "cs.AI"], "created": "2025-06-03", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8882130776320888, "show": 1}
{"institution": ["Peking University", "WeChat AI", "William & Mary", "Westlake University"], "reasoning_step": "è®ºæ–‡æå‡º RewardAnything æ¡†æ¶è§£å†³ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šæ³›åŒ–æ€§å·®ï¼ˆéœ€ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡é‡è®­ï¼‰å’Œéšå¼åå¥½å¯¼è‡´çš„åå·®ã€‚æ–¹æ³•åˆ›æ–°åœ¨äºå°†è‡ªç„¶è¯­è¨€åŸåˆ™ä½œä¸ºåŠ¨æ€è¾“å…¥æ¡ä»¶ï¼Œç»“åˆ GRPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ã€‚å®éªŒè®¾è®¡å…¨é¢ï¼š1ï¼‰åˆ›å»º RABench åŸºå‡†è¯„ä¼°åŸåˆ™æ³›åŒ–èƒ½åŠ›ï¼›2ï¼‰åœ¨ RM-Bench éªŒè¯åå·®ç¼“è§£æ•ˆæœï¼›3ï¼‰é€šè¿‡å¯¹é½æ¡ˆä¾‹è¯æ˜å®ç”¨æ€§ã€‚éœ€æ·±ç©¶çš„ç‚¹ï¼šåˆæˆè®­ç»ƒæ•°æ®ï¼ˆLLM ç”Ÿæˆ+å…±è¯†ç®—æ³•ï¼‰å¯èƒ½å¼•å…¥æ–°åå·®ï¼›GRPO å¥–åŠ±å‡½æ•°è®¾è®¡å¤æ‚ï¼ˆ4 ä¸ªå­æŒ‡æ ‡åŠ æƒï¼‰ï¼Œæ¶ˆèå®éªŒæœªé‡åŒ–å„å­é¡¹è´¡çŒ®ï¼›æ¡ˆä¾‹ç ”ç©¶ä»…ç”¨ 2000 prompts çš„å°è§„æ¨¡éªŒè¯ã€‚", "problem_background": "ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼š1) æ³›åŒ–æ€§å·®ï¼šåŸºäºé™æ€åå¥½æ•°æ®é›†è®­ç»ƒï¼Œæ— æ³•é€‚åº”åŠ¨æ€éœ€æ±‚ï¼ˆå¦‚å®¢æœéœ€ç®€æ´æ€§è€Œç ”ç©¶åŠ©æ‰‹éœ€ç»†èŠ‚ï¼‰ï¼Œåˆ‡æ¢ä»»åŠ¡éœ€é‡æ”¶é›†æ•°æ®å¹¶é‡è®­æ¨¡å‹ï¼›2) éšå¼åå¥½åå·®ï¼šä»…å­¦ä¹ ç»“æœçº§ç›‘ç£ï¼ˆé€‰æ‹©/æ‹’ç»æ ‡ç­¾ï¼‰è€Œå¿½ç•¥å†³ç­–ä¾æ®ï¼Œå¯¼è‡´æ¨¡å‹é€šè¿‡è™šå‡ç›¸å…³æ€§å­¦ä¹ ä»·å€¼è§‚ï¼ˆå¦‚å€¾å‘é•¿æ–‡æœ¬ï¼‰ï¼Œäº§ç”Ÿåå·®ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚", "method": "1) æå‡ºåŸåˆ™éµå¾ªèŒƒå¼ï¼šå¥–åŠ±æ¨¡å‹ä»¥è‡ªç„¶è¯­è¨€åŸåˆ™ P ä¸ºæ¡ä»¶è¾“å…¥ï¼Œè¾“å‡ºå“åº” X_i çš„åˆ†æ•° S(P,Q,X_i) â†’ â„ï¼›2) è®¾è®¡ RewardAnythingï¼šç”Ÿæˆå¼æ¶æ„è¾“å‡ºç»“æ„åŒ–è¯„ä¼°ï¼ˆæ¨ç†+åˆ†æ•°+æ’åï¼‰ï¼›3) è®­ç»ƒæ–¹æ³•ï¼šé‡‡ç”¨ Group Relative Preference Learning (GRPO) å¼ºåŒ–å­¦ä¹ ï¼Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•° r = Î»_f r_f + Î»_a r_aï¼Œå…¶ä¸­ r_f æ¿€åŠ±æ ¼å¼è§„èŒƒï¼ˆå«æ¨ç†è´¨é‡/ç»“æ„å®Œæ•´æ€§ç­‰ï¼‰ï¼Œr_a è¡¡é‡ä¸çœŸå®åå¥½ä¸€è‡´æ€§ï¼ˆå«åŠ æƒé”™æ’æƒ©ç½š/åˆ†æ•°åˆ†å¸ƒåŒ¹é…ç­‰ 4 é¡¹å­æŒ‡æ ‡ï¼‰ã€‚", "experiment": "1) è¯„ä¼°ä½“ç³»ï¼šåˆ›å»º RABench åŸºå‡†ï¼ˆ50 åŸåˆ™+1002 äººå·¥éªŒè¯æ’åï¼‰æµ‹æ³›åŒ–æ€§ï¼ŒRM-Bench æµ‹åå·®ç¼“è§£ï¼›2) æœ‰æ•ˆæ€§ï¼šåœ¨ RM-Bench ç¡¬é›†ä¸Š SotAï¼ˆ86.4%ï¼‰ï¼Œè¯æ˜æ˜¾å¼åŸåˆ™å¯ç¼“è§£ä¼ ç»Ÿæ¨¡å‹çš„é•¿æ–‡æœ¬åå¥½åå·®ï¼›åœ¨ RABench æ¥è¿‘ GPT-4.1ï¼ˆ81.9% vs 82.5%ï¼‰ï¼Œæ˜¾ç¤ºå¼ºæ³›åŒ–ï¼›3) æ¶ˆèå®éªŒï¼šç§»é™¤åŸåˆ™å¯¼è‡´æ€§èƒ½å´©æºƒï¼ˆ-13.4%ï¼‰ï¼Œåˆ—è¡¨è®­ç»ƒâ†’æˆå¯¹è®­ç»ƒé™ 8.4%ï¼ŒGRPOâ†’ç›‘ç£å¾®è°ƒé™ 11.2%ï¼›4) æ¡ˆä¾‹ï¼šä»…ç”¨è‡ªç„¶è¯­è¨€åŸåˆ™å¯¹é½ Qwen3-8Bï¼Œå®‰å…¨å“åº”è´¨é‡æå‡ 12%ã€‚", "one_sentence_summary": "æœ¬æ–‡æå‡º RewardAnythingâ€”â€”åŸºäº GRPO å¼ºåŒ–å­¦ä¹ çš„ç”Ÿæˆå¼åŸåˆ™éµå¾ªå¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€åŸåˆ™åŠ¨æ€è°ƒæ§å¥–åŠ±ç”Ÿæˆï¼Œå®ç°é›¶æ ·æœ¬ä»»åŠ¡é€‚åº”å¹¶ç¼“è§£éšå¼åå¥½åå·®ï¼Œåœ¨ RABench/RM-Bench è¾¾åˆ° SotA ä¸”æ¡ˆä¾‹éªŒè¯é«˜æ•ˆå¯¹é½èƒ½åŠ›ã€‚", "slug": "principle-following-reward-models", "keywords": ["Reward Modeling", "Reinforcement Learning", "Zero-Shot Learning", "Interpretability", "Alignment"], "further_thoughts": "åŸåˆ™éµå¾ªèŒƒå¼å¯æ‰©å±•è‡³å¤šæ¨¡æ€å¯¹é½ï¼ˆå¦‚å›¾æ–‡ä¸€è‡´æ€§åŸåˆ™ï¼‰ï¼Œä½†éœ€è§£å†³åŸåˆ™å†²çªæ—¶çš„ä¼˜å…ˆçº§å»ºæ¨¡ã€‚å½“å‰è®­ç»ƒä¾èµ–åˆæˆæ•°æ®ï¼ˆLLM ç”Ÿæˆ+å…±è¯†ç®—æ³•ï¼‰ï¼Œå¯èƒ½ç»§æ‰¿åŸºç¡€æ¨¡å‹åè§ï¼›GRPO çš„åˆ—è¡¨å¼è¯„ä¼°è®¡ç®—å¼€é”€å¤§ï¼Œè½»é‡åŒ–ç‰ˆæœ¬å€¼å¾—æ¢ç´¢ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥æ–¹æ³•éšå«å®ç°ã€å¯ç¼–ç¨‹ä»·å€¼è§‚ã€â€”â€”é€šè¿‡ä¿®æ”¹åŸåˆ™åŠ¨æ€åˆ‡æ¢é“å¾·æ¡†æ¶ï¼ˆå¦‚åŠŸåˆ©ä¸»ä¹‰ vs ä¹‰åŠ¡è®ºï¼‰ï¼Œä¸ºä¼¦ç†å¯¹é½æä¾›æ–°è·¯å¾„ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.03637", "preference": "unknown", "summary_time": "2025-06-06T07:04:07.985442+00:00", "title": "RewardAnything: Generalizable Principle-Following Reward Models", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "abstract": "Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.", "date": "2025-06-05", "categories": ["cs.CL", "cs.AI", "cs.LG"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8694144410509131, "show": 1}
{"institution": ["University of Maryland", "University of Michigan", "Princeton University", "Amazon AGI", "University of Central Florida"], "reasoning_step": "æœ¬æ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼šåœ¨æ¨ç†æ¨¡å‹ä¸­ï¼Œå»¶é•¿æ€è€ƒæ—¶é—´ï¼ˆå¦‚æ·»åŠ 'Wait'æç¤ºï¼‰ä¼šå¯¼è‡´è¾“å‡ºæ–¹å·®å¢åŠ ï¼Œå½¢æˆæ€§èƒ½å…ˆå‡åé™çš„éå•è°ƒç°è±¡ã€‚è¿™ä¸€ç»“è®ºåŸºäºé«˜æ–¯æ¦‚ç‡æ¨¡å‹çš„ç®€åŒ–å‡è®¾ï¼Œä½†å®é™…è¯­è¨€æ¨¡å‹æ˜¯é«˜ç»´ç¦»æ•£åˆ†å¸ƒï¼Œè¯¥å‡è®¾å¯èƒ½è¿‡åº¦ç®€åŒ–äº†å¤æ‚æ€§ã€‚ä½œè€…æå‡ºçš„'å¹¶è¡Œæ€è€ƒ'æ–¹æ¡ˆæœ¬è´¨ä¸Šæ˜¯Best-of-Né‡‡æ ·çš„å˜ä½“ï¼Œç¼ºä¹ä¸ç°æœ‰æ–¹æ³•ï¼ˆå¦‚è‡ªæ´½æ€§é‡‡æ ·ï¼‰çš„å¯¹æ¯”å®éªŒã€‚æ­¤å¤–ï¼Œå®éªŒä»…ä½¿ç”¨1.5B-8Bè’¸é¦æ¨¡å‹ï¼ŒæœªéªŒè¯åœ¨æ›´å¤§è§„æ¨¡åŸå§‹æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1æœ¬ä½“ï¼‰ä¸Šçš„æ™®é€‚æ€§ã€‚å€¼å¾—æ·±ç©¶çš„æ˜¯ï¼šæ–¹å·®å¢åŠ æ˜¯å¦å¿…ç„¶å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Ÿåœ¨MATHç­‰é«˜éš¾åº¦æ•°æ®é›†ä¸­ï¼Œå¢åŠ æ¢ç´¢æ€§æ˜¯å¦å¯èƒ½æœ‰ç›Šï¼Ÿ", "problem_background": "å½“å‰æµè¡Œè§‚ç‚¹è®¤ä¸ºå»¶é•¿æ¨ç†æ¨¡å‹çš„æ€è€ƒè½¨è¿¹ï¼ˆå¦‚æ·»åŠ 'Wait'æç¤ºï¼‰èƒ½æå‡æ€§èƒ½ã€‚æœ¬æ–‡è´¨ç–‘è¯¥è§‚ç‚¹ï¼Œé€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼šå¢åŠ æ€è€ƒé•¿åº¦ä¼šå…ˆæå‡åé™ä½å‡†ç¡®ç‡ï¼ˆè¿‡æ€è€ƒç°è±¡ï¼‰ã€‚æ ¸å¿ƒé—®é¢˜æ˜¯æ­ç¤ºè¯¥éå•è°ƒè¶‹åŠ¿çš„æœ¬è´¨åŸå› ï¼Œå¹¶æ¢ç´¢æ›´ä¼˜çš„æµ‹è¯•æ—¶è®¡ç®—èµ„æºåˆ†é…ç­–ç•¥ã€‚", "method": "*   è¿‡æ€è€ƒå®è¯åˆ†æï¼šé‡‡ç”¨ä¸¤ç§æµ‹è¯•æ—¶é¢„ç®—æ§åˆ¶æ–¹æ³•â€”â€”(1) 'Wait & Think more'ï¼šæŠ‘åˆ¶ç»ˆæ­¢ç¬¦ï¼Œè¿­ä»£æ·»åŠ 'Wait'å»¶é•¿æ€è€ƒï¼›(2) 'Exact thinking tokens'ï¼šç²¾ç¡®æ§åˆ¶æ€è€ƒtokenæ•°é‡ã€‚\n*   æ–¹å·®é©±åŠ¨è§£é‡Šï¼šæå‡ºç®€åŒ–æ¦‚ç‡æ¨¡å‹ï¼ˆå‡è®¾è¾“å‡ºæœä»é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œè¯æ˜æ–¹å·®å¢åŠ ä¼šå…ˆæ‰©å¤§å¥–åŠ±å‡½æ•°è¦†ç›–ï¼ˆæ€§èƒ½å‡ï¼‰åç¨€é‡Šæ¦‚ç‡å¯†åº¦ï¼ˆæ€§èƒ½é™ï¼‰ã€‚é€šè¿‡æµ‹é‡ç­–ç•¥åˆ†å¸ƒçš„ç†µéªŒè¯æ–¹å·®å¢é•¿ã€‚\n*   å¹¶è¡Œæ€è€ƒæ–¹æ¡ˆï¼šç»™å®šæ€»tokené¢„ç®—Bï¼Œç”ŸæˆNæ¡ç‹¬ç«‹æ¨ç†è·¯å¾„ï¼ˆâˆ‘|zâ½â±â¾|â‰¤Bï¼‰ï¼Œæ¯æ¡è·¯å¾„äº§ç”Ÿç­”æ¡ˆyâ½â±â¾ï¼Œæœ€åé€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ç»ˆç­”æ¡ˆyáµ‡áµ‰Ë¢áµ—ã€‚", "experiment": "*   æ•°æ®é›†ï¼šGSM-8Kï¼ˆ1320æ ·æœ¬ï¼‰ã€MATH-500ï¼ˆ500æ ·æœ¬ï¼‰ã€AIMEï¼ˆ30ç«èµ›é¢˜ï¼‰ã€‚\n*   æ¨¡å‹ï¼šDeepSeek-R1è’¸é¦ç‰ˆï¼ˆ1.5B/7B/8Bï¼‰ã€‚\n*   å…³é”®ç»“æœï¼š\n    - è¿‡æ€è€ƒç°è±¡ï¼šåœ¨ä¸¤ç§é¢„ç®—æ§åˆ¶ä¸‹ï¼Œæ€è€ƒtokenå¢åŠ å‡å¯¼è‡´å‡†ç¡®ç‡å‘ˆå€’Uå‹æ›²çº¿ï¼ˆå¦‚GSM8Kä¸Š1.5Bæ¨¡å‹ï¼štokenä»385å¢è‡³1100æ—¶å‡†ç¡®ç‡ä»82.2%â†’87.3%ï¼Œç»§ç»­å¢è‡³15980åˆ™é™è‡³70.3%ï¼‰ã€‚\n    - æ–¹å·®éªŒè¯ï¼šæ€è€ƒtokenå¢åŠ ä½¿ç†µæå‡12å€ï¼ˆGSM8Kç†µ0.23â†’2.79ï¼‰ï¼Œä¸æ¦‚ç‡æ¨¡å‹é¢„æµ‹ä¸€è‡´ã€‚\n    - å¹¶è¡Œæ€è€ƒä¼˜åŠ¿ï¼šç›¸åŒ16K tokené¢„ç®—ä¸‹ï¼Œç›¸æ¯”åºåˆ—æ€è€ƒçš„11.8%æ€§èƒ½ä¸‹é™ï¼Œå¹¶è¡Œæ€è€ƒå®ç°10.1%æå‡ã€‚\n*   å®éªŒç¼ºé™·ï¼š\n    - ä»…å¯¹æ¯”è‡ªå»ºåŸºçº¿ï¼Œæœªä¸è‡ªæ´½æ€§é‡‡æ ·ç­‰æ ‡å‡†æ–¹æ³•æ¯”è¾ƒï¼›\n    - ç†µæµ‹é‡åŸºäºç›¸åŒpromptçš„å¤šæ¬¡é‡‡æ ·ï¼Œä½†å®é™…æ‰©å±•æ€è€ƒä¼šæ”¹å˜ä¸Šä¸‹æ–‡ï¼Œæµ‹é‡æ–¹æ³•å­˜ç–‘ï¼›\n    - å°è§„æ¨¡è’¸é¦æ¨¡å‹ç»“è®ºæ˜¯å¦é€‚ç”¨äºåŸå§‹å¤§æ¨¡å‹æœªç»éªŒè¯ã€‚", "one_sentence_summary": "æœ¬æ–‡æ­ç¤ºæ¨ç†æ¨¡å‹æµ‹è¯•æ—¶å»¶é•¿æ€è€ƒä¼šå¯¼è‡´è¾“å‡ºæ–¹å·®å¢åŠ å¼•å‘'è¿‡æ€è€ƒ'ç°è±¡ï¼Œæå‡ºé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„çš„é¢„ç®—åˆ†é…ç­–ç•¥ï¼Œåœ¨å›ºå®šè®¡ç®—èµ„æºä¸‹å®ç°æœ€é«˜22%çš„æ€§èƒ½æå‡ã€‚", "slug": "overthinking-parallel-thinking-reasoning", "keywords": ["Variance", "Parallel Sampling", "Test Time", "Efficiency", "Reasoning", "Entropy", "Scaling"], "further_thoughts": "è¿‡æ€è€ƒç°è±¡å¯èƒ½ä¸ä»»åŠ¡å¤æ‚åº¦ç›¸å…³ï¼šåœ¨GSM8Kç­‰ç®€å•ä»»åŠ¡ä¸­æ–¹å·®å¢åŠ ä¸»è¦å¼•å…¥å™ªå£°ï¼Œä½†åœ¨éœ€è¦å‘æ•£æ€ç»´çš„åˆ›æ„æ¨ç†ä¸­æˆ–æœ‰ç›Šå¤„ã€‚å¹¶è¡Œæ€è€ƒæœ¬è´¨ä¸Šæ˜¯ç”¨ç©ºé—´æ¢ç¨³å®šæ€§ï¼Œæœªæ¥å¯ç»“åˆæ—©åœæœºåˆ¶ï¼ˆå¦‚åŸºäºæ–¹å·®çš„åŠ¨æ€ç»ˆæ­¢ï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–èµ„æºåˆ†é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œäººç±»æ¨ç†åŒæ ·å­˜åœ¨'è¿‡åº¦åˆ†æç˜«ç—ª'ç°è±¡ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºç±»äººæ¨ç†çš„AIæ¨¡å‹æä¾›äº†è®¤çŸ¥ç§‘å­¦è§†è§’çš„å¯å‘ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.04210", "preference": "unknown", "summary_time": "2025-06-05T07:04:22.461739+00:00", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.", "date": "2025-06-05", "categories": ["cs.AI", "cs.CL"], "created": "2025-06-04", "updated": "2025-06-05", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8526188594042221, "show": 1}
{"institution": ["Chinese University of Hong Kong"], "reasoning_step": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡½æ•°é€¼è¿‘ç†è®ºçš„æ–°å‹Transformeræ¶æ„ï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ä½œè€…é€šè¿‡å‚…é‡Œå¶åˆ†ææ„å»ºé€šç”¨ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è®¾è®¡Transformerå±‚æ¨¡æ‹Ÿè¿‘ç«¯æ¢¯åº¦ä¸‹é™ç®—æ³•æ±‚è§£Lassoé—®é¢˜ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºè¯¥æ¶æ„èƒ½é€¼è¿‘ä»»æ„æ»¡è¶³å‚…é‡Œå¶æ¢¯åº¦èŒƒæ•°æœ‰ç•Œæ¡ä»¶çš„å‡½æ•°ç±»ï¼Œä½†å­˜åœ¨ä¸‰ä¸ªå…³é”®ç–‘é—®ï¼š1) å‚…é‡Œå¶æ¡ä»¶åœ¨å®é™…ä»»åŠ¡ä¸­æ˜¯å¦æ™®éæˆç«‹ï¼Ÿ2) æ‰€éœ€ç»´åº¦néšè¦†ç›–æ•°å¯¹æ•°å¢é•¿æ˜¯å¦ä¼šå¯¼è‡´å®é™…æ¨¡å‹è¿‡å¤§ï¼Ÿ3) æœªéªŒè¯æ„é€ æ€§æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†çš„æ•ˆæœï¼Œç†è®ºè¾¹ç•Œå¯èƒ½è¿‡äºä¹è§‚ã€‚è¿™äº›é™åˆ¶éœ€è¦åç»­å®è¯ç ”ç©¶éªŒè¯ã€‚", "problem_background": "ç°æœ‰ç ”ç©¶å°†Transformerè§£é‡Šä¸ºä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰çš„é€¼è¿‘å™¨ï¼Œä½†è¯¥æ–¹æ³•å—é™äºä¼˜åŒ–ç®—æ³•æœ¬èº«çš„æ”¶æ•›æ€§è¦æ±‚ï¼Œä»…é€‚ç”¨äºå‡¸é—®é¢˜æˆ–çº¿æ€§å‡½æ•°ç±»ã€‚æœ¬æ–‡æ—¨åœ¨çªç ´è¿™ä¸€é™åˆ¶ï¼Œä»å‡½æ•°é€¼è¿‘è§†è§’æ„å»ºé€šç”¨çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç†è®ºæ¡†æ¶ï¼Œè§£å†³Transformerå¦‚ä½•åŒæ—¶å­¦ä¹ é€šç”¨è¡¨ç¤ºå¹¶åŠ¨æ€é€‚åº”ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„æ ¸å¿ƒé—®é¢˜ã€‚", "method": "1) é€šè¿‡å‚…é‡Œå¶åˆ†ææ„å»ºé€šç”¨ç‰¹å¾é›†ï¼šåˆ©ç”¨Sigmoidæ¿€æ´»å‡½æ•°æ„é€ ç‰¹å¾æ˜ å°„Ï•áµ¢ï¼Œä½¿ä»»æ„ç›®æ ‡å‡½æ•°f(x)-f(0)å¯è¡¨ç¤ºä¸ºâˆ‘Ïáµ¢Ï•áµ¢(x)çš„çº¿æ€§ç»„åˆ\n2) è®¾è®¡Lassoä¼°è®¡å™¨ï¼šå®šä¹‰å¸¦L1æ­£åˆ™çš„ä¼˜åŒ–é—®é¢˜min_Ï Î£(y_i-Ï•áµ¢áµ€Ï)Â² + Î»â€–Ïâ€–â‚æ±‚è§£ç³»æ•°\n3) Transformerå®ç°è¿‘ç«¯æ¢¯åº¦ï¼šæ„é€ å¤šå±‚å¤šå¤´Transformerç²¾ç¡®æ¨¡æ‹Ÿè¿‘ç«¯æ¢¯åº¦è¿­ä»£è¿‡ç¨‹ï¼Œæ¯å±‚æ›´æ–°è§„åˆ™ä¸ºÏ_{t+1} = ST_{Î·Î»}(Ï_t + 2Î·/N Î£(y_i-Ï•áµ¢áµ€Ï_t)Ï•_i) + e_{t+1}\n4) è¾“å…¥ç¼–ç è®¾è®¡ï¼šæ‰©å±•è¾“å…¥ç»´åº¦è‡³d+2n+7ï¼ŒåŒ…å«åŸå§‹è¾“å…¥ã€è¾“å‡ºã€ç‰¹å¾å‘é‡ã€ç³»æ•°ä¼°è®¡ç­‰å¤šæ¨¡æ€ä¿¡æ¯", "experiment": "1) å®éªŒæ€§è´¨ï¼šçº¯ç†è®ºåˆ†æï¼Œæ— å®è¯éªŒè¯\n2) æ•ˆæœæŒ‡æ ‡ï¼šæ¨å¯¼é¢„æµ‹è¯¯å·®ä¸Šç•Œğ”¼[(Å·_{N+1}-f(x_{N+1}))Â²] â‰² âˆš(logN/N) + n/L + (log|ğ’©_Îµ|/n)^{2/3}\n3) å®éªŒç¼ºé™·ï¼šæœªåœ¨çœŸå®æ•°æ®é›†éªŒè¯ç†è®ºè¾¹ç•Œï¼›æœªæ¯”è¾ƒç°æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼›æ„é€ çš„Transformerç»´åº¦éšè¦†ç›–æ•°å¯¹æ•°å¢é•¿ï¼Œå®é™…è®¡ç®—å¼€é”€å­˜ç–‘ï¼›å‚…é‡Œå¶æ¡ä»¶ï¼ˆæ¢¯åº¦å‚…é‡Œå¶å˜æ¢çš„L1èŒƒæ•°æœ‰ç•Œï¼‰çš„ç°å®é€‚ç”¨æ€§æœªè®¨è®º", "one_sentence_summary": "æœ¬æ–‡æå‡ºåŸºäºé€šç”¨å‡½æ•°é€¼è¿‘çš„Transformeræ„é€ æ–¹æ³•ï¼Œé€šè¿‡å‚…é‡Œå¶ç‰¹å¾åˆ†è§£å’Œè¿‘ç«¯æ¢¯åº¦æ¨¡æ‹Ÿï¼Œç†è®ºä¸Šå®ç°ä»»æ„æ»¡è¶³å‚…é‡Œå¶æ¡ä»¶çš„å‡½æ•°ç±»åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„é€¼è¿‘ï¼Œçªç ´ç°æœ‰ä¼˜åŒ–ç®—æ³•é€¼è¿‘æ¡†æ¶çš„å‡¸æ€§é™åˆ¶ã€‚", "slug": "transformers-universal-approximation-in-context-learning", "keywords": ["Transformer", "In-Context Learning", "Universal Approximation", "Representation Learning", "Regression"], "further_thoughts": "è¯¥ç†è®ºæ­ç¤ºäº†Transformeré€šè¿‡ç‰¹å¾ç©ºé—´çº¿æ€§é‡ç»„å®ç°ä¸Šä¸‹æ–‡é€‚åº”çš„æœ¬è´¨æœºåˆ¶ï¼Œè¿™ä¸è®¤çŸ¥ç§‘å­¦ä¸­çš„ç»„åˆæ³›åŒ–ç†è®ºå½¢æˆæœ‰è¶£æ˜ å°„ï¼šäººç±»åœ¨æ–°ä»»åŠ¡ä¸­åŒæ ·å¤ç”¨åŸºç¡€è®¤çŸ¥æ¨¡å—é‡ç»„ã€‚ä½†æ„é€ ä¾èµ–æ˜¾å¼å‚…é‡Œå¶åˆ†è§£ï¼Œä¸çœŸå®Transformeré€šè¿‡æ³¨æ„åŠ›éšå¼å­¦ä¹ å½¢æˆå¯¹æ¯”ï¼Œåç»­å¯æ¢ç´¢å¦‚ä½•å°†ç‰¹å¾æ„é€ è¿‡ç¨‹èå…¥é¢„è®­ç»ƒç›®æ ‡ã€‚å¦éœ€å…³æ³¨ç†è®ºè¾¹ç•Œä¸­LâˆÎµ^{-5/2}çš„è‹›åˆ»æ·±åº¦è¦æ±‚ï¼Œè¿™æˆ–è§£é‡Šä¸ºä½•åƒäº¿å‚æ•°æ¨¡å‹æ‰æ¶Œç°å¼ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.05200", "preference": "unknown", "summary_time": "2025-06-06T07:04:28.448942+00:00", "title": "Transformers Meet In-Context Learning: A Universal Approximation Theory", "authors": ["Gen Li", "Yuchen Jiao", "Yu Huang", "Yuting Wei", "Yuxin Chen"], "abstract": "Modern large language models are capable of in-context learning, the ability to perform new tasks at inference time using only a handful of input-output examples in the prompt, without any fine-tuning or parameter updates. We develop a universal approximation theory to better understand how transformers enable in-context learning. For any class of functions (each representing a distinct task), we demonstrate how to construct a transformer that, without any further weight updates, can perform reliable prediction given only a few in-context examples. In contrast to much of the recent literature that frames transformers as algorithm approximators -- i.e., constructing transformers to emulate the iterations of optimization algorithms as a means to approximate solutions of learning problems -- our work adopts a fundamentally different approach rooted in universal function approximation. This alternative approach offers approximation guarantees that are not constrained by the effectiveness of the optimization algorithms being approximated, thereby extending far beyond convex problems and linear function classes. Our construction sheds light on how transformers can simultaneously learn general-purpose representations and adapt dynamically to in-context examples.", "date": "2025-06-06", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "created": "2025-06-05", "updated": "2025-06-06", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8584234334691381, "show": 1}
{"institution": ["Stanford University", "SynthLabs"], "reasoning_step": "æœ¬æ–‡æå‡ºè‡ªé€‚åº”é•¿åº¦æƒ©ç½šï¼ˆALPï¼‰æ–¹æ³•è§£å†³å¤§æ¨ç†æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦è®¡ç®—çš„é—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨åœ¨çº¿è§£å†³ç‡åŠ¨æ€è°ƒæ•´é•¿åº¦æƒ©ç½šå¼ºåº¦ï¼šé«˜è§£å†³ç‡ï¼ˆç®€å•é—®é¢˜ï¼‰æ–½åŠ å¼ºæƒ©ç½šï¼Œä½è§£å†³ç‡ï¼ˆå›°éš¾é—®é¢˜ï¼‰å¼±æƒ©ç½šã€‚å®éªŒæ˜¾ç¤ºALPæ˜¾è‘—é™ä½å¹³å‡tokenä½¿ç”¨é‡ï¼ˆ50%ï¼‰å¹¶é‡åˆ†é…è®¡ç®—èµ„æºï¼Œä½†éœ€æ‰¹åˆ¤æ€§å®¡è§†ï¼š1) å®éªŒä»…é™æ•°å­¦é¢†åŸŸï¼Œæ³›åŒ–æ€§å­˜ç–‘ï¼›2) å¯¹æ¯”åŸºçº¿ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œè®­ç»ƒé…ç½®ï¼Œå…¬å¹³æ€§ä¸è¶³ï¼›3) è¡Œä¸ºåˆ†ææ˜¾ç¤ºå‹ç¼©å¯èƒ½å‰Šå¼±å›æº¯ç­‰å…³é”®æ¨ç†æœºåˆ¶ï¼›4) å¤šrolloutè®­ç»ƒæœºåˆ¶è®¡ç®—å¼€é”€æœªå……åˆ†è®¨è®ºã€‚éœ€æ·±å…¥åˆ†æALPæ˜¯å¦çœŸæ­£å­¦ä¹ åˆ°é—®é¢˜æœ¬è´¨éš¾åº¦ï¼Œæˆ–ä»…æ˜¯è¿‡æ‹Ÿåˆç‰¹å®šæ•°æ®é›†æ¨¡å¼ã€‚", "problem_background": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ä¸ºæå‡å¤æ‚é—®é¢˜æ€§èƒ½å¸¸ç”Ÿæˆå†—ä½™æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´ç®€å•é—®é¢˜è®¡ç®—èµ„æºæµªè´¹ï¼ˆå¦‚DeepSeek-R1ç”¨10k tokenå›ç­”'2+3'ï¼‰ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™ï¼šç›‘ç£å¾®è°ƒéœ€ç²¾ç»†æ•°æ®æ ‡æ³¨ï¼›ç”¨æˆ·æ§åˆ¶é¢„ç®—éœ€æ‰‹åŠ¨é…ç½®ï¼›ç»Ÿä¸€é•¿åº¦æƒ©ç½šå¿½è§†é—®é¢˜éš¾åº¦å·®å¼‚ã€‚äºŸéœ€è‡ªé€‚åº”è®¡ç®—åˆ†é…æœºåˆ¶ï¼Œä½¿æ¨¡å‹æ ¹æ®é—®é¢˜éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†é•¿åº¦ã€‚", "method": "*   **æ ¸å¿ƒæœºåˆ¶ï¼š** åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¼•å…¥ä¸åœ¨çº¿è§£å†³ç‡æˆåæ¯”çš„è‡ªé€‚åº”é•¿åº¦æƒ©ç½šé¡¹ã€‚\n*   **å®ç°è¿‡ç¨‹ï¼š** \n    1. å¯¹æ¯ä¸ªæç¤ºqé‡‡æ ·Kä¸ªç‹¬ç«‹æ¨ç†è½¨è¿¹ï¼Œè®¡ç®—ç»éªŒè§£å†³ç‡p_solved(q) = (1/K)âˆ‘ğŸ™[answer(yâ½áµâ¾)=yâˆ—]\n    2. è®¾è®¡å¤åˆå¥–åŠ±å‡½æ•°ï¼šr(y,q) = ğŸ™[ç­”æ¡ˆæ­£ç¡®] - Î²N max(p_solved(q), 1/K)\n    3. p_solved(q)è¶Šé«˜ï¼ˆé—®é¢˜è¶Šç®€å•ï¼‰ï¼Œé•¿åº¦æƒ©ç½šæƒé‡è¶Šå¤§ï¼Œå¼ºåˆ¶æ¨¡å‹ç¼©çŸ­å“åº”\n*   **å…³é”®ç‰¹æ€§ï¼š** æ— ç¼é›†æˆGRPOç­‰ç°æœ‰RLç®—æ³•ï¼Œåˆ©ç”¨å…¶å¤šè½¨è¿¹é‡‡æ ·æœºåˆ¶ä¼°ç®—è§£å†³ç‡ï¼Œæ— é¢å¤–è®¡ç®—å¼€é”€ã€‚", "experiment": "*   **æœ‰æ•ˆæ€§éªŒè¯ï¼š** \n    - åœ¨MATH-500/OlympiadBench/AIMEæ•°å­¦æ•°æ®é›†æµ‹è¯•ï¼ŒALPå¹³å‡å‡å°‘50% tokenä½¿ç”¨\n    - å¸•ç´¯æ‰˜åˆ†ææ˜¾ç¤ºï¼šALPä»…ç”¨21% tokenè§£å†³æœ€ç®€å•50%é—®é¢˜ï¼Œå›°éš¾é—®é¢˜tokenåˆ†é…æå‡5.35å€\n*   **å®éªŒç¼ºé™·ï¼š** \n    1. å¯¹æ¯”åŸºçº¿ï¼ˆL1-Exact/ThinkPruneç­‰ï¼‰ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œè®­ç»ƒé…ç½®ï¼Œå…¬å¹³æ€§å­˜ç–‘\n    2. ä»…æµ‹è¯•æ•°å­¦é¢†åŸŸï¼ŒæœªéªŒè¯è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼ˆå¦‚ä»£ç /ç§‘å­¦æ¨ç†ï¼‰\n    3. è¡Œä¸ºåˆ†ææ˜¾ç¤ºå›æº¯è¡Œä¸ºå‡å°‘62%ï¼Œå¯èƒ½å‰Šå¼±æ¨¡å‹è‡ªæˆ‘çº é”™èƒ½åŠ›\n*   **ç»“æœçŸ›ç›¾ç‚¹ï¼š** è¡¨1æ˜¾ç¤ºR1-Distill-Qwenæ¨¡å‹åº”ç”¨ALPåOlympiadBenchæ€§èƒ½æå‡42%ï¼ˆ0.36â†’0.51ï¼‰ï¼Œä½†æœªè§£é‡Šä¸ºä½•åŸºç¡€æ¨¡å‹æ€§èƒ½çªå˜", "one_sentence_summary": "æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”é•¿åº¦æƒ©ç½šæœºåˆ¶ï¼Œé€šè¿‡åœ¨çº¿è§£å†³ç‡åŠ¨æ€è°ƒæ•´æ¨ç†é•¿åº¦ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šå®ç°50%è®¡ç®—æ•ˆç‡æå‡å¹¶é‡åˆ†é…èµ„æºè‡³å›°éš¾é—®é¢˜ï¼Œä½†æ³›åŒ–æ€§ä¸å…¬å¹³æ€§éªŒè¯ä¸è¶³ã€‚", "slug": "adaptive-length-penalty", "keywords": ["Reinforcement Learning", "Reasoning", "Efficiency", "Adaptive Systems"], "further_thoughts": "ALPçš„æ ¸å¿ƒå‡è®¾'è§£å†³ç‡=é—®é¢˜éš¾åº¦'å­˜åœ¨æ ¹æœ¬å±€é™ï¼š1) é«˜è§£å†³ç‡å¯èƒ½å› è®°å¿†è€Œéç†è§£ï¼ˆå¦‚è®­ç»ƒæ•°æ®æ³„éœ²ï¼‰ï¼›2) æ•°å­¦é—®é¢˜æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œä½†å¼€æ”¾åŸŸé—®é¢˜ï¼ˆé“å¾·æ¨ç†ç­‰ï¼‰æ— äºŒå€¼åˆ¤æ–­æ ‡å‡†ï¼›3) å®éªŒæ˜¾ç¤ºL1-Maxåœ¨å›°éš¾é—®é¢˜ä¸Štokenä½¿ç”¨åè€Œå‡å°‘ï¼Œæš—ç¤ºéš¾åº¦ä¼°è®¡å¯èƒ½è¢«å¯¹æŠ—æ ·æœ¬æ“çºµã€‚æœªæ¥åº”æ¢ç´¢å¤šç»´åº¦éš¾åº¦ä¼°è®¡ï¼ˆå¦‚ä¸ç¡®å®šæ€§é‡åŒ–ï¼‰ï¼Œå¹¶ä¸ã€ŠChain-of-Thought Without Promptingã€‹çš„éšå¼æ¨ç†å‹ç¼©æœºåˆ¶ç»“åˆã€‚", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2506.05256", "preference": "unknown", "summary_time": "2025-06-06T07:03:40.279754+00:00", "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning", "authors": ["Violet Xiang", "Chase Blagden", "Rafael Rafailov", "Nathan Lile", "Sang Truong", "Chelsea Finn", "Nick Haber"], "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.", "date": "2025-06-06", "categories": ["cs.AI", "cs.LG"], "created": "2025-06-05", "updated": "2025-06-06", "license": "http://creativecommons.org/licenses/by/4.0/", "year": 2025, "score": 0.8660272910576859, "show": 1}

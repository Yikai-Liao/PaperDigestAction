{"institution": [], "reasoning_step": "本文提出基于范畴论的形式化框架建模LLM的元提示过程，但存在显著问题：1) 实验仅在附录简要提及，缺乏数据集、基线方法等关键细节，统计显著性(p<0.01)未说明检验方法或样本量；2) 理论假设过强(如LLM完全遵循指令、忽略随机性)，与实际情况脱节；3) 将提示建模为范畴态射时，未解决语义模糊性(如\"同构即语义\"假设未经验证)；4) 宣称元提示'始终优于'传统提示的结论仅依赖单一用户评估，未考虑任务复杂度差异。需深究范畴论在此场景的不可证伪性风险。", "problem_background": "现有研究缺乏对LLM上下文学习(ICL)的形式化理论框架，尤其难以建模：1) 提示敏感性与任务泛化性；2) 用户交互的不可预测性；3) 元提示(自动生成提示)的数学基础。传统方法依赖经验性优化，无法解释元提示为何有效或比较不同方法的本质等价性。", "method": "核心方法为构建范畴论框架：\n1. 定义基础范畴**Prompt**：对象为字符串子集，态射为提示指令(如\"总结{X}\")，组合操作为提示串联\n2. 引入任务子范畴**Task**：通过包含函子T↪**Prompt**捕获特定任务(如摘要)的所有提示变体\n3. 建模元提示：利用闭幺半范畴的内部指数对象Z^X(提示集合)，将元提示定义为态射λ: Y→Z^X，根据用户输入Y动态选择提示\n4. 证明关键性质：a) 任务无关性(任意任务可用元提示)；b) 元提示方法等价性(存在自然变换连接不同元提示态射)", "experiment": "实验设计存在严重缺陷：\n1. 仅在附录提及用户评估，未描述具体任务、模型、基线方法或评估指标\n2. 声称元提示生成结果比基线(硬编码提示)更受用户青睐(p<0.01)，但未说明样本量、显著性检验方法(Wilcoxon符号秩检验的适用性存疑)\n3. 实验仅验证提示'合适度'而非实际性能(如准确性)，且未控制提示长度等混淆变量\n4. 未对比主流元提示技术(如AutoPrompt)，缺乏量化结果支撑'明显优于'的结论", "one_sentence_summary": "本文提出基于范畴论的LLM元提示形式化框架，声称证明其任务无关性与方法等价性，并通过有限用户实验断言元提示优于传统提示，但理论假设与实验设计存在显著缺陷。", "slug": "category-theory-meta-prompting", "keywords": ["In-Context Learning", "Meta-Learning", "Foundation Model", "Prompt Engineering", "Theoretical Framework"], "further_thoughts": "范畴论抽象虽优雅但可能掩盖LLM本质问题：1) 将随机性归约为确定性系统(第7章)忽略关键误差传播；2) 实际应用需处理部分函数(如无效指令)，但框架假设全函数性；3) 与概率图模型(如马尔可夫范畴)结合可增强实用性。有趣的是，该框架暗示提示工程本质是范畴重构——通过调整**Task**子范畴优化性能，这解释为何少量提示词修改能显著改变输出，为对抗提示攻击提供新视角。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2312.06562", "preference": "unknown", "summary_time": "2025-06-07T07:05:07.197208+00:00", "title": "On Meta-Prompting", "authors": ["Adrian de Wynter", "Xun Wang", "Qilong Gu", "Si-Qing Chen"], "abstract": "Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.", "date": "2025-06-03", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.CT"], "created": "2025-05-30", "updated": "2025-06-03", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "year": 2025, "score": 0.8518155213731323, "show": 1}
{"id": "2312.13772", "reasoning_step": "在深入阅读这篇论文时，我首先关注了其研究背景和问题，即大型语言模型（LLMs）在低资源场景下的校准问题（miscalibration），特别是上下文学习（ICL）是否能改善模型的校准性能。论文提出，ICL并未一致地提升校准效果，且不同学习范式（如零样本学习ZSL、监督微调SFT、监督上下文学习SICL）在不同数据集上的表现和校准效果差异显著，尤其与数据是否被模型预训练时见过有关。这一发现让我思考数据污染（data contamination）对模型评估的潜在影响，以及如何在实际应用中选择合适的范式。\n\n进一步分析方法部分，论文提出了一种自集成（self-ensembling）策略，通过在上下文示例和提示模板上引入变异，并结合不同的集成策略（如最大概率、平均概率、多数投票）来提升校准和任务性能。我注意到自集成方法的核心在于利用单一模型的多样化输入生成多个预测结果，从而减少过自信问题，这与传统集成方法相比具有较低的计算成本。然而，我对自集成是否在所有场景下都有效持保留态度，尤其是在提示模板设计不佳或上下文示例选择不当时，可能会引入额外噪声。\n\n实验设计上，论文覆盖了7个分类数据集，并模拟低资源场景，这点较为合理。但我发现实验主要基于Flan-T5模型家族，缺乏对其他架构（如仅解码器模型）或更大规模模型的验证，这可能限制了结论的普适性。此外，校准指标（如ECE）虽然直观，但可能无法完全反映模型在实际应用中的可靠性，尤其是在类别不平衡的数据集上。\n\n最后，自集成方法在校准性能上平均提升了43%，这是一个亮点，但任务性能的提升较为有限（平均0.79），这让我思考是否可以通过更复杂的集成策略或结合其他校准方法进一步优化结果。同时，论文提到的数据污染问题也启发了我，是否可以通过设计去污染策略或使用未见过的数据集来进一步验证ICL和SFT的表现差异。", "problem_background": "大型语言模型（LLMs）在低资源场景下常面临过自信（overconfidence）和校准不足（miscalibration）的问题，尤其是在上下文学习（ICL）范式中，尽管其任务性能有所提升，但校准效果是否一致仍是一个开放性问题。本研究聚焦于指令调整后的任务专用语言模型，探讨ICL如何影响模型校准，以及是否能在保持任务性能的同时实现更好的校准效果，为负责任的AI应用提供支持。", "slug": "miscalibration-icl-self-ensembling", "one_sentence_summary": "本文通过对大型语言模型在低资源场景下的校准问题进行深入分析，揭示上下文学习（ICL）未一致改善校准效果，并提出自集成方法显著提升校准性能（平均降低ECE 43%），同时维持或略提升任务性能。", "preference": "unknown", "updated": "2025-05-23", "top_p": 0.8, "method": "论文提出并比较了四种学习范式在低资源场景下的表现：零样本学习（ZSL）、上下文学习（ICL）、监督微调（SFT）和监督上下文学习（SICL），并针对校准问题引入了一种自集成（self-ensembling）方法。\n- **核心思想**：通过在单一模型上引入输入变异（如上下文示例和提示模板的多样性），生成多个预测结果并进行集成，以减少过自信问题，同时维持或提升任务性能。\n- **具体实现**：\n  - **变异类型**：包括上下文示例变异（Var-IC，通过不同示例组合和顺序）、提示模板变异（Var-Prompt，通过不同措辞的模板）和两者结合（Var-Both）。\n  - **集成策略**：包括最大概率（Max Probability，选取每个类别最高概率并归一化）、平均概率（Mean Probability，平均所有变异预测分布）和多数投票（Majority Vote，选取累积概率最高的预测）。\n- **关键点**：自集成不需训练多个模型，计算成本较低，且与传统校准方法（如温度缩放）正交，可进一步结合使用。\n- **批判性思考**：虽然自集成方法创新性较高，但其效果高度依赖于变异设计的质量，提示模板或上下文示例选择不当可能引入噪声，导致性能下降。此外，论文未充分探讨如何系统化地优化变异选择策略，这可能是未来改进的方向。", "created": "2025-05-21", "authors": ["Chengzu Li", "Han Zhou", "Goran Glavaš", "Anna Korhonen", "Ivan Vulić"], "score": 0.8883582906867298, "experiment": "实验在7个分类数据集上进行，模拟低资源场景，数据集包括SST-2、RTE等，部分可能在预训练中被模型见过。使用Flan-T5large作为主要模型，并通过预期校准误差（ECE）等指标评估校准效果。\n- **实验设置**：通过子采样训练数据模拟低资源场景，测试ZSL、ICL、SFT和SICL的表现，并结合自集成方法测试不同变异和集成策略的效果。\n- **结果分析**：\n  - ICL未能在所有数据集中一致改善校准效果，ECE值在5/7数据集上无显著下降；任务性能和校准效果依赖于数据是否被模型见过，ICL在见过的数据（如SST-2）上表现接近SFT/SICL，但在未见过的数据上较差。\n  - 自集成方法显著提升校准性能，平均降低ECE值43%，任务性能提升较小（平均0.79）。最大概率集成策略在校准和性能上表现最佳。\n- **合理性与局限**：实验覆盖多个数据集和学习范式，设计较为全面，但主要依赖Flan-T5模型，缺乏对其他模型架构或更大规模模型的验证，可能影响结论普适性。此外，任务性能提升有限，提示自集成在某些场景下可能仅对校准有效，而非全面优化。\n- **批判性思考**：实验结果显示自集成对SFT和SICL的校准提升更大，但ICL在见过数据上的表现可能受数据污染影响，需进一步验证去污染后的效果。同时，ECE作为主要指标可能无法完全反映实际应用中的可靠性，尤其在类别不平衡数据上。", "institution": ["University of Cambridge", "University of Würzburg"], "source_file": "2312.13772.json", "keywords": ["In-Context Learning", "Supervised Learning", "Classification", "Large Language Model", "Robustness"], "temperature": 0.1, "model": "grok-3", "further_thoughts": "自集成方法在校准性能上的显著提升为解决大型语言模型的过自信问题提供了一个低成本的解决方案，但其任务性能提升有限，提示我们可能需要结合其他技术（如参数高效微调或更复杂的集成策略）来实现全面优化。此外，论文中提到的数据污染问题值得进一步探索，特别是在评估ICL和SFT表现时，如何设计去污染策略或使用完全未见过的数据集可能是未来研究的一个重要方向。另一个有趣的思考是，自集成是否可以扩展到多模态模型或跨领域任务中，例如结合视觉和文本输入的变异设计，以提升多模态系统的校准和性能，这可能与现有的多模态基础模型研究（如Vision Foundation Model）产生交叉启发。最后，考虑到校准在负责任AI中的重要性，自集成方法或许可以与RLHF（基于人类反馈的强化学习）结合，进一步提升模型的信任度和安全性。", "lang": "zh", "categories": ["cs.CL", "cs.AI", "cs.LG"], "summary_time": "2025-05-25T03:24:55.533376+00:00", "abstract": "When adapting ICL with or without fine-tuning, we are curious about whether the instruction-tuned language model is able to achieve well-calibrated results without suffering from the problem of overconfidence (i.e., miscalibration) considering its strong instruction following ability, especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration. Through extensive controlled experiments, we observe that the miscalibration problem exists across all learning methods in low-resource setups. To achieve simultaneous gain for both in-task performance and calibration, we then study the potential of self-ensembling applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies) to make the predictions more calibrated and have comparable or even better performance. We find that self-ensembling with max probability produces robust and calibrated predictions. Our work reveals the potential calibration problem of using ICL despite the improvements in task performance and sheds light on which learning paradigm to choose. We also provide practical guidelines for choosing learning paradigms depending on whether the data has been seen by the model before and a worthwhile solution via self-ensembling on how to enhance both task performance and calibration of LMs, which we hope could encourage further study.", "title": "Large Language Models are Miscalibrated In-Context Learners"}

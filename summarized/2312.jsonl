{"id": "2312.13772", "reasoning_step": "在深入阅读这篇论文时，我首先关注了其研究背景和问题，即大型语言模型（LLMs）在低资源场景下的校准问题（miscalibration），特别是上下文学习（ICL）是否能改善模型的校准性能。论文提出，ICL并未一致地提升校准效果，且不同学习范式（如零样本学习ZSL、监督微调SFT、监督上下文学习SICL）在不同数据集上的表现和校准效果差异显著，尤其与数据是否被模型预训练时见过有关。这一发现让我思考数据污染（data contamination）对模型评估的潜在影响，以及如何在实际应用中选择合适的范式。\n\n进一步分析方法部分，论文提出了一种自集成（self-ensembling）策略，通过在上下文示例和提示模板上引入变异，并结合不同的集成策略（如最大概率、平均概率、多数投票）来提升校准和任务性能。我注意到自集成方法的核心在于利用单一模型的多样化输入生成多个预测结果，从而减少过自信问题，这与传统集成方法相比具有较低的计算成本。然而，我对自集成是否在所有场景下都有效持保留态度，尤其是在提示模板设计不佳或上下文示例选择不当时，可能会引入额外噪声。\n\n实验设计上，论文覆盖了7个分类数据集，并模拟低资源场景，这点较为合理。但我发现实验主要基于Flan-T5模型家族，缺乏对其他架构（如仅解码器模型）或更大规模模型的验证，这可能限制了结论的普适性。此外，校准指标（如ECE）虽然直观，但可能无法完全反映模型在实际应用中的可靠性，尤其是在类别不平衡的数据集上。\n\n最后，自集成方法在校准性能上平均提升了43%，这是一个亮点，但任务性能的提升较为有限（平均0.79），这让我思考是否可以通过更复杂的集成策略或结合其他校准方法进一步优化结果。同时，论文提到的数据污染问题也启发了我，是否可以通过设计去污染策略或使用未见过的数据集来进一步验证ICL和SFT的表现差异。", "problem_background": "大型语言模型（LLMs）在低资源场景下常面临过自信（overconfidence）和校准不足（miscalibration）的问题，尤其是在上下文学习（ICL）范式中，尽管其任务性能有所提升，但校准效果是否一致仍是一个开放性问题。本研究聚焦于指令调整后的任务专用语言模型，探讨ICL如何影响模型校准，以及是否能在保持任务性能的同时实现更好的校准效果，为负责任的AI应用提供支持。", "slug": "miscalibration-icl-self-ensembling", "one_sentence_summary": "本文通过对大型语言模型在低资源场景下的校准问题进行深入分析，揭示上下文学习（ICL）未一致改善校准效果，并提出自集成方法显著提升校准性能（平均降低ECE 43%），同时维持或略提升任务性能。", "preference": "unknown", "updated": "2025-05-23", "top_p": 0.8, "method": "论文提出并比较了四种学习范式在低资源场景下的表现：零样本学习（ZSL）、上下文学习（ICL）、监督微调（SFT）和监督上下文学习（SICL），并针对校准问题引入了一种自集成（self-ensembling）方法。\n- **核心思想**：通过在单一模型上引入输入变异（如上下文示例和提示模板的多样性），生成多个预测结果并进行集成，以减少过自信问题，同时维持或提升任务性能。\n- **具体实现**：\n  - **变异类型**：包括上下文示例变异（Var-IC，通过不同示例组合和顺序）、提示模板变异（Var-Prompt，通过不同措辞的模板）和两者结合（Var-Both）。\n  - **集成策略**：包括最大概率（Max Probability，选取每个类别最高概率并归一化）、平均概率（Mean Probability，平均所有变异预测分布）和多数投票（Majority Vote，选取累积概率最高的预测）。\n- **关键点**：自集成不需训练多个模型，计算成本较低，且与传统校准方法（如温度缩放）正交，可进一步结合使用。\n- **批判性思考**：虽然自集成方法创新性较高，但其效果高度依赖于变异设计的质量，提示模板或上下文示例选择不当可能引入噪声，导致性能下降。此外，论文未充分探讨如何系统化地优化变异选择策略，这可能是未来改进的方向。", "created": "2025-05-21", "authors": ["Chengzu Li", "Han Zhou", "Goran Glavaš", "Anna Korhonen", "Ivan Vulić"], "score": 0.8883582906867298, "experiment": "实验在7个分类数据集上进行，模拟低资源场景，数据集包括SST-2、RTE等，部分可能在预训练中被模型见过。使用Flan-T5large作为主要模型，并通过预期校准误差（ECE）等指标评估校准效果。\n- **实验设置**：通过子采样训练数据模拟低资源场景，测试ZSL、ICL、SFT和SICL的表现，并结合自集成方法测试不同变异和集成策略的效果。\n- **结果分析**：\n  - ICL未能在所有数据集中一致改善校准效果，ECE值在5/7数据集上无显著下降；任务性能和校准效果依赖于数据是否被模型见过，ICL在见过的数据（如SST-2）上表现接近SFT/SICL，但在未见过的数据上较差。\n  - 自集成方法显著提升校准性能，平均降低ECE值43%，任务性能提升较小（平均0.79）。最大概率集成策略在校准和性能上表现最佳。\n- **合理性与局限**：实验覆盖多个数据集和学习范式，设计较为全面，但主要依赖Flan-T5模型，缺乏对其他模型架构或更大规模模型的验证，可能影响结论普适性。此外，任务性能提升有限，提示自集成在某些场景下可能仅对校准有效，而非全面优化。\n- **批判性思考**：实验结果显示自集成对SFT和SICL的校准提升更大，但ICL在见过数据上的表现可能受数据污染影响，需进一步验证去污染后的效果。同时，ECE作为主要指标可能无法完全反映实际应用中的可靠性，尤其在类别不平衡数据上。", "institution": ["University of Cambridge", "University of Würzburg"], "source_file": "2312.13772.json", "keywords": ["In-Context Learning", "Supervised Learning", "Classification", "Large Language Model", "Robustness"], "temperature": 0.1, "model": "grok-3", "further_thoughts": "自集成方法在校准性能上的显著提升为解决大型语言模型的过自信问题提供了一个低成本的解决方案，但其任务性能提升有限，提示我们可能需要结合其他技术（如参数高效微调或更复杂的集成策略）来实现全面优化。此外，论文中提到的数据污染问题值得进一步探索，特别是在评估ICL和SFT表现时，如何设计去污染策略或使用完全未见过的数据集可能是未来研究的一个重要方向。另一个有趣的思考是，自集成是否可以扩展到多模态模型或跨领域任务中，例如结合视觉和文本输入的变异设计，以提升多模态系统的校准和性能，这可能与现有的多模态基础模型研究（如Vision Foundation Model）产生交叉启发。最后，考虑到校准在负责任AI中的重要性，自集成方法或许可以与RLHF（基于人类反馈的强化学习）结合，进一步提升模型的信任度和安全性。", "lang": "zh", "categories": ["cs.CL", "cs.AI", "cs.LG"], "summary_time": "2025-05-25T03:24:55.533376+00:00", "abstract": "When adapting ICL with or without fine-tuning, we are curious about whether the instruction-tuned language model is able to achieve well-calibrated results without suffering from the problem of overconfidence (i.e., miscalibration) considering its strong instruction following ability, especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration. Through extensive controlled experiments, we observe that the miscalibration problem exists across all learning methods in low-resource setups. To achieve simultaneous gain for both in-task performance and calibration, we then study the potential of self-ensembling applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies) to make the predictions more calibrated and have comparable or even better performance. We find that self-ensembling with max probability produces robust and calibrated predictions. Our work reveals the potential calibration problem of using ICL despite the improvements in task performance and sheds light on which learning paradigm to choose. We also provide practical guidelines for choosing learning paradigms depending on whether the data has been seen by the model before and a worthwhile solution via self-ensembling on how to enhance both task performance and calibration of LMs, which we hope could encourage further study.", "title": "Large Language Models are Miscalibrated In-Context Learners"}

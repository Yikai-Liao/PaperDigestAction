{"id": "2404.17785", "reasoning_step": "在深入阅读《Temporal Scaling Law for Large Language Models》这篇论文后，我发现其核心创新点在于从时间维度研究大型语言模型（LLM）的预训练过程中的测试损失变化，这与传统的缩放定律（Scaling Laws）关注最终损失与模型大小、计算预算等静态因素的关系形成鲜明对比。论文提出了一种细粒度的视角，即通过分析每个token位置的损失变化，并用动态双曲线法则（dynamic hyperbolic-law）来建模这些变化，进一步推导出整体测试损失的时间模式。这种方法在预测损失变化和直接在目标模型上选择超参数（如数据混合比例）方面显示出潜力。然而，我对论文的方法和实验设计有一些担忧：首先，动态双曲线法则的参数拟合依赖于早期训练数据，但对于训练过程中的波动和长期趋势的预测可能存在过拟合风险；其次，实验中使用的模型规模（9.8M到6.7B）虽然覆盖了一定范围，但是否能推广到更大规模的模型（如百亿或千亿参数）仍存疑问；最后，论文在超参数选择的应用中，虽然展示了比小模型直接迁移更好的结果，但实验的基准任务和数据集较为有限，可能无法全面反映实际应用场景中的表现。此外，论文未充分讨论其方法在计算成本上的影响，这对于实际应用（如频繁预测和调整超参数）至关重要。总体而言，论文提供了一个新颖的视角，但其理论和实验的严谨性及普适性需要进一步验证。", "problem_background": "大型语言模型（LLM）的性能随着模型规模、计算预算和数据集规模的增加而提升，这种关系已被传统缩放定律（Scaling Laws）以幂律形式建模。然而，预训练过程中测试损失随训练步数变化的时间动态尚未被充分探索，而这一动态对于直接在目标模型上选择超参数（如数据混合比例）和理解预训练学习动态具有重要价值。本文提出了一种新的时间缩放定律（Temporal Scaling Law），旨在解决如何预测和建模LLM预训练过程中测试损失随时间步长的演变问题，以支持更高效的训练策略和超参数优化。", "slug": "temporal-scaling-law-llm", "one_sentence_summary": "本文提出时间缩放定律（Temporal Scaling Law），通过动态双曲线法则建模LLM预训练中每个token位置的损失变化，精准预测整体测试损失演变，支持直接在目标模型上选择超参数并揭示学习动态。", "preference": "unknown", "updated": "2025-05-16", "top_p": 0.8, "method": "本文提出了一种时间缩放定律（Temporal Scaling Law），其核心思想是通过细粒度分析每个token位置的测试损失变化来建模LLM预训练过程中的整体损失动态。具体步骤如下：\n- **细粒度损失分解**：将整体测试损失分解为每个token位置的损失，发现不同位置的损失随上下文长度增加呈现动态双曲线法则（dynamic hyperbolic-law），即 $\\mathcal{L}_i = \\frac{a_0}{1 + a_1 i} + a_2$，其中 $i$ 为token位置，$a_0, a_1, a_2$ 为拟合参数。\n- **参数时间演变建模**：进一步研究动态双曲线法则中参数 $a_0, a_1, a_2$ 随训练步数（以训练token数量 $N$ 表示）的变化，分别用对数、双曲线和分段余弦函数等形式拟合其时间模式，并通过非线性最小二乘法求解拟合参数。\n- **整体损失预测**：基于拟合的参数时间模式，结合每个token位置的损失公式，计算整体测试损失的预测值 $\\mathcal{L}^N = \\frac{1}{n} \\sum_{i=1}^n \\frac{a_0^N}{1 + a_1^N \\cdot i} + a_2^N$。\n\n**批判性思考**：虽然该方法在理论上提供了细粒度的视角，但动态双曲线法则的数学形式是否具有普适性值得怀疑，尤其是在不同模型架构或训练设置下可能需要重新调整参数拟合函数。此外，方法对早期训练数据的依赖可能导致对训练中后期波动或异常情况的预测不准确，缺乏对长期趋势稳定性的充分验证。", "created": "2025-05-15", "authors": ["Yizhe Xiong", "Xiansheng Chen", "Xin Ye", "Hui Chen", "Zijia Lin", "Haoran Lian", "Zhenpeng Su", "Wei Huang", "Jianwei Niu", "Jungong Han", "Guiguang Ding"], "score": 0.7543761513906533, "experiment": "实验设计主要围绕时间缩放定律的拟合效果、预测准确性及应用场景展开：\n- **数据集与模型**：使用Pile数据集进行预训练，验证集包括分布内（ID-Val）和分布外（OOD-Val）数据，模型规模从9.8M到6.7B参数不等，基于LLaMA架构，并扩展到GPT-NeoX和MoE模型以验证普适性。\n- **拟合与预测效果**：通过R²指标评估拟合质量，时间缩放定律在ID-Val和OOD-Val上的R²均超过0.99，远高于传统幂律（Power-law）基线（R²约0.6-0.7）。预测实验中，使用前10%-40%的训练数据预测后续损失，均方误差（MSE）小于$10^{-2}$，显著优于幂律、倒数和对数基线。\n- **应用实验**：在超参数选择（如数据混合比例）中，通过检索-重排（retrieval-rerank）流程，在目标模型（468M/1.2B）上训练少量数据（10B token）后预测最终损失，选择最优比例，结果在困惑度（PPL）和7个常识推理基准任务上大多优于小模型直接迁移或真实损失排序的基线。\n- **学习动态验证**：通过分析token位置的损失下降率，验证了默认预训练策略（对所有位置损失平均加权）的合理性。\n\n**批判性思考**：实验设置覆盖了多种模型规模和数据集，但仍存在局限性：模型规模未达到当前主流LLM的百亿或千亿级别，推广性存疑；预测实验仅基于早期数据，未能充分模拟训练中后期可能出现的学习率调整或数据分布变化等复杂情况；应用实验的基准任务数量有限，且未报告计算成本（如预测和重排的额外开销），这可能限制实际应用价值。总体而言，实验结果支持了方法的有效性，但设计不够全面，未能完全验证方法在真实大规模训练场景中的鲁棒性。", "institution": ["Tsinghua University", "Beijing National Research Center for Information Science and Technology (BNRist)", "Kuaishou Technology", "Beihang University", "Institute of Information Engineering, Chinese Academy of Sciences", "Beijing University of Posts and Telecommunications"], "source_file": "2404.17785.json", "keywords": ["Large Language Model", "Pre-training", "Representation Learning", "Efficiency"], "temperature": 0.1, "model": "grok-3", "further_thoughts": "本文提出的时间缩放定律为LLM预训练提供了一个新颖的时间视角，尤其是在超参数直接优化和学习动态分析方面的应用潜力值得关注。然而，其方法和实验的局限性提示我们需要进一步探索其在更大规模模型和更复杂训练场景（如多模态预训练或迁移学习）中的适用性。一个有趣的延伸方向是结合时间缩放定律与传统的缩放定律，构建一个统一的框架，同时考虑模型规模、数据规模和训练时间等多维度因素，以更全面地指导LLM的设计和训练。此外，论文未讨论的计算成本问题也值得深入研究，例如是否可以通过更高效的近似方法减少预测和参数拟合的开销，尤其是在资源受限的场景下。另一个启发是，token位置的损失差异是否可以进一步用于设计自适应训练策略，例如动态调整不同位置的损失权重，以加速收敛或提升特定任务性能，这可能与现有的注意力机制优化研究（如长上下文处理）产生有趣的交叉。", "lang": "zh", "categories": ["cs.CL"], "summary_time": "2025-05-18T03:16:22.442360+00:00", "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.", "title": "Temporal Scaling Law for Large Language Models"}

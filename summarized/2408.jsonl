{"id": "2408.06621", "reasoning_step": null, "problem_background": "大型语言模型（LLMs）在海量文本数据上预训练，展现出强大的推理和记忆能力。然而，这种记忆能力也带来了隐私和版权风险，敏感信息或受版权保护的内容可能被模型记住并泄露。传统的精确遗忘方法（从头开始重新训练）对于大型模型和数据集来说计算成本极高，难以应对频繁的遗忘请求。因此，研究界转向近似遗忘方法，目标是在不从头训练的情况下移除特定数据知识。现有的近似遗忘方法，如梯度上升（GA），存在优化不稳定和灾难性遗忘（忘记保留的知识）的问题。将 GA 与参数高效微调（如 LoRA）结合，虽然降低了计算成本，但在遗忘效果和性能保持之间权衡较差。因此，需要一种更鲁易、参数更高效的 LLM 知识遗忘方法。", "slug": "low-rank-knowledge-unlearning-for-llms", "one_sentence_summary": "本文提出了低秩知识遗忘（LoKU）框架，包含反向铰链损失（IHL）和 Fisher 加权低秩适配器初始化（FILA），以实现鲁棒且参数高效的大语言模型知识遗忘，有效移除敏感信息同时保持模型原有能力。", "preference": "unknown", "updated": "2025-04-28", "top_p": 0.8, "method": "本文提出的低秩知识遗忘（LoKU）框架包含两个核心技术：\n1.  **反向铰链损失 (Inverted Hinge Loss, IHL):** 针对传统梯度上升 (GA) 遗忘方法的缺点（梯度扩散、无界优化、生成性能下降），IHL 旨在更精确地降低遗忘集中目标 token 的预测概率。GA 通过最大化负对数似然，会无差别地提升所有非目标 token 的概率。IHL 的核心思想是，对于遗忘集中的序列 $(x_1, \\dots, x_T)$，在预测下一个 token $x_t$ 时，它不仅降低真实 token $x_t$ 的概率 $p_\\theta(x_t|x_{<t})$，还特别关注并提升下一个最有可能的 token $v^\\star = \\arg\\max_{v \\ne x_t} p_\\theta(v|x_{<t})$ 的概率，而对其他不相关的 token 影响较小。其损失函数定义为 $\\mathcal{L}_{\\text{IHL}}(\\mathbf{x}) = 1 + p_{\\theta}(x_t|x_{<t}) - \\max_{v \\ne x_t} p_{\\theta}(v|x_{<t})$。这个损失函数是有界的，并且其梯度主要集中在真实 token $x_t$ 和次优 token $v^\\star$ 上，从而实现更稳定和有针对性的遗忘，减少对模型生成能力的损害。\n2.  **Fisher 加权低秩适配器初始化 (Fisher-Initialization of Low-rank Adapters, FILA):** 为了解决在低秩适应（LoRA）范式下，模型可塑性不足导致遗忘效率低的问题，FILA 提出了一种数据自适应的 LoRA 初始化方法。它利用 Fisher 信息来衡量模型参数对于生成遗忘集 ($D^f$) 相对于保留集 ($D^r$) 的相对重要性。具体来说，计算每个参数的经验 Fisher 信息矩阵 $\\hat{\\mathbf{F}}_\\theta(D)$，并使用相对 Fisher 信息 $\\hat{\\mathbf{F}}_W^{\\text{rel}} = \\hat{\\mathbf{F}}_W(D^f) / \\hat{\\mathbf{F}}_W(D^r)$ 作为参数重要性度量。然后，FILA 通过解决一个加权低秩近似问题，利用这些相对重要的参数来初始化 LoRA 适配器权重 $A^*$ 和 $B^*$。这种初始化使得 LoRA 适配器在训练开始时就偏向于调整那些对生成遗忘集至关重要的参数，从而加速遗忘过程，同时保持对保留集知识的稳定性。\n\n最终的 LoKU 训练目标是结合 IHL 和保留集上的语言模型损失（GD 的一部分），并在 FILA 初始化的 LoRA 适配器上进行优化：$\\underset{\\theta_{\\text{PLA}}}{\\text{minimize}} \\sum_{\\boldsymbol{\\mathfrak{w}}_{r} \\in \\mathcal{D}_{f}, \\boldsymbol{\\mathfrak{w}}_{f} \\in \\mathcal{D}_{r}} \\mathcal{L}_{\\text{IHL}}(\\boldsymbol{\\mathfrak{x}}_{f}) + \\mathcal{L}_{\\text{LM}}(\\boldsymbol{\\mathfrak{x}}_{r})$，其中 $\\theta_{\\text{PLA}}$ 是 LoRA 适配器参数。", "created": "2025-04-24", "authors": ["Sungmin Cha", "Sungjun Cho", "Dasol Hwang", "Moontae Lee"], "score": 0.7014947932320964, "experiment": "本文在两个主要数据集上进行了实验：\n*   **Training Data Extraction Challenge (TDEC):** 使用 GPT-Neo 125M, 1.3B, 和 2.7B 模型，从 Pile 数据集中随机抽取 32 个序列作为遗忘集（D<sup>f</sup>），使用 WikiText 作为保留集（D<sup>r</sup>）。评估指标包括 n-gram 提取可能性 (ELn) 和记忆准确率 (MA) 来衡量遗忘效果，以及在 9 个分类任务（推理能力）、4 个对话生成任务（生成能力）和 Pile 数据集上的困惑度 (PPL) 来衡量模型在遗忘后的性能保持情况。实验比较了全参数微调的 GA、GD、IHL 方法以及使用 LoRA (rank 16) 的 GD、IHL、GD+FILA、IHL+FILA 方法。结果表明，LoKU (IHL+FILA) 在实现有效遗忘的同时，相比其他 LoRA 方法更能保持模型在推理和生成任务上的性能，并且比全参数方法更高效。\n*   **Task of Fictitious Unlearning (TOFU):** 使用 Phi-1.5B 和 Llama2-7B 模型，首先在 TOFU 数据集上进行微调，然后尝试遗忘 1%、5% 或 10% 的虚构作者信息。评估指标包括 Kolmogorov-Smirnov 检验的 p 值（Forget Quality）衡量遗忘程度（与仅在保留集上微调的模型进行比较），以及在保留集、真实作者和世界事实数据集上的综合性能（Model Utility）衡量模型其他知识的保留情况。实验比较了使用 LoRA (rank 4, 8, 16, 32) 的 GA, GD, IHL, GD+FILA, IHL+FILA 方法，以及与 KL, DPO, NPO 等其他基线方法进行对比。结果显示，LoKU (IHL+FILA) 在不同遗忘比例和 LoRA 秩下，都能在保持较高 Model Utility 的同时实现更好的 Forget Quality，优于大多数基线方法。实验还发现，LoRA 应用于 FFN 层比仅应用于注意力层更能有效实现遗忘。", "institution": ["New York University", "University of Wisconsin-Madison", "LG AI Research", "University of Illinois Chicago"], "source_file": "2408.06621.json", "keywords": ["Large Language Model", "Parameter-Efficient Fine-Tuning", "Privacy-Preserving Machine Learning", "Robustness", "Efficiency", "Reasoning"], "temperature": 0.0, "model": "gemini-2.5-flash-preview-04-17", "further_thoughts": "本文提出的 IHL 和 FILA 方法为 LLM 知识遗忘提供了新的视角。IHL 通过有针对性地调整预测概率，避免了 GA 的梯度扩散和无界优化问题，这对于大型词汇表和复杂任务尤其重要。FILA 利用 Fisher 信息来指导 LoRA 初始化，将参数高效微调与知识遗忘的关键参数识别相结合，这种数据自适应的初始化思路非常值得借鉴，未来可以探索是否能将其应用于其他 PEFT 任务或模型编辑场景。此外，论文在 TOFU 实验中观察到的“Streisand effect”（过度遗忘导致遗忘质量下降）现象，提示我们在实际应用中需要更鲁棒的遗忘停止准则，以及不依赖于参考模型的遗忘效果评估方法。这与现实世界中难以获得“理想”参考模型的情况相符，是未来研究的重要方向。Fisher 信息在本文中用于识别与遗忘集相关的参数，这与一些模型可解释性或神经元重要性研究方向也有关联，或许可以进一步探索 Fisher 信息在理解 LLM 知识存储和遗忘机制中的作用。", "lang": "zh", "categories": ["cs.LG", "cs.CL"], "summary_time": "2025-05-05T17:27:23.608285+00:00", "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.", "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs"}
{"id": "2408.08696", "reasoning_step": "在深入阅读这篇论文《Token Recycling》后，我发现其核心创新点在于对大型语言模型（LLM）推理解码过程中被丢弃的候选词（candidate tokens）的再利用。这种方法试图解决推理解码中的延迟问题，通过存储和动态更新候选词来构建草稿树（draft tree），从而加速推理解码过程。论文提出了一种无训练（train-free）的方法，相较于其他需要额外训练或大规模存储的推测解码（speculative decoding）方法，具有一定的创新性。然而，我对方法的实际效果和实验设计的合理性有一些疑问。首先，论文声称取得了约2倍的加速比，并超越了其他无训练方法30%以及训练方法Medusa 25%，但实验结果是否全面反映了方法的优越性？是否在不同任务和模型规模上的表现一致？其次，邻接矩阵（adjacency matrix）的存储和更新机制是否真的如论文所述高效？<2MB的存储需求看似很小，但对于超大规模词汇表是否仍然适用？此外，论文提到的静态树结构是否限制了方法的适应性？在进一步分析中，我注意到实验主要集中在贪婪解码（greedy decoding）和批量大小为1的场景，这可能无法完全代表实际应用中的复杂情况。论文在某些任务（如翻译）上表现出色，但在总结任务（Summarization）上略逊于PLD，这可能暗示方法对内容重复性高的任务优化不足。总体而言，Token Recycling提供了一个有趣的视角，但其实际应用价值和局限性需要更深入探讨。", "problem_background": "大型语言模型（LLM）由于其自回归解码策略（auto-regressive decoding），在推理过程中每次只能生成一个词（token），导致推理延迟高，成为应用中的主要瓶颈。这种延迟主要源于每次解码步骤中将大量参数从高带宽内存传输到加速器缓存，而非计算本身。推测解码（speculative decoding）作为一种无损加速技术，通过‘猜测与验证’（guess-and-verify）范式在单次解码步骤中生成多个词，但现有方法要么依赖额外训练的模型架构，要么依赖存储需求大、检索耗时且适应性差的检索库。论文提出了一种新方法，旨在解决现有推测解码方法中候选词被浪费的问题，通过回收解码过程中生成的候选词来构建动态检索库，从而提高推理效率。", "slug": "token-recycling-speculative-decoding", "one_sentence_summary": "Token Recycling 提出了一种无训练的推测解码方法，通过回收候选词并利用邻接矩阵构建草稿树，实现大型语言模型推理约 2 倍加速，相较于其他无训练方法提升超 30%。", "preference": "unknown", "updated": "2025-05-20", "top_p": 0.8, "method": "Token Recycling (TR) 的核心思想是回收大型语言模型在解码过程中生成的候选词（candidate tokens），将其存储并用于后续推测解码的加速。\n- **邻接矩阵存储**：TR 使用一个邻接矩阵（adjacency matrix）存储每个词的 top-k 候选词，矩阵初始为空或通过‘热启动’（hot start）继承先前矩阵以提高初始预测能力。矩阵规模较小（<2MB），支持高效并行处理。\n- **草稿树构建**：在每次解码前，基于当前内容最后一个词，通过类似广度优先搜索（BFS）的算法从矩阵中检索候选词，构建一个静态且不平衡的草稿树（draft tree）。树结构预定义，优先分配计算资源给概率较高的路径。\n- **验证与更新**：利用树注意力（tree attention）机制并行验证草稿树中的多个序列，选择最长的正确序列作为输出；随后用解码过程中新生成的候选词（包括接受和拒绝的词）更新矩阵，使检索空间动态适应当前内容。\n**批判性思考**：虽然 TR 的无训练特性降低了资源需求，但静态树结构可能限制了方法对复杂序列的适应性。此外，矩阵更新中对重复词的并行处理策略（CUDA合并）可能导致更新结果不一致，论文虽称影响较小，但未提供足够证据证明长期使用下的稳定性。邻接矩阵的存储效率在超大词汇表场景下也值得进一步验证。", "created": "2025-05-19", "authors": ["Xianzhen Luo", "Yixuan Wang", "Qingfu Zhu", "Zhiming Zhang", "Xuanyu Zhang", "Qing Yang", "Dongliang Xu"], "score": 0.9172828074709924, "experiment": "实验在 SpecBench（涵盖多轮对话、翻译、总结、问答、数学推理和检索增强生成）和 MBPP（代码生成数据集）上进行，使用 Vicuna（通用任务）和 Code Llama（代码任务）模型，规模覆盖 7B、13B 和 33B/34B 参数。实验设置聚焦于贪婪解码和批量大小为1的场景，评估指标包括平均接受词数（MAT）、每秒词数（Ts/s）和加速比（Speedup ratio）。\n- **结果**：TR 在 SpecBench 上实现了约 2 倍加速比，相较于其他无训练方法（Lookahead、PLD、REST）提升超过 30%，甚至比需训练的 Medusa 方法高出约 25%。在代码领域（MBPP），TR 加速比达到 2.3 倍，表现尤为突出。TR 在大多数子任务（如翻译、数学推理）上表现优异，但在总结任务上略逊于 PLD，可能是由于总结任务重复内容较多，而 TR 对新词生成优化更强。\n- **实验设计分析**：实验覆盖了多种任务和模型规模，设置较为全面，但贪婪解码和单批量设置可能限制了结果的普适性，未反映复杂采样策略或多批量场景下的表现。此外，论文未充分探讨树结构参数（如深度和广度）对不同任务的影响，仅在 MT-Bench 上做了初步分析，缺乏跨任务的系统性对比。\n- **批判性思考**：虽然结果显示 TR 效果显著，但部分任务（如总结）表现不佳可能暗示方法对内容重复性高的场景优化不足。此外，实验未涉及推理延迟对响应质量的潜在影响，尤其是在树注意力与自回归解码选择不同词的罕见情况下，论文仅轻描淡写地提及质量影响很小，缺乏数据支持。", "institution": ["Harbin Institute of Technology", "Du Xiaoman (Beijing) Science Technology Co., Ltd."], "source_file": "2408.08696.json", "keywords": ["Large Language Model", "Efficiency", "Generative AI", "Prediction"], "temperature": 0.1, "model": "grok-3", "further_thoughts": "Token Recycling 的方法提供了一个有趣的视角，即如何利用解码过程中的‘废弃’信息来提升效率，这种思路或许可以扩展到其他生成式 AI 领域，如图像生成或多模态模型中，探索是否能通过类似‘候选特征回收’的方式加速生成过程。此外，TR 的动态更新机制让我联想到联邦学习（Federated Learning）中的模型更新策略，是否可以借鉴联邦学习中的隐私保护技术来优化矩阵更新过程，避免潜在的信息泄露风险，尤其是在矩阵继承‘热启动’时可能包含敏感模式的情况下。另一方面，论文中静态树结构的局限性提示我们，是否可以引入自适应树结构或结合强化学习（Reinforcement Learning）来动态调整树的参数，以更好地适应不同任务需求？这种改进可能进一步提升 TR 的泛化能力，但也可能增加计算开销，如何权衡是一个值得深入研究的方向。", "lang": "zh", "categories": ["cs.CL", "cs.LG"], "summary_time": "2025-05-23T03:14:21.432025+00:00", "abstract": "Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \\textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\\% and even a widely recognized training method by 25\\%.", "title": "Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling"}
{"id": "2408.08780", "reasoning_step": "在深入阅读这篇论文后，我发现其核心主张是大型语言模型（LLMs）在上下文学习（ICL）中对提示格式的敏感性远高于对描述内容的关注。这一观点通过提出一种新的'Ensemble'提示框架并在机器翻译（MT）和其他任务上进行实验验证。然而，论文的实验设计和结论引发了一些疑问：首先，'Ensemble'框架的改进是否真的源于格式本身，而非描述内容？实验中即使使用随机名词描述也能提升性能，这是否意味着模型完全忽略了描述内容？其次，实验覆盖的任务和模型有限，尤其是对大型模型如GPT-3.5的测试结果未显示显著改进，这是否表明该框架的适用性有限？此外，论文未深入探讨为何格式会带来改进，仅通过注意力权重分析提供了一些表面证据，缺乏更深层次的理论支持。我还注意到，实验中对提示格式的消融研究较为初步，未充分探索其他可能的格式设计。最后，作者承认了研究的局限性，如模型覆盖不足和对训练数据的不可知，这也限制了结论的普适性。在总结时，我需要客观评价其贡献，同时指出其不足，特别是在理论深度和实验广度上的欠缺。", "problem_background": "大型语言模型（LLMs）通过上下文学习（ICL）在多种自然语言处理任务中表现出色，但描述性指令在ICL中的作用尚未被充分探索。本研究关注于在ICL中，是否通过描述上下文示例的选择标准（例如基于词汇或句法相似性）能够提升模型性能，进而探讨LLMs是否真正理解描述内容，还是仅仅对提示格式敏感。", "slug": "ensemble-prompt-framework-icl", "one_sentence_summary": "本文提出了一种'Ensemble'提示框架，通过描述上下文示例选择标准提升大型语言模型在上下文学习中的性能，实验表明模型对提示格式的敏感性远高于描述内容本身，尤其在小型模型上效果显著。", "preference": "unknown", "updated": "2025-05-20", "top_p": 0.8, "method": "本文提出了一种新的'Ensemble'提示框架，核心思想是通过在提示中加入示例级别的描述，告知模型上下文示例的选择依据。具体步骤如下：\n- **示例选择**：在机器翻译（MT）任务中，基于词汇相似性（使用BM25算法）和句法相似性（使用多项式算法）分别选择上下文示例，通常每个类型各占一半（例如4个示例中2个基于词汇，2个基于句法）。\n- **提示设计**：在'Ensemble'框架中，为每组示例添加描述性指令，例如'以下是基于相似词汇选择的示例'和'以下是基于相似句法选择的示例'，并对比常规无描述的'Vanilla'提示。\n- **变体测试**：设计多种提示变体，包括描述与示例选择不匹配、描述使用随机名词甚至完全无意义内容，以测试模型是否关注描述内容。\n- **扩展应用**：将该框架应用于常识问答、逻辑推理、数学推理和幻觉检测任务，测试其普适性。\n**批判性思考**：该方法虽然简单，但在理论上缺乏对为何格式比内容更重要的深入解释。随机名词描述也能提升性能的发现令人惊讶，但论文未充分探讨这是否意味着模型完全忽略描述内容，或是否仅对某些结构化提示有反应。此外，示例选择方法的组合方式较为机械，未优化不同任务的需求。", "created": "2025-05-17", "authors": ["Chenming Tang", "Zhixiang Wang", "Hao Sun", "Yunfang Wu"], "score": 0.8555386349118066, "experiment": "实验分为两大部分：\n- **机器翻译（MT）任务**：在FLORES-101数据集上测试6个翻译方向（涉及英语与德语、法语、俄语），使用Europarl和ParaCrawl作为示例数据库。模型包括XGLM7.5B和Alpaca 7B，评估指标为COMET分数。结果显示，'Ensemble'提示框架在大多数情况下比'Vanilla'提示提升性能，即使描述内容与示例选择不匹配或使用随机名词（如'Ensemble (Random + Random)'）。消融研究表明，移除描述或简化格式会降低性能增益，表明格式本身至关重要。注意力权重分析进一步显示，模型对有意义描述和随机描述的关注度差异不大。\n- **其他任务**：在9个数据集（涵盖常识问答、逻辑推理、数学推理和幻觉检测）上测试，使用Alpaca、Llama3、Mistral（均为7B参数）和GPT-3.5模型，评估指标为准确率。结果表明，在小型模型上，'Ensemble (Random + Random)'显著优于或等同于'Vanilla'提示，尤其结合链式思维（CoT）时效果更佳；但在GPT-3.5上改进不明显。\n**批判性思考**：实验设置较为全面，覆盖多种任务和模型，但存在以下问题：1）MT任务中示例选择方法（BM25+Polynomial）的组合未充分优化，且未与其他先进方法对比，难以判断提升是否显著；2）其他任务仅使用随机示例选择，未测试精心设计的示例选择是否会改变结论；3）对大型模型如GPT-3.5的测试结果未显示显著改进，表明框架可能对强大模型的适用性有限；4）注意力权重分析较为表面，未深入探讨模型内部机制。总体而言，实验结果支持了格式优于内容的结论，但未能完全解释原因，且普适性存疑。", "institution": ["National Key Laboratory for Multimedia Information Processing, Peking University", "MOE Key Laboratory of Computational Linguistics, Peking University", "School of Computer Science, Peking University"], "source_file": "2408.08780.json", "keywords": ["Large Language Model", "In-Context Learning", "Prompt Engineering", "Translation", "Reasoning"], "temperature": 0.1, "model": "grok-3", "further_thoughts": "本文提出的'Ensemble'提示框架提供了一个有趣的视角，即提示格式可能比内容更关键，这与近年来关于提示工程（Prompt Engineering）的研究趋势相呼应，例如链式思维（CoT）和结构化提示对模型性能的影响。然而，这一发现也引发了更深层次的问题：是否所有结构化提示都能带来类似改进，还是存在某种特定的格式模式（如分层描述）对模型的上下文理解有特殊作用？此外，论文未探讨的另一个方向是，这种格式敏感性是否与模型的预训练数据分布有关，例如模型是否在训练中对某些提示结构有偏好？与其他领域的研究联系起来，可以考虑将这一框架与强化学习从人类反馈（RLHF）结合，测试是否通过调整提示格式可以进一步优化模型的指令跟随能力。最后，考虑到大型模型如GPT-3.5未显示显著改进，未来研究应聚焦于模型规模与提示格式效果之间的关系，或许可以通过分析模型的注意力机制或中间层表示，揭示格式影响性能的深层原因。", "lang": "zh", "categories": ["cs.CL"], "summary_time": "2025-05-26T03:41:38.088520+00:00", "abstract": "With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance. But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns. We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions. Our code will be publicly available once this paper is published.", "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions"}
{"institution": ["Princeton University", "Meta"], "reasoning_step": "这篇论文的核心创新点在于利用大语言模型（LLM）的元认知能力自动提取技能标签，再通过随机技能组合生成高质量指令微调数据。但需要批判性审视几个关键问题：1）仅用4K样本就声称超越Claude 3 Opus等顶级模型，但实验显示其WildBench分数为-36.91（负值表示劣于基准），与Claude 3 Opus的-21.20存在显著差距，存在选择性展示指标嫌疑；2）依赖GPT-4-Turbo生成数据并评估，存在循环验证风险，尽管作者用Claude 3 Opus交叉验证，但Table 5显示不同评估模型差异显著；3）性能在4K样本后饱和，说明方法可能仅优化特定评估基准而非通用能力。此外，Table 6揭示该方法对低质量数据极度敏感，20%劣质数据就导致性能腰斩，实际应用鲁棒性存疑。", "problem_background": "指令微调需要高质量监督数据，但人工标注成本高昂（如Instruct-GPT需2万工时），现有公开数据集（Alpaca/UltraChat等）效果不佳。核心矛盾在于：1）传统众包数据质量不均，52K样本中仅1K有效；2）合成数据多样性不足，即使150万样本的UltraChat仍无效；3）RLHF/DPO等后续优化依赖昂贵人类反馈。本文试图解决用低成本自动化方案生成高效指令微调数据的问题，尤其针对开源模型微调效果远逊于同结构商业模型的现象。", "method": "两阶段自动化流程：\n1. 技能提取：直接提示GPT-4-Turbo生成指令遵循所需的核心技能集（如批判性思维、系统设计），无需人类参与或现有数据集，利用LLM元认知能力。\n2. 数据生成：随机选择k个技能组合（通常k=2），提示同一LLM生成融合这些技能的（指令,响应）对。例如组合(文学技能, 沟通技巧)生成文学分析指令，(系统设计, 批判思维)生成技术方案设计指令。\n关键创新：通过N选k技能组合（N≈500时组合数>12.5万）实现多样性，成本控制在600美元内生成4K样本。", "experiment": "实验结果存在显著不平衡：\n优势：\n- 在AlpacaEval 2.0上，LLaMA-3-8B基模型+4K样本达42.76%胜率，超过LLaMA-3.1-405B指令模型(22.9%)。\n- Mistral-7B基模型+4K样本在WildBench达-29.25，优于其官方指令模型(-54.7)。\n- 消融实验证实技能提取是关键：基于Alpaca技能生成的1K样本(27.04%胜率)显著优于Alpaca原始1K最长样本(10.09%)。\n缺陷：\n1. 指标矛盾：Gemma-9B+2K样本MT-Bench得分8.12优于GPT-3.5，但其AlpacaEval胜率36.18%却低于Gemma官方指令模型(37.21%)。\n2. 过拟合明显：性能在4K样本后饱和，WildBench成绩仍为负值（最优-29.25 vs Claude 3 Opus -21.2）。\n3. 数据敏感致命：添加20%低质量响应（如敷衍回答）使Mistral模型胜率从31.57%暴跌至0.77%（Table 6）。\n实验设置较全面但基准有局限：未测试长文本生成/多轮对话等实际场景。", "one_sentence_summary": "提出基于LLM元认知的自动化流程Instruct-SkillMix，通过技能提取与随机组合生成高质量指令微调数据，仅需2K-4K样本即可让基模型在部分基准媲美顶级商业模型，但暴露对数据质量敏感和性能饱和缺陷。", "slug": "instruct-skillmix-instruction-tuning", "keywords": ["Instruction Tuning", "Synthetic Data", "Meta-Learning", "Supervised Learning", "Foundation Model"], "further_thoughts": "该方法揭示了指令微调的本质可能是激活预训练已存在的能力而非注入新知识，这解释了为何少量高质量样本足矣。但技能组合的随机性实则是隐式数据增强，其效果可能随教师模型能力边界而受限——当GPT-4-Turbo被更先进模型取代时，该方法是否依然有效？有趣的是，Table 4显示即使使用相同教师模型，技能组合数据也优于传统人工精选数据，暗示指令微调效果更多取决于问题复杂度而非数据量。未来可将技能提取框架迁移至安全对齐领域，例如定义有害内容规避技能并组合生成对抗性微调数据。此外，性能饱和点（4K样本）与模型参数量的关系值得探究，或存在计算最优的数据量缩放律。", "model": "deepseek-reasoner", "temperature": 0.7, "top_p": 0.8, "lang": "zh", "id": "2408.14774", "preference": "unknown", "summary_time": "2025-05-30T07:05:39.633359+00:00", "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning", "authors": ["Simran Kaur", "Simon Park", "Anirudh Goyal", "Sanjeev Arora"], "abstract": "We introduce Instruct-SkillMix, an automated approach for creating diverse, high quality SFT data for instruction-following. The pipeline involves two stages, each leveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to extract core \"skills\" for instruction-following by directly prompting the model. This is inspired by ``LLM metacognition'' of Didolkar et al. (2024); (2) Data generation: uses the powerful LLM to generate (instruction, response) data that exhibit a randomly chosen pair of these skills. Here, the use of random skill combinations promotes diversity and difficulty. The estimated cost of creating the dataset is under $600.   Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from Instruct-SkillMix leads to strong gains on instruction following benchmarks such as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples, LLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0, a level similar to frontier models like Claude 3 Opus and LLaMA-3.1-405B-Instruct. Ablation studies also suggest plausible reasons for why creating open instruction-tuning datasets via naive crowd-sourcing has proved difficult. In our dataset, adding 20% low quality answers (``shirkers'') causes a noticeable degradation in performance. The Instruct-SkillMix pipeline seems flexible and adaptable to other settings.", "date": "2025-05-30", "categories": ["cs.LG", "cs.CL"], "created": "2025-05-28", "updated": "2025-05-30", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "year": 2025, "score": 0.8540427087710089, "show": 1}
